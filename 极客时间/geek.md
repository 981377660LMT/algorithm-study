散列表

参赛选手的编号我们叫作键（key）或者关键字。我们用它来标识一个选手。我们把**参赛编号转化为数组下标**的映射方法就叫作散列函数（或“Hash 函数”“哈希函数”），而散列函数计算得到的值就叫作散列值（或“Hash 值”“哈希值”）。
再好的散列函数也无法避免散列冲突。那究竟该如何解决散列冲突问题呢？我们常用的散列冲突解决方法有两类，开放寻址法（open addressing）和链表法（chaining）。
Word **文档中的单词拼写检查功能是如何实现的**
常用的英文单词有 20 万个左右，假设单词的平均长度是 10 个字母，平均一个单词占用 10 个字节的内存空间，那 20 万英文单词大约占 2MB 的存储空间，就算放
大 10 倍也就是 20MB。对于现在的计算机来说，这个大小完全可以放在内存里面。所以我们可以用散列表来存储整个英文单词词典。
当用户输入某个英文单词时，我们拿用户输入的单词去散列表中查找。如果查到，则说明拼写正确；如果没有查到，则说明拼写可能有误，给予提示。借助散列表这种数据结构，我们就可以轻松实现快速判断是否存在拼写错误。

0. 有两个字符串数组，每个数组大约有 10 万条字符串，如何快速找出两个数组中相同的字符串？
   以第一个字符串数组构建散列表，key 为字符串，value 为出现次数。再遍历第二个字符串数组，以字符串为 key 在散列表中查找，如果 value 大于零，说明存在相同字符串。时间复杂度 O(N)

1. 假设我们有 10 万条 URL 访问日志，如何按照访问次数给 URL 排序？
   遍历 10 万条数据，以 URL 为 key，访问次数为 value，存入散列表，同时记录下访问次数的最大值 K，时间复杂度 O(N)。
   如果 K 不是很大，可以使用桶排序，时间复杂度 O(N)。如果 K 非常大（比如大于 10 万），就使用快速排序，复杂度 O(NlogN)。

如果散列表中有 10 万个数据，退化后的散列表查询的效率就下降了 10 万倍。更直接点说，如果之前运行 100 次查询只需要 0.1 秒，那现在就需要 1 万秒。这样就有可
能因为查询操作消耗大量 CPU 或者线程资源，导致系统无法响应其他请求，从而达到拒绝服务攻击（DoS）的目的。这也就是散列表碰撞攻击的基本原理。

工业级散列表举例分析 Java 中的 HashMap

1. 初始大小
   HashMap 默认的初始大小是 16，当然这个默认值是可以设置的，如果事先知道大概的数据量有多大，可以通过修改默认初始大小，减少动态扩容的次数，这样会
   大大提高 HashMap 的性能。
2. 装载因子和动态扩容
   最大装载因子默认是 0.75，当 HashMap 中元素个数超过 0.75\*capacity（capacity 表示散列表的容量）的时候，就会启动扩容，每次扩容都会扩容为原来的两倍大小。
3. 散列冲突解决方法
   HashMap 底层采用链表法来解决冲突。即使负载因子和散列函数设计得再合理，也免不了会出现拉链过长的情况，一旦出现拉链过长，则会严重影响 HashMap 的性
   能。
   于是，在 JDK1.8 版本中，为了对 HashMap 做进一步优化，我们引入了红黑树。而当链表长度太长（默认超过 8）时，链表就转换为红黑树。我们可以利用红黑树快
   速增删改查的特点，提高 HashMap 的性能。当红黑树结点个数少于 8 个的时候，又会将红黑树转化为链表。因为在数据量较小的情况下，红黑树要维护平衡，比起
   链表来，性能上的优势并不明显。
4. 散列函数
   散列函数的设计并不复杂，追求的是简单高效、分布均匀。我把它摘抄出来，你可以看看。

**为什么散列表和链表经常会一起使用**
Redis 有序集合不仅使用了跳表，还用到了散列表。
链表实现的 LRU 缓存淘汰算法的时间复杂度是 O(n)，通过散列表可以将这个时间复杂度降低到 O(1)。
JAVA 中的 LinkedHashMap,是通过散列表和链表组合在一起实现的,Linked 实际上是指的是双向链表,和 LRU 缓存淘汰策略实现一模一样。支持按照插入顺序遍历数据，也支持按照访问顺序遍历数据。
python 的 orderedDict 也是散列表和链表组合在一起实现的
**散列表这种数据结构虽然支持非常高效的数据插入、删除、查找操作，但是散列表中的数据都是通过散列函数打乱之后无规律存储的。也就说，它无法支持按照某种顺序快速地遍历数据.如果希望按照顺序遍历散列表中的数据，那我们需要将散列表中的数据拷贝到数组中，然后排序，再遍历(连续的空间)。**
我们知道散列表是动态的数据结构，需要频繁的插入和删除数据，那么每次顺序遍历之前都需要先排序，这势必会造成效率非常低下。
如何解决上面的问题呢？就是将散列表和链表（或跳表）结合起来使用。

跳表 redis 有序集合
**跳表能支持快速的范围查询**
在有序集合中，每个成员对象有 2 个重要的属性，即 key（键值）和 score（分值）。
不仅会通过 score 来查找数据，还会通过 key 来查找数据。
举个例子，比如用户积分排行榜有这样一个功能：可以通过用户 ID 来查找积分信息，也可以通过积分区间来查找用户 ID。这里用户 ID 就是 key，积分就是 score
有序集合的操作如下：
① 添加一个对象；
② 根据键值删除一个对象；
③ 根据键值查找一个成员对象；
④ 根据分值区间查找数据，比如查找积分在[100.356]之间的成员对象；
⑤ 按照分值从小到大排序成员变量。
这时可以**按照分值 score 将成员对象组织成跳表结构**，**按照键值 id 构建一个散列表**。那么上面的所有操作都非常高效。

**哈希算法的四个应用场景**
第一个应用是唯一标识，哈希算法可以对大数据做信息摘要，通过一个较短的二进制编码来表示很大的数据。
第二个应用是用于校验数据的完整性和正确性。
第三个应用是安全加密，我们讲到任何哈希算法都会出现散列冲突，但是这个冲突概率非常小。越是复杂哈希算法越难破解，但同样计算时间也就越长。所以，选择哈希算法的时候，要权衡安全性和计算时间来决定用哪种哈希算法。
第四个应用是散列函数，这个我们前面讲散列表的时候已经详细地讲过，它对哈希算法的要求非常特别，更加看重的是散列的平均性和哈希算法的执行效率。
区块链使用的是 SHA256 哈希算法，计算哈希值非常耗时，如果要篡改一个区块，就必须重新计算该区块后面所有的区块的哈希值，短时间内几乎不可能做到。

**哈希算法在分布式系统中有哪些应用**
负载均衡(对客户端 IP 地址或者会话 ID 计算哈希值，将取得的哈希值与服务器列表的大小进行取模运算，最终得到的值就是应该被路由到的服务器编号)

数据分片
**如何统计“搜索关键词”出现的次数？**
假如我们有 1T 的日志文件，这里面记录了用户的搜索关键词，我们想要快速统计出每个关键词被搜索的次数，该怎么做呢？
MapReduce 的基本设计思想:对数据进行分片，然后采用多台机器处理的方法，来提高处理速度
具体的思路是这样的：为了提高处理的速度，我们用 n 台机器并行处理。我们从搜索记录的日志文件中，依次读出每个搜索关键词，并且通过哈希函数计算哈希值，然后再跟 n 取模，最终得到的值，就是应该被分配到的机器编号。这样，哈希值相同的搜索关键词就被分配到了同一个机器上。也就是说，**同一个搜索关键词会被分配到同一个机器上**。每个机器会分别计算关键词出现的次数，
最后合并起来就是最终的结果。
**如何快速判断图片是否在图库(1 亿张)中？**
我们的图库中有 1 亿张图片，很显然，在单台机器上构建散列表是行不通的。因为单台机器的内存有限，而 1 亿张图片构建散列表显然远远超过了单台机器的内存上限。
我们同样可以对数据进行分片，然后采用多机处理。我们准备 n 台机器，让每台机器只维护某一部分图片对应的散列表。我们每次从图库中读取一个图片，计算唯一标识，然后与机器个数 n 求余取模，得到的值就对应要分配的机器编号，然后将这个图片的唯一标识和图片路径发往对应的机器构建散列表。当我们要判断一个图片是否在图库中的时候，我们通过同样的哈希算法，计算这个图片的唯一标识，然后与机器个数 n 求余取模。假设得到的值是 k，那就去编号 k 的机器构建的散列表中查找。
给这 1 亿张图片构建散列表大约需要多少台机器。
散列表中每个数据单元包含两个信息:哈希值和图片文件的路径
假设我们通过 MD5 来计算哈希值，那长度就是 128 比特，也就是 16 字节
文件路径长度的上限是 256 字节，我们可以假设平均长度是 128 字节。如果我们用链表法来解决冲突，那还需要存储指针，指针只占用 8 字节。所以，散列表中每个数据单元就占用 152 字节（这里只是估算，并不准确）。
假设一台机器的内存大小为 2GB，散列表的装载因子为 0.75，那一台机器可以给大约 1000 万（2GB\*0.75/152）张图片构建散列表。
实际上，针对这种海量数据的处理问题，我们都可以采用多机分布式处理。借助这种分片的思路，可以突破单机内存、CPU 等资源的限制。

分布式存储
如何决定将哪个数据放到哪个机器上呢？我们可以借用前面数据分片的思想，即通过哈希算法对数据取哈希值，然后对机器个数取模，这个最终值就是应该存储的缓存机器编号。
但是，如果数据增多，原来的 10 个机器已经无法承受了，我们就需要扩容了，比如扩到 11 个机器，这时候麻烦就来了。因为，这里并不是简单地加个机器就可以了。
原来的数据是通过与 10 来取模的。比如 13 这个数据，存储在编号为 3 这台机器上。但是新加了一台机器中，我们对数据按照 11 取模，原来 13 这个数据就被分配到 2 号这台机器上了。
因此，所有的数据都要重新计算哈希值，然后重新搬移到正确的机器上。这样就相当于，缓存中的数据一下子就都失效了。所有的数据请求都会穿透缓存，直接去请求数据库。这样就可能发生雪崩效应，压垮数据库。
所以，我们需要一种方法，使得在新加入一个机器后，并不需要做大量的数据搬移。这时候，**一致性哈希算法**就要登场了。Redis 集群就是应用的一致性哈希算法

在负载均衡应用中，利用哈希算法替代映射表，可以实现一个会话粘滞(session sticky,也就是说，在一次会话中的所有请求都路由到同一个服务器上)的负载均衡策略。
在数据分片应用中，通过哈希算法对处理的海量数据进行分片，多机分布式处理，可以突破单机资源的限制。
在分布式存储应用中，利用一致性哈希算法，可以解决缓存等分布式系统的扩容、缩容导致数据大量搬移的难题。

二叉树
给定一组数据，比如 1，3，5，6，9，10。你来算算，可以构建出 C(2n,n)/n+1 种不同的二叉树
**有了如此高效的散列表，为什么还需要二叉树**
第一，散列表中的数据是无序存储的，如果要输出有序的数据，需要先进行排序。而对于二叉查找树来说，我们只需要中序遍历，就可以在 O(n)的时间复杂度内，输出有序的数据序列。
第二，散列表扩容耗时很多，而且当遇到散列冲突时，性能不稳定，尽管二叉查找树的性能不稳定，但是在工程中，我们最常用的平衡二叉查找树的性能非常稳定，时间复杂度稳定在 O(logn)。
第三，笼统地来说，尽管散列表的查找等操作的时间复杂度是常量级的，但因为哈希冲突的存在，这个常量不一定比 logn 小，所以实际的查找速度可能不一定比 O(logn)快。加上哈希函数的耗时，也不一定就比平衡二叉查找树的效率高。
第四，散列表的构造比二叉查找树要复杂，需要考虑的东西很多。比如散列函数的设计、冲突解决办法、扩容、缩容等。平衡二叉查找树只需要考虑平衡性这一个问题，而且这个问题的解决方案比较成熟、固定。
支持重复数据的二叉查找树
二叉查找树中每一个节点不仅会存储一个数据，因此我们通过链表和支持动态扩容的数组等数据结构，把值相同的数据都存储在同一个节点上。

堆这种数据结构
几个非常重要的应用：优先级队列、求 Top K 和求中位数,99%响应时间。
我们维护两个堆，一个大顶堆，一个小顶堆。假设当前总数据的个数是 n，大顶堆中保存 nx99%个数据，小顶堆中保存 nx1%个数据。大顶堆堆顶的数据就是我们要找的 99%响应时间。
在每次新插入数据之后，我们都要重新计算，这个时候大顶堆和小顶堆中的数据个数，是否还符合 99:1 这个比例。如果不符合，我们就将一个堆中的数据移动到另一个堆，直到满足这个比例。

图的表示：如何存储微博、微信等社交网络中的好友关系？

字符串匹配
**单模式串匹配**(一个串跟一个串进行匹配)
BF 算法和 RK 算法
RK:如果模式串长度为 m，主串长度为 n，那在主串中，就会有 n-m+1 个长度为 m 的子串，我们只需要暴力地对比这 n-m+1 个子串与模式串，就可以找出主串与模式串匹配的子串。我们通过哈希算法对主串中的 **n-m+1 个子串分别求哈希值，然后逐个与模式串的哈希值比较大小**;**因为哈希值是一个数字，数字之间比较是否相等是非常快速的，所以模式串和子串比较的效率就提高了。**
26 进制求哈希：我们把 a ～ z 这 26 个字符映射到 0 ～ 25 这 26 个数字，a 就表示 0，b 就表示 1，以此类推，z 表示 25。而相邻的两个子串哈希值有关系

BM 算法和 KMP 算法
**多模式串匹配**(一个串中同时查找多个串)
Trie 树和 AC 自动机

1. 要有优化意识，前面的 BF，RK 算法已经能够满足我们需求了，为什么发明 BM 算法？是为了减少时间复杂度，但是带来的弊端是，优化代码变得复杂，维护成本变高。
2. 需要查找，需要减少时间复杂度，应该想到什么？散列表。
3. 如果某个表达式计算开销比较大，又需要频繁的使用怎么办？预处理，并缓存。

Trie 树：如何实现搜索引擎的搜索关键词提示功能
我们有 6 个字符串，它们分别是：how，hi，her，hello，so，see。我们希望在里面多次查找某个字符串是否存在。如果每次查找，都是拿要查找的字符串跟这 6 个字符串依次进行字符串匹配，那效率就比较低，有没有更高效的方法呢？

回溯思想:0/1 背包，正则表达式(灾难性回溯)

动态规划实战：如何实现搜索引擎中的**拼写纠错功能**？

- 如何量化两个字符串的相似度？
  编辑距离有多种不同的计算方式，比较著名的有莱文斯坦距离（Levenshtein distance）和最长公共子串长度（Longest common substring length）。其中，莱文斯坦距离允许增加、删除、
  替换字符这三个编辑操作，最长公共子串长度只允许增加、删除字符这两个编辑操作。

当用户在搜索框内，输入一个拼写错误的单词时，我们就拿这个单词跟词库中的单词一一进行比较，计算编辑距离，取出编辑距离最小的 TOP 10，然后根据其他参数，决策选择哪个单词作为拼写纠错单词。比如使用搜索热门程度来决定哪个单词作为拼写纠错单词。
我们还可以用多种编辑距离计算方法，采用选举思想
过统计用户的搜索日志，得到最常被拼错的单词列表，以及对应的拼写正确的单词，引入个性化因素。

拓扑排序：如何确定代码源文件的编译依赖关系
最短路径：地图软件是如何计算出最优出行路径的？

**Dijkstra 算法的核心思想**
我们有一个翻译系统，只能针对单个词来做翻译。如果要翻译一整个句子，我们需要将句子拆成一个一个的单词，再丢给翻译系统。针对每个单词，翻译系统会返回一组可选的翻译列表，并且针对每个翻译打一个分，表示这个翻译的可信程度。
针对每个单词，我们从可选列表中，选择其中一个翻译，组合起来就是整个句子的翻译。每个单词的翻译的得分之和，就是整个句子的翻译得分。随意搭配单词的翻译，会得到一个句子的不同翻译。针对整个句子，我们希望计算出得分最高的前 k 个翻译结果，你会怎么编程来实现呢？
见 leecode 问题:**K 个数组选出一个数使得和最大**

**位图(BitMap)**：如何实现网页爬虫中的 URL 去重功能？
10 亿个网页（像 Google、百度这样的通用搜索引擎，爬取的网页可能会更多），为了判重，我们把这 10 亿网页链接存储在散列表中。你来估算下，**大约需要多少内存**？
假设一个 URL 的平均长度是 64 字节，那单纯存储这 10 亿个 URL，需要大约 60GB 的内存空间。因为散列表必须维持较小的装载因子，才能保证不会出现过多的散列冲突，导致操作的性能下降。而且，用链表法解决冲突的散列表，还会存储链表指针。所以，如果将这 10 亿个 URL 构建成散列表，那需要的内存空间会远大
于 60GB，有可能会超过 100GB。

位图（BitMap）
我们有 1 千万个整数，整数的范围在 1 到 1 亿之间。如何快速查找某个整数是否在这 1 千万个整数中
呢？我们申请一个大小为 1 亿、数据类型为布尔类型的数组。我们将这 1 千万个整数作为数组下标，将对应的数组值设置成 true。比如，整数 5 对应下标为 5 的数组值设置为 true，也就是 array[5]=true。
当我们查询某个整数 K 是否在这 1 千万个整数中的时候，我们只需要将对应的数组值 array[K]取出来，看是否等于 true。如果等于 true，那说明 1 千万整数中包含这个整数 K；相反，就表示不包含这个整数 K。
如果用散列表存储这 1 千万的数据，数据是 32 位的整型数，也就是需要 4 个字节的存储空间，那总共至少需要 40MB 的存储空间。如果我们通过
位图的话，数字范围在 1 到 1 亿之间，只需要 1 亿个二进制位，也就是 12MB 左右的存储空间就够了。
不过，这里我们有个假设，就是数字所在的范围不是很大。如果数字的范围很大，比如刚刚那个问题，数字范围不是 1 到 1 亿，而是 1 到 10 亿，**那位图的大小就是 10 亿个二进制位，也就是 120MB 的大小，消耗的内存空间，不降反增。**

**这个时候，布隆过滤器就要出场了**。布隆过滤器就是为了解决刚刚这个问题，对位图这种数据结构的一种改进。
数据**个数是 1 千万**，**数据的范围是 1 到 10 亿**。布隆过滤器的做法是，我们仍然使用一个 1 亿个二进制大小的位图，然后通过**哈希函数，对数字进行处理**，**让它落在这 1 到 1 亿范围内**。比如我们把哈希函数设计成 f(x)=x%n。其中，x 表示数字，n 表示位图的大小（1 亿），也就是，对数字跟位图的大小进行取模求余。
布隆过滤器使用 K 个哈希函数，对同一个数字进行求哈希值，那会得到 K 个不同的哈希值,在位图中都设置成 true，也就是说，我们用 K 个二进制位，来表示一个数字的存在。
如果某个数字经过布隆过滤器判断不存在，那说明这个数字真的不存在，不会发生误判；
如果某个数字经过布隆过滤器判断存在，这个时候才会有可能误判，
尽管布隆过滤器会存在误判，但是，这并不影响它发挥大作用。很多场景对误判有一定的容忍度。比如我们今天要解决的爬虫判重这个问题，**即便一个没有被爬取过的网页，被误判为已经被爬取，对于搜索引擎来说，也并不是什么大事情**，是可以容忍的，毕竟网页太多了，搜索引擎也不可能 100%都爬取到。
我们用布隆过滤器来记录已经爬取过的网页链接，假设需要判重的网页有 10 亿，那我们可以用一个 10 倍大小的位图来存储，也就是 100 亿个二进制位(保证足够大地容忍哈希冲突)，换算成字节，那就是大约 1.2GB。之前我们用散列表判重，需要至少 100GB 的空间。

**假设我们有 1 亿个整数，数据范围是从 1 到 10 亿，如何快速并且省内存地给这 1 亿个数据从小到大排序？**
传统的做法：1 亿个整数，存储需要 400M 空间，排序时间复杂度最优 N×log(N)
使用位图算法：数字范围是 1 到 10 亿，用位图存储 125M 就够了，然后将 1 亿个数字依次添加到位图中，然后再将位图按下标从小到大输出值为 1 的下标，排序就完成了，时间复杂度为 N

**概率统计：如何利用朴素贝叶斯算法过滤垃圾短信？**

- 基于黑名单的过滤器:我们要存储 500 万个手机号码，我们把位图大小设置为 10 倍数据大小，也就是 5000 万，那也只需要使用 5000 万个二进制位（5000 万 bits），换算成字节，也就是不到 7MB 的存储空
  间。比起散列表的解决方案，内存的消耗减少了很多。(布隆过滤器可能会存在误判情况，可能会导致用户投诉)
- 我们还有一种时间换空间的方法，可以将内存的消耗优化到极致。我们可以把黑名单存储在服务器端上，把过滤和拦截的核心工作，交给服务器端来做。手机端只负责将要检查的号码发送给服务器端，服务器端通过查黑名单，判断这个号码是否应该被拦截，并将结果返回给手机端。
- 基于规则的过滤器(短信中包含特殊单词（或词语）,群发号码，回拨，花哨，模板)
- 基于概率统计的过滤器:以通过分词算法，把一个短信分割成 n 个单词。这 n 个单词就是一组特征项，全权代表这个短信。因此，判定一个短信是否是垃圾短信这样一个问题，就变成了，判定同时包含这几个单词的短信是否是垃圾短信。

分类过滤，最好的可能是机器学习，通过大量的垃圾短信样本来训练特征，最后可以达到过滤短信和邮件的目的，而且这种方法应该效果更好，至于电话拦截，实际上就是电话号码黑名单的问题，我觉得用布隆过滤器可以满足通用场景，一般实际场景中，对于这种电话是提示谨慎接听，但是我们可以本地和云端结合处理，解决部分的误报问题，当判断是黑名单的时候再去云端查，确认是否是真的黑名单。这样用布隆过滤器+云端也是一种方式

**向量空间：如何实现一个简单的音乐推荐系统？**

1. 基于相似用户做推荐
   对老用户：
   我们只需要遍历所有的用户，对比每个用户跟你共同喜爱的歌曲个数，并且设置一个阈值，如果你和某个用户共同喜爱的歌曲个数超过这个阈值，我们就把这个用户看作跟你口味相似的用户，把这个用户喜爱但你还没听过的歌曲，推荐给你。某个人对某首歌曲是否喜爱，我们不再用“1”或者“0”来表示，而是对应一个具体的分值。
   我们把每个用户对所有歌曲的喜爱程度，都用一个向量表示。我们计算出两个向量之间的欧几里得距离，作为两个用户的口味相似程度的度量。从图中的计算可以看出，**小明与你的欧几里得距离距离最小，也就是说，你俩在高维空间中靠得最近，所以，我们就断定，小明跟你的口味最相似**。
2. 基于相似歌曲做推荐
   对新用户：
   我们对歌曲定义一些**特征项**，比如是伤感的还是愉快的，是摇滚还是民谣，是柔和的还是高亢的等等。类似基于相似用户的推荐方法，我们给每个歌曲的每个特征项打一个分数，这样每个歌曲就都对应一个特征项向量。我们可以基于这个特征项向量，来计算两个歌曲之间的欧几里得距离。**欧几里得距离越小，表示两个歌曲的相似程度越大**。对于收录了海量歌曲的音乐 App 来说，这显然是一个非常大的工程。此外，人工标注有很大的主观性，也会影响到推荐的准确性。**基于歌曲特征项计算相似度不可行**
   **对于两首歌，如果喜欢听的人群都是差不多的，那侧面就可以反映出，这两首歌比较相似。**

Bmore 树：MySQL 数据库索引是如何实现的？
根据**某个值**查找数据，比如 select _ from user where id=1234；
根据**区间值**来查找某些数据，比如 select _ from user where id > 1234 and id < 2345。
数据库索引所用到的数据结构跟跳表非常相似，叫作 B+树。不过，它是通过二叉查找树演化过来的，而非跳表。
为了让二叉查找树支持按照区间来查找数据，我们可以对它进行这样的改造
**树中的节点并不存储数据本身，而是只是作为索引**。除此之外，我们把每个**叶子节点串在一条双向链表(不是单链表；用以支持前后遍历)上，链表中的数据是从小到大有序的**。
如果我们要求某个区间的数据。我们只需要拿区间的起始值，在树中进行查找，当查找到某个叶子节点之后，我们再顺着链表往后遍历，直到链表中
的结点数据值大于区间的终止值为止。所有遍历到的数据，就是符合区间值的所有数据。

**如何解决这个索引占用太多内存的问题呢**
把索引存储在硬盘中，而非内存中
为了节省内存，如果把树存储在硬盘中，那么每个节点的读取（或者访问），都对应一次磁盘 IO 操作。**树的高度就等于每次查询数据时磁盘 IO 操作的次数**。我们前面讲到，比起内存读写操作，磁盘 IO 操作非常耗时，所以我们优化的重点就是尽量减少磁盘 IO 操作，也就是，尽量降低树的高度。**那如何降低树的高度呢？**
把索引构建成 m 叉树，高度比二叉树要小
不管是内存中的数据，还是磁盘中的数据，操作系统都是按页（一页大小通常是 4KB，这个值可以通过 getconfig PAGE_SIZE 命令查看）来读取的，一次会读一页的数据。如果要读取的数据量超过一页的大小，就会触发多次 IO 操作。所以，**我们在选择 m 大小的时候，要尽量让每个节点的大小等于一个页的大小。读取一个节点，只需要一次磁盘 IO 操作**。
数据的写入过程，会涉及索引的更新，这是索引导致写入变慢的主要原因。

一般情况，根节点会被存储在内存中，其他节点存储在磁盘中。

搜索：如何用 A\*搜索算法实现游戏中的寻路功能？
人物角色自动寻路。当人物处于游戏地图中的某个位置的时候，我们用鼠标点击另外一个相对较远的位置，人物就会自动地绕过障碍物走过去
A\*算法就是对 Dijkstra 算法的简单改造
A\*算法属于一种启发式搜索算法（Heuristically Search Algorithm）。
启发式搜索算法利用**估价函数**，避免“跑偏”，贪心地朝着最有可能到达终点的方向前进。这种算法找出的路线，**并不是最短路线**。但是，实际的软件开发中的路线规划问题，我们往往并不需要非得找最短路线。所以，鉴于启发式搜索算法能很好地平衡路线质量和执行效率，它在实际的软件开发中的应用更加广泛。实际上，在第 44 节中，我们讲到的地图 App 中的出行路线规划问题，也可以利用启发式搜索算法来实现。

索引：如何在海量数据中快速查找某个数据？
像 MySQL 这种结构化数据的查询需求，我们可以实现针对多个关键词的组合，建立索引；
对于像搜索引擎这样的非结构数据的查询需求，我们可以针对单个关键词构建索引，然后通过集合操作，比如求并集、求交集等，计算出多个关键词组合的查询结果。

**构建索引常用的数据结构有哪些？**
**散列表**增删改查操作的性能非常好，时间复杂度是 O(1)。一些键值数据库，比如 Redis、Memcache，就是使用散列表来构建索引的。这类索引，一般都构建在内存中。
**红黑树**作为一种常用的平衡二叉查找树，数据插入、删除、查找的时间复杂度是 O(logn)，也非常适合用来构建内存索引。Ext 文件系统中，对磁盘块的索引，用的就是红黑树。
**B+树**比起红黑树来说，更加适合构建存储在**磁盘中的索引**。B+树是一个多叉树，所以，对相同个数的数据构建索引，B+树的高度要低于红黑树。当借助索引查询数据的时候，读取 B+树索引，需要的磁盘 IO 次数非常更少。所以，大部分关系型数据库的索引，比如 MySQL、Oracle，都是用 B+树来实现的。
**跳表**也支持快速添加、删除、查找数据。而且，我们通过灵活调整索引结点个数和数据个数之间的比例，可以很好地平衡索引对内存的消耗及其查询效
率。Redis 中的有序集合，就是用跳表来构建的。
**位图和布隆过滤器**这两个数据结构，也可以用于索引中，辅助存储在磁盘中的索引，加速数据查找的效率。我们可以针对数据，构建一个布隆过滤器，并且存储在内存中。当要查询数据的时候，我们可以先通过布隆过滤器，判定是否存在。如果通过布隆过滤器判定数据不存在，那我们就没有必要读取磁盘中的索引了。

**并行算法：如何利用并行处理提高算法的执行效率？**

算法应用

1. redis

- 字符串
- 列表 压缩列表/双向循环链表
- 字典 压缩列表/散列表
- 集合 有序数组/散列表
- 有序集合 跳表
  数据结构的持久化问题，或者对象的持久化问题。这里的“持久化”，你可以笼统地可以理解为“存储到磁盘”。

2. 搜索引擎
   搜索引擎大致可以分为四个部分：搜集、分析、索引、查询
   实际上 scrapy(搜集)+es(分析、索引、查询) 可以模拟

   **搜集**:

   - 图的 BFS 遍历
   - 1.待爬取网页链接文件：links.bin
   - 2.网页判重文件：bloom_filter.bin
   - 3.原始网页存储文件：doc_raw.bin
   - 4.网页链接及其编号的对应文件：doc_id.bin

   **分析**

   - 1.抽取网页文本信息
   - 2.分词并创建临时索引

   **索引**

   - 1.将分析阶段产生的临时索引，构建成倒排索引(如何通过临时索引文件，构建出倒排索引文件呢)多路归并排序,用 MapReduce
   - 2.分词并创建临时索引

   **查询**

3. 高性能队列 Disruptor(线程之间的消息队列)
   如何解决线程并发往队列中添加数据时，导致的数据覆盖、运行不正确问题
   最简单的处理方法就是给这段代码加锁，同一时间只允许一个线程执行 add()函数。这就相当于将这段代码的执行，由并行改成了串行
   **基于无锁的并发“生产者-消费者模型”**
   为了在保证逻辑正确的前提下，尽可能地提高队列在并发情况下的性能，Disruptor 采用了“两阶段写入”的方法。**在写入数据之前，先加锁申请批量的空闲存储单元**，之后往队列中写入数据的操作就不需要加锁了，写入的性能因此就提高了。Disruptor 对消费过程的改造，跟对生产过程的改造是类似的。**它先加锁申请批量的可读取的存储单元**，之后从队列中读取数据的操作也就不需要加锁了，读取的性能因此也就提高了。

为了提高存储性能，我们往往通过分库分表的方式设计数据库表。假设我们有 8 张表用来存储用户信息。这个时候，每张用户表中的 ID 字段就不能通过自增的方式来产生了。因为这样的话，就会导致不同表之间的用户 ID 值重复。
为了解决这个问题，我们需要实现一个 ID 生成器，可以为所有的用户表生成唯一的 ID 号。那现在问题是，**如何设计一个高性能、支持并发的、能够生成全局唯一 ID 的 ID 生成器呢**？
使用 Redis 生成全局唯一 ID
1、 redis Incr 为原子操作，支持并发
2、redis 基于内存，性能较高

4. 微服务接口鉴权限流背后的数据结构和算法
   微服务是最近几年才兴起的概念。简单点讲，就是把复杂的大应用，解耦拆分成几个小的应用。这样做的好处有很多。比如，这样有利于团队组织架构的拆分，
   毕竟团队越大协作的难度越大；再比如，每个应用都可以独立运维，独立扩容，独立上线，各个应用之间互不影响。不用像原来那样，一个小功能上线，整个大
   应用都要重新发布。

鉴权的原理比较简单、好理解。那具体到实现层面，我们该用什么数据结构来存储规则呢？用户请求 URL 在规则中快速匹配，又该用什么样的算法呢
**精确匹配**：字符串匹配算法/有序数组
**前缀匹配**: Trie,Trie 树中的每个节点不是存储单个字符，而是存储接口被“/”分割之后的子目录（比如“/user/name”被分割为“user”“name”两个子目录）
**模糊匹配**:把不包含通配符的规则和包含通配符的规则分开处理。回溯算法

所谓限流，顾名思义，就是对接口调用的频率进行限制。比如每秒钟不能超过 100 次调用，超过之后，我们就拒绝服务。
最简单的限流算法叫**固定时间窗口限流算法**
我们可以对固定时间窗口限流算法稍加改造。我们可以限制任意时间窗口（比如 1s）内，接口请求数都不能超过某个阈值（ 比如 100 次）。因此，相对于固定时间窗口限流算法，这个算法叫**滑动时间窗口限流算法**。

5. 如何用学过的数据结构和算法实现一个短网址系统
   当用户点击短网址的时候，短网址服务会将浏览器重定向为原始网址

- 如何让短网址更短？
  哈希+62 进制转换(0 ～ 9、a ～ z、A ～ Z 这样 62 个字符)
  假设短网址与原始网址之间的对应关系，就存储在 MySQL 数据库中。
  当有一个新的原始网址需要生成短网址的时候，我们先利用 MurmurHash 算法，生成短网址。然后，我们拿这个新生成的短网址，在 MySQL 数据库中查找。如果没有找到相同的短网址，这也就表明，这个新生成的短网址没有冲突。于是我们就将这个短网址返回给用户（请求生成短网址的用户），然后将这个短网址与原始网址之间的对应关系，存储到 MySQL 数据库中。

总结：
**第一种实现思路是通过哈希算法生成短网址**。我们采用计算速度快、冲突概率小的 MurmurHash 算法，并将计算得到的 10 进制数，转化成 62 进制表示法，进一步缩短短网址的长度。对于哈希算法的哈希冲突问题，我们通过给原始网址添加特殊前缀字符，重新计算哈希值的方法来解决。
**第二种实现思路是通过 ID 生成器来生成短网址**。我们维护一个 ID 自增的 ID 生成器，给每个原始网址分配一个 ID 号码，并且同样转成 62 进制表示法，拼接到短网址服务的域名之后，形成最终的短网址。
借助布隆过滤器优化 SQL 语句次数

---

1. hooks 使用数组保存
2. 队列的实际使用
   HTTP2 解决了 HTTP1.1 中的**队头阻塞**问题
   http1.0 协议规定， 对于同一个 tcp 连接，所有的 http1.0 请求放入队列中，只有前一个请求的响应收到了，才能发送下一个请求，这个时候就发生了阻塞，并且这个阻塞主要发生在客户端
   在 HTTP/1.0 中每一次请求都需要建立一个 TCP 连接，请求结束后立即断开连接
   在 HTTP/1.1 中，每一个连接都默认是长连接 (persistent connection)。对于同一个 tcp 连接，允许一次发送多个 http1.1 请求，也就是说，不必等前一个响应收到，就可以发送下一个请求。这样就解决了 http1.0 的客户端的队头阻塞，而这也就是 HTTP/1.1 中管道 (Pipeline)的概念了。
   但是，http1.1 规定，服务器端的响应的发送要根据请求被接收的顺序排队
   可见，http1.1 的队首阻塞是发生在服务器端。
   **为了解决 HTTP/1.1 中的服务端队首阻塞**，HTTP/2 采用了**二进制分帧** 和 **多路复用** 等方法
   帧是 HTTP/2 数据通信的最小单位。在 HTTP/1.1 中数据包是文本格式，而 HTTP/2 的数据包是二进制格式的，也就是二进制帧
3. 浏览器的执行栈就是一个基本的栈结构，从数据结构上说，它就是一个栈。 这也就解释了，我们用递归的解法和用循环+栈的解法本质上是差不多的。
4. React fiber 是基于链表实现
   fiber 出现的目的其实是为了解决 react 在执行的时候是无法停下来的，需要一口气执行完的问题的
5. 那么有了线性结构，我们为什么还需要非线性结构呢？ 答案是为了高效地兼顾静态操作和动态操作，我们一般使用树去管理需要大量动态操作的数据。
6. immutable 与 字典树
   immutableJS 的底层就是 share + tree
7. AST 厉害就厉害在它本身不涉及到任何语法，因此你只要编写相应的转义规则，就可以将任何语法转义到任何语法。
   这就是 babel， PostCSS, prettier， typescript 等的原理，
8. 算法在日常开发中的应用都有哪些

   - `我们的系统需要支持用户撤销和重做最近十次的操作`

   1. store 可以被计算出来。因此我们只要存储 action 即可。
      比如我们要回退到第二步，我们拿出来 store1，然后和 action 运算一次，得到 store2，
      然后将 store2 覆盖到当前的 store 即可。
   2. 我们可以用树来表示我们的 store。每次修改 store，我们不是将整个 store
      销毁然后创建一个新的，而是重用可以重用的部分。

   - 巨型 Mapper 的优化
     由于业务需要，我们需要在前端缓存一些 HTTP 请求。
     我们的 key 中的前缀是有规律的，即有很多重复的数据在。 返回值也有可能是有很多重复的。
     这是一个典型的数据压缩算法。

   - 实现自动联想功能
     很多输入框都带了自动联想的功能， 很多组件库也实现了自动填充组件。
     我们可以用前缀树，很高效的完成这个工作。
   - 相似度检测
     相似度检测，我们其实可以借助“最小编辑距离”算法。
     如果编辑距离为 0 表示是相同的字符串，
     相似度为 100%。 我们可以加入自己的计算因子，将相似度
     离散在 0 - 100%之间。

9. 从大的范围上前端领域都在做什么
   架构和平台
   规范和标准化
   生态体系
10. 性能和优雅，我全都要

假如你现在开发一款类似石墨文档的多人在线协作编辑文档系统。

- 权限系统
  不同的角色可以分配不同的文件权限。 比如查看，下载，编辑，审批等。
  我们需要递归往上搜索，看有没有相应权限，如果有，则这个角色有文件的该操作权限。
  参考了 linux 的设计。
  使用一个二进制来标示一个权限有还是没有。
  只需要 4 个 bit 就可以存储权限信息
  是否可以下载/ 是否可以编辑/是否可以查看/是否可以审批
- 状态机
  我们以现实中广泛使用的有限状态机（以下简称 FSM）为例进行讲解
  FSM 应用非常广泛， 比如正则表达式的引擎，编译器的词法和语法分析，网络协议，企业应用等很多领域都会用到。
  通过状态机去控制系统内部的状态以及状态流转，逻辑会比较清晰，尤其在逻辑比较复杂的时候，这种作用越发明显。
  状态机的实际应用场景
  **匹配三的倍数**
  数字可以非常大，以至于超过 Number 的表示范围，因此我们需要用 string 来存储。

  **状态模式**
  有这样一个业务场景，我们需要设计一款答题活动，让用户过来进行答题，
  我们预先设置 N 道题目。 规则如下：
  初始状态用户会进入欢迎页面
  答对之后可以直接进入下一个题目
  答错了可以使用复活卡重新答，也可以使用过关卡，直接进入下一题
  用户可以通过其他途径获取复活卡和过关卡
  答对全部 N 道题之后用户过关，否则失败
  不管是过关还是失败都展示结果页面，只不过展示不同的文字和图片
  我会用 **FSM 来实现**

  假设我们后端服务器是一主一备，我们将所有的数据都同时存储在两个服务器上。
  假如某一天，有一份数据丢失了，我们如何快速找到有问题的服务器。
  可以抽象成【Single Number 问题】
  因此很多时候，不是缺乏应用算法的场景，
  而是缺乏这种将现实业务进行抽象为纯算法问题的能力。
  我们会被各种细枝末节的问题遮蔽双眼，无法洞察隐藏在背后的深层次的规律。

11. 浏览器的进程模型
    Chrome 采用多进程架构，其顶层存在一个 Browser process 用以协调浏览器的其它进程。
    渲染进程几乎负责 Tab 内的所有事情，渲染进程的核心目的在于转换 HTML CSS JS 为用户可交互的 web 页面。
    渲染进程由以下四个线程组成：主线程 Main thread ， 工作线程 Worker thread，光栅线程 Raster thread 和排版线程 Compositor thread。

        Main thread
        Worker thread: Web Woker 和 Service Worker 两种。

    假如我们可以涉及一个算法，智能地根据当前系统的硬件条件和网络状态，
    自动判断应该将哪部分交给工作线程，哪部分代码交给主线程，会是怎么样的场景？

12. 四个命令概括了 Git 的所有套路
    git 的三个「分区」
    Git 的三个分区分别是：working directory，stage/index area，commit history。

13. eslint 中的魔法数
    eslint 有一个 rule 是 no-magic-number. (就是说，常量定义应该拿出来而不应该写在代码里)

真的魔法数字(字符串吧)就是不好的么？

```JS
// 一般的
// 这在大家眼中已然成为了一种共识，那么这种所谓的魔法数字代码的不可读问题就不存在了。 我们仍可以轻易知道代码的含义。
MS = 0;
if (type === "day") {
  MS = 24 * 60 * 60 * 1000;
}
if (type === "week") {
  MS = 7 * 24 * 60 * 60 * 1000;
}

// 变量命令不是越长越好，越具体越好，而是根据具体的限定范围
// 比如你在 queue 的 class 中定义的 size 字段可以直接叫 size ，而不是 queue_size。
MS = 0;
const HOURS_PER_DAY = 24;
const MINUTES_PER_HOUR = 60;
const SECONDs_PER_MINUTE = 60;
const MS_PER_SECOND = 1000;
const DAYS_PER_WEEK = 7;
if (type === "day") {
  MS = HOURS_PER_DAY * MINUTES_PER_HOUR * SECONDs_PER_MINUTE * MS_PER_SECOND;
}
if (type === "week") {
  MS =
    DAYS_PER_WEEK *
    HOURS_PER_DAY *
    MINUTES_PER_HOUR *
    SECONDs_PER_MINUTE *
    MS_PER_SECOND;
}
```

[训练一个桌游 AI！从《情书/LoveLetter》开始](https://i.ctm49.com/diy/531.html)

好的，这篇文章详细记录了作者从零开始，为桌游《情书》（Love Letter）训练一个人工智能（AI）的全过程。以下是对文章内容的详细讲解：

### 核心思想

作者的目标是为《情书》这款“不完全信息博弈”游戏创建一个高水平的 AI。不完全信息博弈的难点在于，玩家无法知道游戏的所有情况（比如对手的手牌），因此需要根据对手的行为来推断未知信息，并做出最优决策。作者通过学习强化学习（Reinforcement Learning, RL）的基础知识，并利用现有的 RL 框架，最终成功开发并部署了一个可在线游玩的《情书》AI。

### 分步解析

1.  **前言与动机**

    - **目标**：为《情书》游戏找到“标准答案”，即最优策略。
    - **动机**：作者是桌游爱好者，受到 AlphaGo 等游戏 AI 的启发，希望通过 AI 来深入理解游戏策略，并解决现实中“找不到人玩”或“找不到高手玩”的困境。
    - **成果**：提供了一个可以直接在线与 AI 对战的网页版游戏。

2.  **基础知识学习**

    - 在动手之前，作者强调了学习理论知识的重要性，这有助于理解问题和解决问题。
    - **学习资源**：推荐了邱锡鹏的《神经网络与深度学习》和邹伟的《强化学习》两本入门书籍，并善用 GPT 等 AI 工具辅助学习。
    - **核心概念**：掌握了强化学习的几个基本概念，如马尔可夫决策过程（MDP）、价值函数、策略和 Q-learning，为后续实战打下基础。

3.  **《情书》游戏简介**

    - **游戏类型**：定义为“动态不完全信息博弈”，其挑战性远高于象棋、围棋等“完全信息博弈”。
    - **理论最优解 (SOTA)**：介绍了这类游戏的理论最优解是“完美贝叶斯均衡”（PBE）。通俗地讲，就是玩家需要根据观察到的行动来更新对未知信息（如对手手牌）的概率判断，并采取能让自己长期收益最大化的行动。
    - **基本规则**：简述了游戏“抽一打一”的核心玩法和 8 种卡牌各自的功能，点明了游戏策略的关键在于通过卡牌效果推测对手手牌，从而淘汰对手或在游戏结束时比拼点数获胜。

4.  **RLCard 框架**

    - **作用**：这是一个专门用于训练牌类游戏 AI 的开源框架，极大地简化了开发工作。
    - **优点**：内置了多种牌类游戏环境和成熟的强化学习算法（如 DQN, NFSP, CFR 等）。开发者只需专注于实现特定游戏的逻辑，而无需从头编写复杂的 AI 算法。
    - **开发流程**：介绍了在 RLCard 中的基本交互模式：训练脚本通过游戏环境与 AI 模型交互，进行一局游戏，收集数据（状态、动作、奖励等），然后用这些数据来训练模型。

5.  **开发过程详解**
    这是文章最核心的部分，详细描述了从 0 到 1 的实现细节。

    - **5.1 游戏逻辑实现**:

      - 作者借鉴了框架中已有的 UNO 项目，通过 AI 辅助编程，快速搭建了《情书》的游戏框架。
      - **关键难点 1：信息追踪**。对于牧师（看牌）、男爵（比牌）、国王（换牌）等改变信息状态的牌，必须精确记录“谁知道了谁的什么牌”。这种信息知识还会传递（例如 A 知道 B 的牌，B 和 C 换牌后，A 就知道 C 的牌了），也需要正确处理。这是让 AI 理解不完全信息博弈的关键。
      - **关键难点 2：合法动作空间**。需要精确定义在任何情况下，一个玩家可以执行哪些操作（例如，侍卫不能猜自己，但王子可以指定自己弃牌）。

    - **5.2 训练逻辑 (NFSP 算法)**:

      - 作者选用了**神经虚拟自博弈 (NFSP)** 算法。该算法的核心是同时训练两个网络：
        1.  **强化学习 (RL) 网络**：基于 DQN，目标是学习一个 Q 函数，用于评估在某个状态下执行某个动作的长期价值（奖励），从而做出最优决策。
        2.  **监督学习 (SL) 网络**：模仿 RL 网络做出的“最佳”决策。它学习的是一个从状态到最佳动作的映射。
      - 通过这种方式，RL 网络探索最优策略，SL 网络学习并稳定这个策略的平均表现，使 AI 的整体策略逐渐收敛到一个难以被对手预测和利用的均衡点（纳什均衡）。

    - **5.3 状态与动作编码**:
      - 这是连接“游戏世界”和“AI 大脑”的桥梁。作者将复杂的游戏局面“翻译”成神经网络能理解的数字向量（张量）。
      - **状态编码 (State)**：精心设计了输入给 AI 的信息，分为三部分：
        1.  **私有信息**：自己的手牌、自己知道的别人手牌、别人知道的自己手牌。
        2.  **行动历史**：最近几个回合的行动记录，让 AI 能理解游戏的动态进程。
        3.  **全局状态**：所有玩家的存活/保护状态、牌堆剩余牌数、各种牌的分布情况等。
      - **动作编码 (Action)**：将 AI 输出的决策数字“翻译”回具体游戏动作（如：对 2 号玩家使用侍卫，猜测他是公主）。

6.  **训练与调参**

    - 作者尝试了不同大小的神经网络，发现小网络虽然收敛快但不够“聪明”，大网络则容易“过拟合”（即只学会了针对特定情况的弱策略，泛化能力差）。
    - **最终方案**：通过使用较大的网络 `[512,512,256]` 并加入 `dropout` 技术（一种防止过拟合的方法），找到了一个较好的平衡点。
    - **训练结果**：AI 在不同人数的对局中都取得了超越随机 AI 的胜率，证明了训练的有效性。但作者也坦言，AI 在某些方面仍不如人类聪明，还有优化空间。

7.  **WebUI 制作**
    - 为了方便与 AI 对战，作者使用 Phaser.js 游戏引擎开发了一个网页版《情书》。
    - **开发方式**：同样大量借助 AI 编程工具（如 CodeBuddy）来辅助开发，将 Python 中的游戏逻辑和 AI 逻辑移植到 JavaScript 中。
    - **挑战**：主要在于 UI 设计和确保 Python 与 JavaScript 之间逻辑的完美对齐，这需要大量的调试工作。
    - **最终成果**：一个功能完整、可以在线游玩的 Web 版《情书》AI 对战平台。

### 总结

这篇文章是一篇非常完整和高质量的技术分享。它不仅展示了如何应用强化学习解决一个具体的游戏问题，还分享了从理论学习、工具选择、逻辑实现、模型训练到最终产品部署的全链路过程。对于想学习游戏 AI 开发的读者来说，这是一个极佳的实践案例。

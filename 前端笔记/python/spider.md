1. 反爬虫怎么做?怎么解决的？
2. JavaScript 逆向你一般怎么做的，先如何，再如何？
3. 你常用的数据库是？为什么会选择它。
4. 有在项目中用过消息中间件吗，例如 Kafka、RabbitMQ 之类的。
5. 如果遇到硬茬，你通常怎么做？
6. 代码混淆的手段有哪些？怎么实现的？
7. APP 逆向你现在到什么程度？
8. 讲讲 hook 原理和具体的操作过程。(就是回调)
9. top 命令:显示当前系统正在执行的进程的相关信息，包括进程 ID、内存占用率、CPU 占用率等
10. 布隆过滤器原理：如何实现、一般要几次哈希函数 6
11. 爬虫失败之后的重试是如何处理的，有没有针对具体的失败去做针对性的报警和重试
    `onerror 记录日志`
12. 爬虫资源配置怎么做的
13. 自己写的一些中间件
14. 爬虫大厂、搜索引擎等开源的爬虫框架、思想之类的
15. scrapy 会用到哪些中间件，各个中间件有什么功能
16. 验证码如何处理，TensorFlow 训练成功率多少
17. wireshark 报文如何查看
18. 逆向是怎么学的
19. JS 混淆 :当破解成本高到一定程度，就起到了防的效果。
20. crontab 命令报错了可能有哪些原因
21. 常用的网络数据爬取方法
    正则表达式
    Beautiful Soup
    Lxml
22. 反爬虫策略以及解决方法
    1. 通过 headers 反爬虫:对于基本网页的抓取可以自定义 headers,添加 headers 的数据
    2. 基于用户行为的发爬虫：(同一 IP 短时间内访问的频率):使用多个代理 ip 进行抓取或者设置抓取的频率降低一些，
    3. 动态网页反爬虫(通过 ajax 请求数据，或者通过 JavaScript 生成):动态网页的可以使用 selenium + phantomjs 进行抓取
    4. 对部分数据进行加密处理的(数据是乱码):对部分数据进行加密的，可以使用 selenium 进行截图，使用 python 自带的 pytesseract 库进行识别，但是比较慢最直接的方法是找到加密的方法进行`逆向推理`。
23. 如何知道一个网站是动态加载的数据(ajax)
    查看页面源代码，ctrl +F 查询输入内容，源代码里面并没有这个值，说明是动态加载数据。
    Selenium+Phantomjs 爬
    对及时性要求很高怎么处理？
    尽量不使用 sleep 而使用 `WebDriverWait`
24. 遇到反爬机制怎么处理？
    反爬机制:headers 方向
    判断 `User-Agent`、判断 `Referer`、判断 `Cookie`。
    将浏览器的 headers 信息全部添加进去
    注意：Accept-Encoding；gzip,deflate 需要注释掉
25. 如果让你来防范网站爬虫，你应该怎么来提高爬取的难度 ？
    1、判断 headers 的 User-Agent；
    2、检测同一个 IP 的访问频率；
    3、数据通过 Ajax 获取；
    4、爬取行为是对页面的源文件爬取，如果要爬取静态网页的 html 代码，可以使用 jquery 去模仿写 html。
    5、JS 混淆
26. urllib 和 urllib2 的区别？
    这两个都是接收 url 请求的模块。两者常常配合一块使用，但是功能不一样。
    1、urllib2 可以接收一个 Request 类的实例来设置 url 请求的 headers；而 urllib 只能接收 url，因此不能伪装 User-Agent 字符串。
    2、urllib 提供了 urlencode 方法用来 get 查询字符串的产生，而 urllib2 没有。
    因此总是 urllib，urllib2 常会一起使用
27. 常见的页面数据抽取方式有哪些？
    xpath
    re
    beautfulsoup4
    jsonpath
    cssselector
28. 分布式爬虫主要解决什么问题？
    面对海量待抓取网页，只有采用分布式架构，才有可能在较短时间内完成一轮抓取工作。
29. 为什么 python 更适合写爬虫程序
    python 中封装了很多爬虫库，如 urllib ,re,bs,scrapy 等，开发效率更高
30. 如何提高爬取效率？
    爬虫下载慢主要原因是阻塞等待发往网站的请求和网站返回
    1，采用异步与多线程，扩大电脑的 cpu 利用率；
    2，采用消息队列模式
    3，提高带宽
31. 爬虫协议？
    Robots 协议（也称为爬虫协议、爬虫规则、机器人协议等）也就是 robots.txt，网站通过 robots 协议告诉搜索引擎哪些页面可以抓取，哪些页面不能抓取。
    Robots 协议是网站国际互联网界通行的道德规范，其目的是保护网站数据和敏感信息、确保用户个人信息和隐私不被侵犯。因其不是命令，故需要搜索引擎自觉遵守。
32. 为什么 requests 请求需要带上 header？
    原因是：模拟浏览器，欺骗服务器，获取和浏览器一致的内容
33. 谈一谈你对 Selenium 和 PhantomJS 了解
    Selenium 是一个 Web 的`自动化测试工具`，可以根据我们的指令，让浏览器自动加载页面，获取需要的数据，甚至页面截屏，或者判断网站上某些动作是否发生。`Selenium 自己不带浏览器，不支持浏览器的功能`，它需要与第三方浏览器结合在一起才能使用。但是我们有时候需要让它内嵌在代码中运行，所以我们可以用一个叫 PhantomJS 的工具代替真实的浏览器。Selenium 库里有个叫 `WebDriver 的 API`。WebDriver 有点儿像可以加载网站的浏览器，但是它也可以像 BeautifulSoup 或者其他 Selector 对象一样用来查找页面元素，与页面上的元素进行交互 (发送文本、点击等)，以及执行其他动作来运行网络爬虫。
    PhantomJS 是一个基于 Webkit 的“无界面”(headless)浏览器，它会把`网站加载到内存并执行页面上的 JavaScript`，因为不会展示图形界面，所以运行起来比完整的浏览器要高效。相比传统的 Chrome 或 Firefox 浏览器等，资源消耗会更少。
    如果我们把 Selenium 和 PhantomJS 结合在一起，就可以运行一个非常强大的网络爬虫了，这个爬虫可以处理 JavaScrip、Cookie、headers，以及任何我们真实用户需要做的事情。主程序退出后，selenium 不保证 phantomJS 也成功退出，最好手动关闭 phantomJS 进程。（有可能会导致多个 phantomJS 进程运行，占用内存）。WebDriverWait 虽然可能会减少延时，但是目前存在 bug（各种报错），这种情况可以采用 sleep。phantomJS 爬数据比较慢，可以选择多线程。如果运行的时候发现有的可以运行，有的不能，可以尝试将 phantomJS 改成 Chrome。

34. 如何检测网站数据更新？
    增量式爬虫
35. 如何实现数据清洗？

    - 清洗空值（缺失值）
    - 清洗重复值
    - 清洗异常值

36. 验证码的解决?

    1. 输入式验证码:这里我们推荐使用 Python 的第三方库，tesserocr。
       对于有嘈杂的背景的验证码这种，直接识别识别率会很低，遇到这种我们就得需要先处理一下图片，先对图片进行灰度化，然后再进行二值化，再去识别，这样识别率会大大提高。
       转化成灰度图
       去背景噪声
       图片分割

       **也可以将验证码图片下载到本地，然后使用超级鹰识别；**

    2. 滑动式验证码:使用 selenium 模拟人工拖动，对比验证图片的像素差异，找到滑动的位置然后获取它的 location 和 size，然后 top，bottom，left，right = location['y'] ,location['y']+size['height']+ location['x'] +size['width'] ,然后截图，最后抠图填入这四个位置就行。

37. 怎么监控爬虫的状态?

    1. 2 使用 python 的 STMP 包将爬虫的状态信心发送到指定的邮箱
    2. Scrapyd、pyspider
    3. 引入日志
       集成日志处理平台来进行监控，如 elk

38. 爬下来数据你会选择什么存储方式，为什么
    大多数的应用场景是 MySQL（主）+Redis（辅），MySQL 做为主存储，Redis 用于缓存，加快访问速度。需要高性能的地方使用 Redis，不需要高性能的地方使用 MySQL。存储数据在 MySQL 和 Redis 之间做同步；
    MongoDB 与 MySQL 的适用场景：
    MongoDB 的适用场景为：**数据不是特别重要**（例如通知，推送这些），`数据表结构变化较为频繁`，`数据量特别大`，数据的`并发性特别高`，数据`结构比较特别`（例如地图的位置坐标），这些情况下用 MongoDB ， 其他情况就还是用 MySQL ，这样组合使用就可以达到最大的效率。
    MongoDB 缺点：
    MongoDB 不支持事务操作(最主要的缺点)

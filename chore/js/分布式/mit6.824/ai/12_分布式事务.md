## 初探分布式事务

### 核心观点

分布式事务的核心是为跨越多个服务器的复杂操作提供**ACID**保证，其精髓在于**隔离性（Isolation）**，而隔离性的黄金标准是**可序列化（Serializability）**。可序列化向程序员承诺了一个极其简单的编程模型：尽管多个事务在底层可能是高度并发、交错执行的，但其最终结果必须等同于这些事务按照**某个串行顺序**、一个接一个地执行所产生的结果。为了在分布式环境中实现这一承诺，系统必须依赖两大核心机制：**并发控制（Concurrency Control）**来确保隔离性，以及**原子提交（Atomic Commit）**来确保在面临节点故障时的原子性。

---

### 逻辑梳理

#### 1. 问题的根源：数据分片与跨节点操作

- **背景：** 为了负载均衡和存储扩展，大型系统会将数据（如银行账户、文章）**分片（Sharding）**到多个服务器上。
- **挑战：** 一个业务操作（如银行转账）可能需要同时修改位于不同服务器上的数据。这引入了并发和局部故障的复杂性。
- **目标：** 向应用程序开发者**隐藏**这种分布式复杂性，让他们感觉像在操作一个单一、可靠的数据库。

#### 2. 解决方案：事务与 ACID 保证

事务是将一组读写操作打包成一个逻辑单元的抽象。它通过提供 ACID 属性来保证正确性：

- **A (Atomic - 原子性):** “要么全做，要么全不做”。即使服务器崩溃，操作也不会只完成一半。
- **C (Consistent - 一致性):** 保证事务将数据库从一个一致的状态转移到另一个一致的状态（本文不重点讨论）。
- **I (Isolated - 隔离性):** 事务的执行互不干扰，仿佛各自在独立运行。
- **D (Durable - 持久性):** 一旦事务被确认（提交），其结果就是永久的，不会因后续故障而丢失。

#### 3. 隔离性的核心定义：可序列化 (Serializability)

这是理解事务正确性的关键。

- **定义：** 一组并发事务的执行结果是“可序列化的”，当且仅当这个结果与按**某种串行顺序**（一次一个）执行这些事务所能产生的某个结果完全相同。
- **银行转账示例：**
  - **事务 T1 (转账):** `X += 1`, `Y -= 1`
  - **事务 T2 (审计):** `t1 = read(X)`, `t2 = read(Y)`, `print(t1, t2)`
  - **初始状态:** `X=10, Y=10`
- **合法的串行执行结果只有两种：**
  1.  **顺序 T1 -> T2:** T1 先执行，数据库变为 `X=11, Y=9`。然后 T2 执行，读取并打印出 **"11, 9"**。
  2.  **顺序 T2 -> T1:** T2 先执行，读取到原始值 `X=10, Y=10`，打印出 **"10, 10"**。然后 T1 执行，数据库最终变为 `X=11, Y=9`。
- **不合法的结果：** 任何并发交错执行如果产生了除 "11, 9" 或 "10, 10" 之外的任何打印结果（如 "10, 9" 或 "11, 10"），都违反了可序列化，被认为是**不正确的**。例如，T2 读取了 X，然后 T1 完成了转账，T2 再读取 Y，就会看到不一致的中间状态。

#### 4. 实现可序列化事务的两大支柱

为了在分布式系统中兑现 ACID（特别是可序列化）的承诺，需要两个关键的技术组件：

1.  **并发控制 (Concurrency Control):**

    - **目标：** 实现**隔离性/可序列化**。
    - **作用：** 管理和协调并发事务对共享数据的访问，通过锁、时间戳等机制，强制它们的行为效果等同于某个串行顺序，防止出现不正确的交错执行。

2.  **原子提交 (Atomic Commit):**
    - **目标：** 实现**原子性**，尤其是在分布式环境中。
    - **作用：** 确保一个跨越多个服务器的事务，其所有参与者（服务器）要么**全部提交**修改，要么**全部中止（Abort）**并回滚修改。即使在通信中断或部分服务器崩溃的情况下，也能保证“全有或全无”的最终结果。两阶段提交（2PC）是实现这一目标的经典协议。

**结论：** 分布式事务的本质，就是通过**并发控制**和**原子提交**这两大技术手段，在复杂的分布式环境下，为开发者提供一个名为“可序列化”的、极其简单的编程幻象——你可以放心地编写业务逻辑，系统会保证你的操作就像在单机上独占运行一样，既不会被其他并发操作干扰，也不会因局部故障而留下一个烂摊子。

---

## 并发控制

### 核心观点

并发控制的核心目标是实现事务的**隔离性（可序列化）**，其主流策略之一是**悲观并发控制**，而实现悲观并发控制的经典方法就是**两阶段锁（Two-Phase Locking, 2PL）**。2PL 协议通过一条简单而严格的规则——**事务在结束（提交或中止）之前绝不释放任何已获得的锁**——来强制将并发操作的效果序列化，从而有效防止事务读到其他事务不一致的中间状态或最终被撤销的“脏”数据，但其代价是可能牺牲性能并引入死锁风险。

---

### 逻辑梳理

#### 1. 并发控制的两大哲学

系统如何处理潜在的事务冲突？存在两种截然不同的策略：

1.  **悲观并发控制 (Pessimistic):**

    - **假设：** 冲突很可能发生。
    - **策略：** “先问后走”。在访问数据前，必须先通过**加锁**来获得许可。如果数据已被其他事务锁定，则必须等待。
    - **优点：** 一旦获得锁，就能保证操作的正确性，不会因冲突而重试。
    - **缺点：** 锁本身有开销，且等待锁会降低并发性能。适用于**高冲突**环境。

2.  **乐观并发控制 (Optimistic):**
    - **假设：** 冲突很少发生。
    - **策略：** “先走后看”。直接在临时区域执行操作，不加锁。在事务**最后提交时**，再检查在此期间是否有其他事务修改了自己所依赖的数据。
    - **优点：** 无锁操作，并发度高。适用于**低冲突**环境。
    - **缺点：** 如果检测到冲突，必须**中止（Abort）并重试**整个事务，造成浪费。

本文重点讨论悲观策略，其核心是两阶段锁。

#### 2. 两阶段锁 (2PL) 的核心规则

2PL 将事务的生命周期划分为两个阶段，但其关键在于第二条规则：

1.  **规则一 (增长阶段 - Growing Phase):** 在访问任何数据之前，必须先获取该数据的锁。事务在此阶段可以不断获取新的锁。
2.  **规则二 (持有阶段 - Holding Phase):** 事务**在提交或中止之前，不得释放任何已经获得的锁**。一旦释放了一个锁，就不能再获取任何新的锁（严格 2PL 简化了这一点，要求所有锁在事务结束时一次性释放）。

**关键点：** “两阶段”指的不是时间上的两个阶段，而是关于锁操作的两个阶段——一个只增不减的“加锁阶段”和一个“持有/释放阶段”。核心是**在事务完成前，锁的数量只能增加或不变，不能减少**。

#### 3. 为什么必须持有锁直到事务结束？

提前释放锁会破坏隔离性，导致两种经典的错误：

1.  **破坏可序列化（Inconsistent Read / 不可重复读）:**

    - **场景：** T2 读取 X (值为 10) 并**立即释放锁** -> T1 执行，修改 X 为 11，Y 为 9 -> T2 再次获取锁并读取 Y (值为 9)。
    - **结果：** T2 看到了一个 `(X=10, Y=9)` 的“撕裂”状态，这不对应于任何一个合法的串行执行结果，违反了可序列化。

2.  **读取未提交数据（Dirty Read）:**
    - **场景：** T1 修改 X 为 11 并**立即释放锁** -> T2 读取 X (值为 11) -> T1 因某种原因**中止（Abort）**，并将对 X 的修改回滚。
    - **结果：** T2 读取并使用了一个逻辑上“从未存在过”的值（11），这严重违反了隔离性。

**结论：** “持有锁直到结束”这条规则，通过将一个事务所做的所有修改“圈禁”起来，直到其最终命运（提交或中止）被确定，才一次性地让外界可见或消失，从而保证了可序列化。

#### 4. 2PL 的代价：死锁

- **场景：** T1 获取了 X 的锁，等待 Y 的锁；同时 T2 获取了 Y 的锁，等待 X 的锁。
- **结果：** 两个事务相互等待，形成死锁循环，永远无法继续。
- **解决方案：** 数据库系统必须有**死锁检测**机制（如超时、等待图检测），并在检测到死锁时，选择一个事务作为“牺牲品”将其**中止**，以打破循环。

---

## 2PC 两阶段提交

### 核心观点

两阶段提交（Two-Phase Commit, 2PC）是解决分布式事务**原子性**问题的经典协议。其核心思想是引入一个**事务协调者（TC）**，将事务的最终决定权（提交或中止）集中于一点，并通过两个阶段的通信来同步所有**参与者（Participants）**的行为。第一阶段是**投票阶段（Voting Phase）**，协调者询问所有参与者是否**能够**提交；第二阶段是**决定阶段（Decision Phase）**，协调者根据收到的所有投票，向所有参与者广播一个**统一的、不可撤销的**最终决定（要么全部提交，要么全部中止）。这个协议强制所有参与者在做出最终行动前达成共识，从而确保了在面临局部故障或逻辑错误时，整个分布式事务的“要么全做，要么全不做”的原子性。

---

### 逻辑梳理

#### 1. 问题的本质：分布式环境下的部分失败

- **场景：** 一个转账事务需要修改服务器 S1 上的账户 X 和服务器 S2 上的账户 Y。
- **风险：**
  - **节点故障：** S1 成功修改了 X，但 S2 在此之后崩溃了，导致 Y 未被修改。
  - **逻辑错误：** S1 成功修改了 X，但 S2 发现 Y 账户不存在或余额不足，无法完成操作。
- **结果：** 事务只完成了一半，破坏了原子性，导致数据不一致（总金额发生变化）。

#### 2. 解决方案：引入协调者和两阶段提交 (2PC)

为了避免上述问题，系统引入一个中心角色——**事务协调者（TC）**，以及多个执行具体操作的**参与者（Participants）**。它们之间通过一个结构化的协议（2PC）来协同工作。

**2PC 的两个阶段：**

**阶段一：投票/准备阶段 (Phase 1: Voting/Prepare)**

1.  **执行操作：** TC 指挥各个参与者执行事务中的具体操作（如读写数据）。参与者会获取必要的锁（遵循 2PL），并将修改记录在本地的**预写日志（WAL）**中，但**不真正提交**这些修改，使其对其他事务可见。
2.  **协调者发起投票：** 当所有操作执行完毕，TC 向**所有**参与者发送一个 `PREPARE` 消息，询问：“你们准备好了吗？能否保证在任何情况下都能提交这个事务？”
3.  **参与者投票：**
    - 收到 `PREPARE` 后，参与者进行最终检查。如果它能确保完成事务（例如，所有数据都已写入持久化日志，没有逻辑错误），它就向 TC 回复 `YES`。**一旦回复 `YES`，参与者就进入了一个“承诺”状态，它必须放弃单方面中止事务的权利，只能等待 TC 的最终指令。**
    - 如果参与者因任何原因无法完成事务（如死锁、磁盘空间不足），它会回复 `NO`。

**阶段二：决定/提交阶段 (Phase 2: Decision/Commit)**

1.  **协调者做出决定：**
    - **如果 TC 收到了来自所有参与者的 `YES` 投票**，它就决定**提交（Commit）**该事务。
    - **如果 TC 收到了任何一个 `NO` 投票，或者在等待投票时超时**，它就决定**中止（Abort）**该事务。
2.  **协调者广播决定：** TC 向**所有**参与者广播最终决定——要么是 `COMMIT` 消息，要么是 `ABORT` 消息。
3.  **参与者执行决定：**
    - 收到 `COMMIT` 消息后，参与者将之前记录在日志中的修改正式应用，使其永久生效，然后释放该事务所持有的所有锁。
    - 收到 `ABORT` 消息后，参与者根据日志回滚所有修改，然后释放该事务所持有的所有锁。
4.  **完成：** 参与者向 TC 发送 `ACK` 确认消息，TC 在收到所有 `ACK` 后，认为该事务已彻底结束。

#### 3. 2PC 如何保证原子性

- **关键点：** 存在一个“**不归路点**”（Point of No Return）。对于参与者来说，是回复 `YES` 的时刻；对于协调者来说，是做出最终决定并写入自己日志的时刻。
- **统一行动：** 协议确保了在协调者做出最终决定之前，没有任何修改是永久性的。而一旦决定做出，它将被强制应用到所有参与者，无论之后发生什么（后续会讨论恢复机制）。
- **一个 `NO` 顶所有 `YES`：** 任何一个参与者的否定都会导致整个事务的中止，这完美地实现了“要么全做，要么全不做”的逻辑。

**结论：** 两阶段提交通过一个“先投票，后决定”的民主集中制过程，将多个分布式节点的命运捆绑在一起。它以增加通信延迟和引入一个中心协调者为代价，换取了在分布式环境下至关重要的原子性保证，是构建可靠分布式系统的基石之一。

---

## 故障恢复

### 核心观点

两阶段提交（2PC）协议通过引入**持久化日志（WAL）**和**超时重传机制**来应对节点崩溃和消息丢失，从而保证其原子性。其鲁棒性的关键在于定义了明确的“**不归路点**”：对参与者而言，是**回复 `YES` 之前将事务状态写入日志**；对协调者而言，是**做出最终决定（Commit/Abort）之前将决定写入日志**。然而，这种可靠性是有代价的：2PC 存在一个致命的“**阻塞窗口**”——参与者在回复 `YES` 之后、收到最终决定之前，必须无限期地等待协调者的指令，既不能单方面提交也不能中止。这个阻塞问题是 2PC 最广为人知的缺陷，它可能导致在协调者长时间故障期间，资源被锁定，系统吞吐量严重下降。

---

### 逻辑梳理

#### 1. 核心机制：用持久化日志对抗遗忘

为了在崩溃后能恢复到正确的状态，2PC 的所有角色都必须在关键时刻将自己的状态写入非易失性存储（如磁盘上的日志）。

- **参与者 (Participant) 的责任：**

  - **在回复 `YES` 之前**，必须将该事务的所有修改内容、持有的锁、以及“准备提交”（Prepared）的状态写入本地日志。
  - **为什么？** 因为一旦回复 `YES`，就做出了一个**承诺**。如果此时崩溃，重启后必须能通过日志记起这个承诺，并继续等待协调者的最终决定。如果没写日志就回复 `YES`，崩溃后就会“失忆”，可能导致其他节点已提交而它却永远无法提交，破坏原子性。

- **协调者 (Coordinator) 的责任：**
  - **在发送第一个 `COMMIT` 或 `ABORT` 消息之前**，必须将最终的决定（Commit/Abort）连同事务 ID 和参与者列表写入自己的日志。
  - **为什么？** 因为一旦发出了一个 `COMMIT`，就意味着事务的命运已被决定。如果此时崩溃，重启后必须能通过日志记起这个决定，并确保将它传达给所有参与者，防止出现部分节点提交、部分节点中止的情况。

#### 2. 应对故障场景

- **场景 1：参与者在回复 `YES` 前崩溃。**

  - **结果：** 安全。协调者会因收不到投票而超时，并决定 `Abort`。参与者重启后，因为它没有做出过承诺，可以安全地中止或忘记该事务。

- **场景 2：参与者在回复 `YES` 后、收到最终决定前崩溃。**

  - **结果：** 进入**阻塞状态**。参与者重启后，从日志中发现自己处于“Prepared”状态。它不知道最终决定是什么，所以**必须联系协调者**询问。如果协调者也挂了，它就只能无限期等待，并持有相关数据的锁。

- **场景 3：协调者在做出决定前崩溃。**

  - **结果：** 安全。协调者重启后，发现日志中没有该事务的最终决定，因此可以安全地决定 `Abort` 该事务，并通知所有参与者。

- **场景 4：协调者在做出决定后（写入日志后）崩溃。**
  - **结果：** 安全但需要恢复。协调者重启后，从日志中读取到最终决定，并向所有参与者**重新发送** `COMMIT` 或 `ABORT` 消息，直到收到所有 `ACK`。这要求参与者能够处理重复的指令（幂等性）。

#### 3. 应对消息丢失与超时

- **协调者等待投票超时：** 协调者可以安全地单方面决定 `Abort`，因为它知道还没有任何参与者收到过 `COMMIT` 指令。
- **参与者等待 `PREPARE` 消息超时：** 参与者可以安全地单方面决定 `Abort`，因为它知道自己还未做出承诺，协调者不可能决定 `Commit`。
- **参与者等待最终决定超时（最关键的场景）：**
  - **绝对不能**单方面做决定。
  - **原因：** 参与者无法区分是协调者决定了 `Abort` 但消息丢失了，还是协调者决定了 `Commit` 但消息还没到，或者协调者本身崩溃了。任何单方面的猜测都可能破坏原子性。
  - **行为：** 必须**阻塞（Block）**，不断询问协调者，直到得到最终答复。

#### 4. 协议的终结：何时可以忘记？

- **协调者：** 当收到**所有**参与者的 `ACK` 后，它确信所有人都已执行了最终决定，此时可以安全地从日志中删除该事务记录。
- **参与者：** 当执行完 `COMMIT/ABORT` 并向协调者发送 `ACK` 后，它可以安全地忘记该事务。如果之后收到协调者重发的指令，只需简单地再次回复 `ACK` 即可。

**结论：** 两阶段提交通过严格的日志记录和明确的协议状态转换，成功地在不可靠的网络和会崩溃的节点之上构建了原子性保证。但它的阿喀琉斯之踵在于协调者的单点瓶颈以及由此引发的“阻塞”问题。这个缺陷促使了后续三阶段提交（3PC）以及 Paxos/Raft 等更复杂的共识协议的诞生，它们试图在不同程度上缓解或解决这个问题。

---

## 总结

spanner：2PC over Raft

### 核心观点

两阶段提交（2PC）是实现分布式事务原子性的基础协议，但因其固有的**高延迟**（多轮通信和同步写盘）和**低可用性**（任何单点故障都可能导致系统阻塞）而声名狼藉。它与旨在通过**复制**实现**高可用**的 Raft 协议在目标和机制上完全不同：2PC 用于协调执行**不同任务**的所有参与者，而 Raft 用于确保执行**相同任务**的多数派副本保持一致。现代高性能分布式数据库（如 Spanner）的解决方案并非在两者中择一，而是将它们**组合**：将 2PC 协议中的每一个脆弱的单点组件（协调者和参与者）都替换成一个由 Raft 协议保证其高可用的复制集群，从而在宏观上构建出一个既能支持分布式原子提交，又具备高可用性的强大系统。

---

### 逻辑梳理

#### 1. 两阶段提交（2PC）的现实困境

尽管 2PC 是实现跨分片事务原子性的理论基石，但在实践中它存在三大致命缺陷：

1.  **慢（多轮通信）：** 协议包含多轮消息往返（Prepare -> Vote, Commit -> ACK），在网络延迟敏感的系统中性能低下。
2.  **慢（同步写盘）：** 协调者和参与者在协议的关键节点（决定 Commit 前、投票 Yes 前）都必须将状态同步写入磁盘日志，这引入了显著的 I/O 延迟，并在此期间锁定资源，严重影响并发吞吐量。
3.  **脆弱（阻塞与低可用性）：**
    - **阻塞：** 参与者在投票 `YES` 后，若协调者崩溃，必须无限期等待并持有锁，导致系统停滞。
    - **低可用性：** 任何一个组件（协调者或任一参与者）的故障都可能导致整个事务失败或阻塞。系统的可用性是所有组件可用性的乘积，远低于单个组件。

由于这些问题，纯粹的 2PC 只适用于小范围、高可靠的内部网络环境。

#### 2. 2PC vs. Raft：目标与机制的根本不同

表面上看，两者都有一个 Leader 和多个 Follower，但它们解决的问题截然不同。

| 特性               | 两阶段提交 (2PC)                                                  | Raft (及 Paxos)                                            |
| :----------------- | :---------------------------------------------------------------- | :--------------------------------------------------------- |
| **核心目标**       | **原子性 (Atomicity)**                                            | **高可用性 (High Availability)**                           |
| **工作模式**       | **任务分片 (Partitioning)**                                       | **状态复制 (Replication)**                                 |
| **参与者角色**     | 每个参与者执行**不同**的、独立的工作（如 S1 修改 X，S2 修改 Y）。 | 所有参与者（副本）执行**相同**的操作序列，维护一致的状态。 |
| **成功条件**       | **必须所有**参与者都成功。                                        | **只需多数派 (Majority)** 参与者成功即可。                 |
| **对故障的容忍度** | **极低**。任何一个节点失败都可能导致系统停滞。                    | **高**。可以容忍少数派节点（如 `(N-1)/2` 个）的失败。      |

**一句话总结：** 2PC 确保一群各做各事的人要么一起成功，要么一起失败；Raft 确保一群做着同样事情的人，即使有人掉队，大多数人也能保持步调一致。

#### 3. 终极解决方案：2PC over Raft

既然 2PC 保证正确性但脆弱，Raft 保证可用性但只用于复制，那么如何构建一个既正确又可用的分布式事务系统？答案是**分层组合**。

1.  **架构思想：** 将 2PC 协议中的**逻辑角色**（协调者、参与者）视为服务。
2.  **实现方式：** 用 Raft 将每个服务本身打造成一个高可用的复制状态机。
    - **高可用协调者：** 不再是一个单点服务器，而是一个由多个副本组成的 Raft 集群。如果 Leader 崩溃，Raft 会自动选举出新的 Leader 接管协调工作。
    - **高可用参与者（分片）：** 每个数据分片也不再是单个服务器，而是一个独立的 Raft 集群，负责存储和操作该分片的数据。
3.  **运行流程：** 2PC 协议在这些**Raft 集群之间**运行。协调者集群的 Leader 向参与者集群的 Leader 发送 `PREPARE` 消息。即使在协议执行过程中有单个服务器崩溃，Raft 机制也能确保其所属的逻辑角色（协调者或参与者）作为一个整体持续可用，从而避免了 2PC 的阻塞和单点故障问题。

**结论：** 这种“2PC over Raft”的架构，通过在宏观层面运行 2PC 协议来保证跨分片的**原子性**，同时在微观层面利用 Raft 协议来保证每个组件的**高可用性**，最终实现了两全其美，是现代分布式数据库（如 Google Spanner、TiDB 等）处理分布式事务的核心思想。

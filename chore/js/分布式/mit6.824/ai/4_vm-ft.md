## 复制(Replication)

### 核心观点

复制（Replication）是实现高可用性的核心技术，但它是一个**有明确适用范围和前提条件的工具，而非万能药**。它主要用于应对**独立的、可被转化为“停止服务”的故障（Fail-stop）**，但对软件 Bug 和关联性灾难无能为力，并且其高昂的成本决定了它并非适用于所有场景。

---

### 逻辑梳理

#### 1. 复制能解决什么问题？—— Fail-Stop 故障

- **定义：** Fail-stop 指的是系统组件（如服务器）在遇到问题时，会**干净利落地停止工作**，而不会产生错误的计算结果。
- **典型例子：**
  - 电源被拔掉。
  - 网线被拔掉（从外部看，等同于停止服务）。
  - 硬件过热导致 CPU 自动关机。
- **延伸：** 很多非 Fail-stop 错误可以被**检测并转换**为 Fail-stop 错误。
  - **软件层面：** 无关软件导致的操作系统崩溃（Kernel Panic），对你的服务而言就是一次 Fail-stop。
  - **硬件/网络层面：** 网络包传输错误被**校验和（Checksum）**检测出来后丢弃；磁盘读取错误被**纠错码（ECC）**检测出来后报错。这些机制将“数据算错”的风险转换为了“服务中止”的 Fail-stop 模式，从而使复制机制能够生效。

#### 2. 复制解决不了什么问题？—— 系统性与关联性故障

- **软件 Bug 和硬件设计缺陷：**
  - **原因：** 复制的是软件本身。如果软件逻辑有 Bug，那么所有副本都会以完全相同的方式计算出相同的错误结果。复制无法抵御这种系统性的、内生的错误。
- **关联性故障（Correlated Failures）：**
  - **前提：** 复制机制有效的前提是**故障的独立性**，即一个副本的故障不会增加另一个副本故障的概率。
  - **反面例子（关联故障）：**
    1.  **同批次硬件缺陷：** 从同一厂商购买的大量服务器可能存在相同的制造缺陷（如散热问题），导致它们可能在相近的时间内集体发生故障。
    2.  **共享基础设施灾难：** 将所有副本放在同一个机房，一旦发生地震、火灾、大面积停电，所有副本会同时失效。
  - **对策：** 物理隔离。将副本部署在不同的城市、不同的电网，以确保故障域相互独立。

#### 3. 是否值得复制？—— 一个经济问题

- **核心权衡：** 复制的成本（2 倍、3 倍甚至更多的硬件、电力、维护开销） vs. 服务中断造成的损失。
- **决策依据：** 这不是一个技术问题，而是一个**经济和业务问题**。
  - **值得：** 对于银行系统、核心电商交易等服务，停机一分钟的损失可能远超增加一台服务器的成本，因此复制是必须的。
  - **不值得：** 对于个人博客、课程网站等非关键服务，宕机的影响很小，为其投入双倍成本进行复制可能是不划算的。

---

## 状态转移与复制状态机

### 核心观点

保持副本同步主要有两种对立的策略：**状态转移（State Transfer）**和**复制状态机（Replicated State Machine）**。状态转移通过暴力复制**整个系统状态**来实现同步，虽然简单但开销巨大；而复制状态机则巧妙地只复制**外部输入**，虽然高效但实现复杂且脆弱。VMware FT 的独特之处在于，它将“复制状态机”模型应用到了极致的**硬件层面**，从而像“魔法棒”一样，能为任何现有软件提供容错能力，无需修改软件本身。

---

### 逻辑梳理

#### 1. 两种核心复制策略的对决

- **状态转移 (State Transfer) - “发送结果”**

  - **工作方式：** Primary 定期将其**完整的当前状态**（例如，整个内存的快照）打包发送给 Backup。
  - **优点：**
    - **简单粗暴：** 逻辑非常直接。
    - **健壮：** 对并行计算、多核等不确定性行为免疫，因为它只关心最终状态，不关心过程。
  - **缺点：**
    - **开销巨大：** 传输的数据量极大（可能是 GB 级别），对网络带宽是巨大考验。

- **复制状态机 (Replicated State Machine, RSM) - “重放过程”**
  - **工作方式：** 假设 Primary 和 Backup 从完全相同的初始状态开始，Primary 只将导致状态改变的**外部输入/事件**（如网络请求、中断）按顺序发送给 Backup。Backup 按相同顺序执行这些输入，从而理论上能与 Primary 保持状态一致。
  - **优点：**
    - **高效：** 传输的数据量通常很小（操作指令远小于整个内存）。
  - **缺点：**
    - **复杂且脆弱：** 对**确定性（Determinism）**有严格要求。

#### 2. 复制状态机（RSM）的致命弱点：不确定性 (Non-Determinism)

RSM 成立的前提是“相同输入+相同顺序=相同结果”，但现实中很多操作会破坏这个前提：

- **指令层面的不确定性：** 获取当前时间、读取 CPU 序列号等指令在不同机器上会产生不同结果。
  - **VMware FT 的解决方案：** Primary 执行这些指令，然后将**结果**发送给 Backup。Backup 则跳过实际执行，直接使用 Primary 发来的结果。
- **架构层面的不确定性：** 在多核处理器上，不同核心间的交互、内存访问的竞争顺序是高度不确定的。即使运行相同的程序，两个多核系统的内部状态演化路径也几乎不可能完全一致。
  - **后果：** 这使得在多核系统上实现纯粹的 RSM 变得极其困难，这也是为什么后来的多核 FT 系统倾向于回归到更健壮的**状态转移**模型。

#### 3. 复制的层次：应用层 vs. 机器层

这是一个关于“复制什么”的问题，决定了系统的通用性和效率。

- **应用层复制 (Application-Level) - 如 GFS**

  - **做什么：** 只复制应用自己定义的、有意义的状态（如 GFS 的 Chunk）。
  - **优点：** **高效**，因为只复制必要的东西。
  - **缺点：** **不通用**，复制逻辑必须深度集成在每个应用程序的设计中。

- **机器层复制 (Machine-Level) - 如 VMware FT**
  - **做什么：** 复制整台虚拟机的**所有底层状态**（CPU 寄存器、内存、设备状态）。
  - **优点：** **极其通用**。它可以让任何能在其上运行的软件（无需源码、无需修改）瞬间获得容错能力。
  - **缺点：** **效率较低**，因为它需要精确模拟和同步每一个底层细节，比如中断必须在两个副本的完全相同指令点上发生。

**结论：** VMware FT 选择了一条艰难但“神奇”的道路。它通过在硬件层面实现一个极其精密的复制状态机，以牺牲部分效率为代价，换来了前所未有的通用性，解决了“如何为已有系统添加容错”这一经典难题。

---

## VMFT 工作原理

### 核心观点

VMware FT 的工作原理是在**虚拟机监控器（VMM/Hypervisor）**层面实现了一个极致的**复制状态机**。VMM 扮演着“上帝”的角色，它拦截所有进入 Primary 虚拟机的**外部事件**（如网络包、中断），将这些事件作为“日志”精确地“喂”给 Backup 虚拟机进行重放，同时屏蔽 Backup 的所有对外输出。通过这种方式，它在硬件层面强制实现了两个虚拟机的同步，而虚拟机内部的操作系统和应用程序对此毫不知情。

---

### 逻辑梳理

整个过程可以分为**正常运行**和**故障切换**两个阶段。

#### 1. 正常运行：输入拦截与重放 (Input Interception & Replay)

1.  **输入到达 Primary：** 客户端的一个网络请求到达 Primary 所在的物理服务器。

2.  **VMM 拦截与分发：** Primary 的 VMM 捕获这个请求（这是一个外部事件）。它会做两件事：

    - **注入 Primary：** 将该请求作为一个虚拟中断注入到 Primary 虚拟机中，让其正常处理。
    - **转发 Backup：** 将这个请求原封不动地通过一个专用的“日志通道”（Log Channel）发送给 Backup 所在的 VMM。

3.  **Backup 重放：** Backup 的 VMM 收到日志通道传来的请求后，以完全相同的方式将其作为虚拟中断注入到 Backup 虚拟机中。

4.  **同步执行：** 由于 Primary 和 Backup 从相同的状态开始，并接收了完全相同的输入序列（由 VMM 保证），它们会执行完全相同的指令序列，最终达到相同的内部状态。

5.  **输出控制：**
    - **Primary 输出：** Primary 处理完请求后生成一个回复包。它的 VMM 捕获这个输出，并将其正常发送给客户端。
    - **Backup 输出：** Backup 同样会生成一个一模一样的回复包。但它的 VMM 知道自己是“备胎”，会**拦截并直接丢弃**这个输出包，防止对外界产生干扰。

**关键点：**

- **VMM 是核心：** 整个同步逻辑完全由 VMM 实现，对虚拟机内的操作系统和应用完全透明。
- **日志通道是命脉：** 所有导致状态变化的外部事件（网络包、定时器中断、随机数结果等）都通过日志通道从 Primary 发往 Backup。

#### 2. 故障切换：备胎上位 (Failover)

1.  **故障检测：** Primary 物理机或其 VMM 突然宕机。最直接的后果是，**日志通道沉默了**。Backup 的 VMM 会持续收到来自 Primary 的心跳式日志（如定时器中断），一旦这些日志在预设时间内（如 1 秒）没有到达，Backup 的 VMM 就判定 Primary 已死。

2.  **Backup 上线 (Go Live)：** Backup 的 VMM 立即执行“上位”操作：
    - **停止等待：** 不再依赖日志通道的输入，开始自由接收外部世界的真实网络请求。
    - **身份接管：** 通过网络技术（如发送 GARP 报文）宣告自己拥有原 Primary 的网络身份（IP 和 MAC 地址），将原本发往 Primary 的流量引导到自己这里。
    - **开启输出：** 不再丢弃虚拟机的输出，而是将其正常发送给客户端。

**结论：** Backup 在一瞬间从一个只听不说的“影子”变成了一个功能完全的、新的 Primary，从而在对客户端几乎无感的情况下接管了服务。VMware FT 的精妙之处在于，它将分布式系统中最复杂的容错逻辑，封装在了 V-M-M 这个底层基础设施中，从而为上层任意软件提供了“即插即用”的高可用能力。

---

## Non-Deterministic Events

### 核心观点

VMware FT 的核心挑战是处理**非确定性（Non-Deterministic）事件**，即那些无法仅凭当前内存状态预测其结果的操作。它的解决方案是：由 **Primary 独占地执行**这些非确定性事件，然后将其**发生的时间点**和**产生的结果**打包成日志，通过日志通道（Log Channel）发送给 Backup。Backup 则**完全放弃**自己处理这些事件的能力，严格按照日志的指示，在完全相同的时刻“伪造”出完全相同的结果，从而强制实现与 Primary 的同步。

---

### 逻辑梳理

#### 1. 什么是非确定性事件？

这是破坏“相同输入=相同输出”这一理想模型的“捣乱分子”，主要分为两类：

- **外部输入（Asynchronous Inputs）：**

  - **事件：** 客户端的网络请求、定时器中断等。
  - **不确定性来源：**
    1.  **内容：** 请求的内容是不可预测的。
    2.  **时机：** 请求到达的**精确时刻**是随机的。这个时刻决定了中断会在虚拟机执行到**哪一条指令**时被触发，从而影响后续的执行路径。

- **“怪异”指令（Weird Instructions）：**
  - **事件：** 某些特殊的 CPU 指令。
  - **不确定性来源：** 指令的输出不依赖于内存和寄存器输入，而是与外部或物理状态相关。
    - `RDTSC`：读取时间戳计数器，在不同时刻、不同机器上结果不同。
    - `RDRAND`：获取硬件随机数。
    - `CPUID`：获取 CPU 的唯一标识。

#### 2. VMware FT 的统一解决方案：捕获、记录、重放

VMM 采用“三步走”策略来驯服所有非确定性事件：

1.  **捕获（Capture on Primary）：**

    - Primary 的 VMM 拦截所有非确定性事件。
    - 当事件发生时，VMM 暂停 Primary 虚拟机，并记录下当前执行到的**指令序号**（一个从启动开始计数的全局指令计数器）。
    - Primary 正常处理该事件（例如，执行“怪异”指令并获得结果，或接收网络包）。

2.  **记录与发送（Log & Send）：**

    - VMM 将该事件打包成一个**日志条目（Log Entry）**，发送到日志通道。
    - **日志条目内容：**
      - **指令序号：** 事件在 Primary 上发生的精确位置。
      - **事件类型：** 是网络包，还是定时器中断，还是某个“怪异”指令。
      - **事件数据：**
        - 如果是网络包，数据就是包的完整内容。
        - 如果是“怪异”指令，数据就是该指令在 Primary 上执行后得到的**结果**。

3.  **重放（Replay on Backup）：**
    - Backup 的 VMM 收到日志条目。
    - 它会精确控制 Backup 虚拟机执行到日志中指定的**指令序号**处，然后暂停。
    - VMM 根据日志类型，在 Backup 虚拟机中**伪造**一个一模一样的事件：
      - 如果是网络包，VMM 就将包内容拷贝到 Backup 内存，并模拟一个网卡中断。
      - 如果是“怪异”指令，VMM 会**拦截** Backup 对该指令的执行，直接将日志中带来的**结果**喂给它。
    - 通过这种方式，Backup 在完全相同的逻辑时间点，看到了完全相同的内容或结果。

#### 3. 关键机制与保证

- **指令序号是同步的“节拍器”：** 它是保证事件在两个副本上精确对齐的关键。这需要特殊的硬件支持（指令计数器和按指令数中断的能力）。
- **Backup 永远落后（Never Ahead）：** 为防止 Backup 执行“超前”于 Primary，导致日志到达时为时已晚，VMware FT 强制规定：Backup 只有在日志缓冲区中有待处理的日志时才能执行，并且其执行的指令序号永远不能超过当前日志条目中记录的序号。这保证了 Backup 始终是 Primary 的一个“滞后”的影子。
- **Bounce Buffer 机制：** 为控制网络包到达的时机，所有入站网络包先由 VMM 接收（进入 Bounce Buffer），再由 VMM 在精确的指令点注入虚拟机，而不是让网卡直接 DMA 到虚拟机内存。

---

## 输出控制

### 核心观点

VMware FT 的**输出规则（Output Rule）**是其保证外部一致性的关键防线。该规则的核心是：**Primary 在向外界发送任何输出（如对客户端的响应）之前，必须先暂停，并等待 Backup 确认收到了导致该输出的那个输入事件的日志**。这个看似简单的规则，通过强制“Backup 的认知”永远领先于“客户端的认知”，从根本上杜绝了因故障切换而导致的状态回滚等诡异现象。

---

### 逻辑梳理

#### 1. 问题所在：最坏的故障场景

如果不对输出加以控制，会发生什么？

1.  **初始状态：** Primary 和 Backup 的计数器都是 10。
2.  **Primary 处理请求：** Primary 收到“自增”请求，将计数器更新为 11，并**立即**生成响应“结果是 11”发送给客户端。
3.  **灾难发生：** 在 Primary 将“自增”请求的日志发送给 Backup **之前**（或日志在网络中丢失），Primary 突然崩溃。
4.  **状态分歧：**
    - **客户端视角：** 它收到了响应，认为计数器已经是 11。
    - **Backup 视角：** 它从未收到“自增”请求的日志，其内部计数器仍然是 10。
5.  **Backup 上位：** Backup 接管服务后，客户端再次发送“自增”请求。新的 Primary（原 Backup）将计数器从 10 更新到 11，并返回响应“结果是 11”。
6.  **外部不一致：** 客户端连续两次执行“自增”操作，却两次都收到了结果 11。这破坏了服务的外部一致性，对于一个“黑盒”的容错系统来说是不可接受的。

#### 2. 解决方案：输出规则 (The Output Rule)

为了防止上述灾难，VMware FT 引入了严格的输出控制流程：

1.  **输入到达：** 客户端的“自增”请求到达 Primary 的 VMM。
2.  **日志先行：** Primary 的 VMM **立即**将该请求打包成日志，通过日志通道发往 Backup 的 VMM。
3.  **Primary 执行，但输出被“扣留”：** Primary 的 VMM 将请求注入 Primary 虚拟机。虚拟机执行操作（计数器变为 11），并生成响应包“结果是 11”。但这个响应包被 Primary 的 VMM **拦截并扣留**，不准发出。
4.  **Backup 确认：** Backup 的 VMM 收到日志后，**立即**向 Primary 的 VMM 发送一个 ACK（确认收到），此时 Backup 虚拟机**甚至还未开始处理**这个日志。
5.  **释放输出：** Primary 的 VMM 收到来自 Backup 的 ACK 后，才将被扣留的响应包释放，发送给客户端。

#### 3. 输出规则如何解决问题？

- **保证因果关系：** 该规则强制建立了一个因果链：**Backup 知道输入 -> Primary 才能输出**。
- **杜绝状态回滚：** 如果在日志发送过程中 Primary 崩溃，由于 Backup 未发送 ACK，Primary 的 VMM 绝不会释放输出。因此，客户端根本不会收到响应，它只会认为请求超时了。从客户端视角看，操作从未发生，也就不会出现状态回滚的矛盾。
- **核心思想：** 确保任何外部可见的状态变化（客户端收到的响应），其对应的状态变更事件（输入日志）**必然已经安全地持久化在 Backup 中**。

#### 4. 代价：性能的牺牲

- **同步等待的延迟：** 这个“等待 ACK”的过程引入了至少一个网络来回（RTT）的延迟。这意味着，即使 Primary 和 Backup 在相邻机架，每个需要响应的操作也凭空增加了零点几毫秒的延迟。如果跨城市部署，延迟会增加到数毫秒甚至更多。
- **对吞吐量的影响：** 虽然可以通过高并发来掩盖部分延迟，但对于低延迟敏感的应用，这种同步等待是性能的硬伤。这也是为什么很多系统选择在更灵活的**应用层**实现复制，以避免这种对每个操作都一刀切的同步开销。

---

## 重复输出

### 核心观点

即使有严格的输出规则，VMware FT 在故障切换的瞬间仍然可能产生**重复的输出**。这个问题并非 VMware FT 的设计缺陷，而是几乎所有主备切换系统的固有难题。幸运的是，这个问题可以被**客户端的协议栈（主要是 TCP）优雅地解决**，因为 TCP 内置了对重复报文的检测和丢弃机制，从而对上层应用屏蔽了底层的这次“小瑕疵”。

---

### 逻辑梳理

#### 1. 问题场景：重复输出是如何产生的？

这个场景发生在“输出规则”的临界点之后：

1.  **Primary 完成同步并发送输出：** Primary 遵循输出规则，在收到 Backup 的 ACK 后，将响应报文（例如，TCP 序列号为 `S`）发送给了客户端。
2.  **灾难发生：** 在发送完响应报文之后，但在 Backup 虚拟机**实际执行**完对应的日志之前，Primary 崩溃了。
3.  **Backup 的状态：**
    - Backup 的 VMM 已经确认收到了导致该输出的日志。
    - Backup 虚拟机本身还未执行该日志。
4.  **Backup 上位并重放：** Backup 检测到 Primary 死亡后，开始“上线（Go Live）”。上线的**第一步**是处理完其日志缓冲区中所有待办事项，以确保自己的状态追上 Primary 崩溃前的最终状态。
5.  **重复输出产生：** Backup 执行了最后那条日志，这导致它也生成了一个**一模一样**的响应报文（因为内存状态完全相同，所以 TCP 序列号也是 `S`）。由于此时它已经“上线”，这个响应报文被正常发送给了客户端。
6.  **结果：** 客户端先后收到了两个 TCP 序列号同为 `S` 的响应报文。

#### 2. 解决方案：利用 TCP 的幂等性

- **TCP 的角色：** TCP 协议栈被设计用来处理不可靠网络中的各种问题，其中就包括**报文重复**。
- **如何工作：** 当客户端的 TCP 协议栈收到第二个序列号为 `S` 的报文时，它会识别出这是一个已经成功接收过的重复报文。
- **最终效果：** 客户端的 TCP 协议栈会**默默地丢弃**这个重复的报文，**不会**将其递交给上层的应用程序。因此，对于应用程序来说，它只收到了一次响应，底层的这次重复被完全屏蔽了。

#### 3. 关键洞察与引申

- **状态完全一致的威力：** VMware FT 能够依赖 TCP 解决这个问题，其根本原因在于它实现了**精确到比特级别**的状态复制。这保证了 Primary 和 Backup 生成的 TCP 报文（包括源/目的 IP、端口、序列号等）是**完全相同**的，从而使得客户端的 TCP 栈能够正确识别其为重复。
- **重复是切换的固有属性：** 几乎所有主备切换系统都无法完美避免在切换瞬间产生重复。这是一个普遍性的难题。
- **客户端的责任：** 这也揭示了一个更广泛的原则：**在一个分布式系统中，客户端需要具备处理重复响应的能力**。VMware FT 巧妙地将这个责任“外包”给了通用的 TCP 协议，但如果系统不使用 TCP，则需要在应用层自行实现类似的幂等性或重复检测机制（例如，通过请求 ID）。
- **网络身份接管：** 为了让 Backup 能够无缝替换 Primary，它必须接管 Primary 的网络身份（IP 和 MAC 地址）。在以太网环境中，这通常通过 Backup 在上线时发送一个**免费 ARP（GARP）**报文来实现，强制网络交换机更新其 MAC 地址表，将发往该 MAC 地址的流量重定向到新的物理端口。

---

## Test-and-Set 服务

### 核心观点

为了解决分布式系统中最危险的**“脑裂”（Split-Brain）**问题，VMware FT 引入了一个外部的、权威的**仲裁者（Arbitrator）**——**Test-and-Set 服务**。该服务本质上是一个**分布式锁**，任何副本在宣告自己成为主节点（Go Live）之前，都必须先赢得对这个锁的争夺。这确保了无论发生何种故障（机器宕机或网络分区），在任何时刻都绝对只有一个副本能够成为活跃的 Primary。

---

### 逻辑梳理

#### 1. 问题：无法区分的故障与“脑裂”

- **核心困境：** 在分布式系统中，一个节点无法区分“对方真的宕机了”和“我与对方之间的网络断了”。这被称为**故障检测的模糊性**。
- **最坏场景（脑裂）：**
  1.  Primary 和 Backup 之间的网络发生分区。
  2.  Primary 看不到 Backup，认为 Backup 已死。
  3.  Backup 看不到 Primary，认为 Primary 已死。
  4.  两者都认为自己有责任接管服务，于是都尝试“上线（Go Live）”。
  5.  结果：系统出现了**两个活跃的 Primary**，它们各自接收客户端请求，导致数据状态彻底分裂，造成灾难性后果。

#### 2. 解决方案：引入第三方权威仲裁

- **思路：** 当两个节点无法就“谁是主”达成一致时，引入一个它们都信任的、唯一的第三方来做出裁决。
- **实现：Test-and-Set 服务**
  - 这是一个独立于 Primary 和 Backup 的网络服务。
  - 它维护一个简单的原子标志位（或锁）。

#### 3. 工作机制：一场争夺锁的竞赛

1.  **触发条件：** 任何一个副本（Primary 或 Backup）只要认为对方已失联，并想成为唯一的活跃主节点时。
2.  **获取许可：** 该副本必须向 Test-and-Set 服务发送一个 `test-and-set` 请求。
3.  **原子操作：** Test-and-Set 服务会原子性地执行：
    - 检查标志位当前的值。
    - 将标志位设置为“已占用”。
    - 返回标志位的**旧值**。
4.  **裁决结果：**
    - **胜利者：** 第一个到达的副本会收到返回值“未占用”（例如 0）。它成功获得了锁，被授权成为 Primary。
    - **失败者：** 稍后到达的副本会收到返回值“已占用”（例如 1）。它获取锁失败，被禁止上线，通常会选择自我关闭或进入待命状态。

#### 4. 新的问题：仲裁者自身的可靠性

- **单点故障：** 这个设计将“脑裂”的风险转移了，但 Test-and-Set 服务本身变成了一个新的**单点故障（Single-Point-of-Failure）**。如果这个仲裁服务宕机了，那么整个系统的故障切换机制就瘫痪了。
- **递归的解决方案：** 正如文中所指出的，在现实世界中，这个 Test-and-Set 服务本身**也必须是一个高可用的、复制的**服务。这引出了一个递归问题：**一个容错系统依赖于另一个容错系统来解决其一致性问题。这正是更高级的共识算法（如 Paxos 或 Raft，将在后续课程中学习）要解决的核心问题。**

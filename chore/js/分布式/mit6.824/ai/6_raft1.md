## 脑裂(Split Brain)

https://mit-public-courses-cn-translatio.gitbook.io/mit6-824/lecture-06-raft1/6.1-split-brain

### 核心观点

脑裂（Split-Brain）是构建**简单**多副本系统时固有的、灾难性的风险。它源于一个无法解决的困境：在网络分区面前，如果为了容错而允许系统在只有部分副本响应时继续工作，就必然会产生多个独立的“大脑”，从而彻底破坏系统的一致性。为了避免脑裂，系统必须依赖一个**单一的决策点**，但这又会引入单点故障。

---

### 逻辑梳理

#### 1. 问题的根源：从单点决策到多副本决策的演进

- **观察：** 之前的所有系统（MapReduce, GFS, VMware FT）虽然都使用了复制技术，但最终都依赖一个**单点决策者**（Master 节点、Test-and-Set 服务）来避免混乱。
- **优点：** 单点决策者不可能与自己产生分歧，天然避免了脑裂。
- **缺点：** 这个决策者本身成为了**单点故障（Single Point of Failure）**。
- **自然演进：** 为了消除这个单点故障，我们很自然地会想到：把这个决策者也做成多副本的。然而，这恰恰打开了通往“脑裂”的潘多拉魔盒。

#### 2. 两副本决策系统的“两难困境”

当我们尝试用两个副本（S1, S2）来构建一个容错的决策服务（如 Test-and-Set 服务）时，会立即面临一个无法调和的矛盾：

- **选择一：要求两个副本都响应才算成功。**

  - **结果：** 获得了**强一致性**，S1 和 S2 永远不会分裂。
  - **代价：** 失去了**容错性**。任何一个副本宕机或网络不通，整个系统就瘫痪了，这比单点系统更脆弱。

- **选择二：只要求一个副本响应就算成功。**
  - **结果：** 获得了**容错性**，系统可以在一个副本失效时继续服务。
  - **代价：** 必然导致**脑裂**。

#### 3. 脑裂的经典复现场景

1.  **网络分区：** 网络发生故障，将系统分割成两个孤岛。
    - 孤岛 A：客户端 C1 和服务器 S1。
    - 孤岛 B：客户端 C2 和服务器 S2。
2.  **各自为政：**
    - 在孤岛 A，C1 无法联系到 S2，根据“单副本响应即可”的规则，它向 S1 发送请求。S1 状态从 0 变为 1，并返回成功。C1 认为自己成为了主节点。
    - 在孤岛 B，C2 无法联系到 S1，同样根据规则，它向 S2 发送请求。S2 状态也从 0 变为 1，并返回成功。C2 也认为自己成为了主节点。
3.  **灾难发生：** 系统中同时出现了两个主节点（C1 和 C2），它们各自独立运行，状态开始分歧，整个系统的一致性被彻底破坏。

#### 4. 早期的（不完美的）解决方案

在现代共识算法出现之前，人们只能用昂贵或低效的方法来规避这个问题：

- **假设网络不出错：** 投入巨资建设一个极其可靠、物理上得到良好保护的网络（类似 CPU 和内存总线），从物理上杜绝网络分区的可能性。
- **人工仲裁：** 系统在遇到不确定情况时（如只有一个副本响应），完全停止自动切换，并立即报警。由运维人员**手动**介入，去机房确认故障，并手动指定一个主节点。这本质上是把“人”当作了那个最终的、虽然响应缓慢但不会“脑裂”的单点决策者。

---

## Majority Vote（过半票决）

### 核心观点

**过半票决（Majority Vote）**是现代分布式系统解决“脑裂”问题的基石。其核心思想是：通过使用**奇数个服务器**，并规定任何关键操作都必须获得**超过半数（Quorum）**服务器的批准，来从数学上保证**不可能同时存在两个都能独立做决策的网络分区**。这打破了“两副本系统”的对称性僵局，为实现自动故障切换提供了理论基础。

---

### 逻辑梳理

#### 1. 打破对称性：从偶数到奇数

- **两副本的困境：** 当两个副本的系统发生网络分区时，会形成两个完全**对称**的孤岛（1 vs 1）。由于双方“势均力敌”，且运行相同逻辑，它们会同时尝试成为主节点，导致脑裂。
- **奇数副本的优势：** 当一个拥有奇数（如 3 个）副本的系统发生分区时，形成的两个孤岛必然是**不对称**的（例如 2 vs 1）。这种不对称性是避免脑裂的关键。

#### 2. 核心规则：过半票决 (Majority Vote)

- **规则定义：** 任何需要达成共识的关键操作（如选举 Leader、提交日志），都必须获得一个**多数派（Quorum）**的批准。
- **多数派的计算：** 对于一个总数为 `N` 的系统，多数派的大小是 `floor(N/2) + 1`。
  - 3 个副本的系统，多数派是 2。
  - 5 个副本的系统，多数派是 3。
- **容错能力：** 一个拥有 `2F + 1` 个副本的系统，可以容忍 `F` 个副本的故障。因为即使 `F` 个副本失效，剩下的 `F + 1` 个副本仍然足以构成一个多数派。

#### 3. 过半票决如何防止脑裂？

- **唯一多数派原则：** 这是最直接的保证。由于两个不相交的集合的成员数之和不可能都超过总数的一半，因此在一个系统中，**在任何时刻，最多只能有一个分区能够形成多数派**。

  - 在 3 副本系统中，一个分区有 2 个副本（是多数派），另一个必然只有 1 个（不是多数派）。那个少数派分区因为凑不齐票数，无法执行任何关键操作，从而被有效地“冻结”了。

- **多数派交集保证（Quorum Intersection）：** 这是更深层次、对算法正确性至关重要的保证。**任意两个多数派集合，必然至少有一个重叠的成员。**
  - **为什么重要？** 这个特性保证了信息的连续性。
  - **应用在 Raft 中：**
    1.  **Leader 选举：** 一个新 Leader 必须获得一个多数派的选票。这个多数派必然与旧 Leader 提交最后一条日志时所依赖的多数派有交集。这意味着，新 Leader 的选举过程中，**至少能接触到一个拥有最新日志的副本**。
    2.  **信息传递：** 通过这个重叠的成员，旧 Leader 的信息（如任期号、已提交的日志）可以被新 Leader 获知。这确保了新 Leader 不会推翻已经被确认的决策，从而维护了系统状态的线性演进。

#### 4. 历史与影响

- 过半票决的思想是 Paxos 和 Viewstamped Replication (VSR) 等早期共识算法的核心，而 Raft 算法正是这些思想的现代简化和实现。
- 这一突破使得构建能够**自动、安全地**进行故障切换的分布式系统成为可能，彻底改变了人们对容错系统的设计思路。

---

## Raft 初探

### 核心观点

Raft 是一个以**库（Library）**形式存在的共识模块，它与上层应用程序（如 Key-Value 数据库）协同工作，将一个单机服务改造为高可用的多副本服务。其核心工作流程是：Leader 节点的 Raft 模块负责将客户端请求作为**日志条目**复制到**过半**的副本上；只有当日志被成功复制后，Raft 模块才会通知**所有**副本的应用程序去**按顺序执行**该日志代表的操作，从而保证所有副本状态的一致性。

---

### 逻辑梳理

#### 1. 软件架构：两层结构

在一个基于 Raft 的服务器节点上，软件被清晰地分为两层：

- **上层：应用程序（Application）**
  - **职责：** 实现具体的业务逻辑（如 Key-Value 存储），处理客户端请求。
  - **状态：** 维护业务自身的状态（如 Key-Value 表）。
- **下层：Raft 库（Raft Library）**
  - **职责：** 实现共识算法，负责在多个副本间同步操作日志，处理 Leader 选举和故障切换。
  - **状态：** 维护一个持久化的、顺序的**操作日志（Log）**。

#### 2. 一个写请求（Put）的完整生命周期

这个过程揭示了 Raft 的核心机制——**先复制，后执行**。

1.  **请求到达 Leader：** 客户端将一个 `Put(key, value)` 请求发送给当前 Raft 集群的 Leader 节点。

2.  **应用层委托 Raft：** Leader 节点的**应用程序**收到请求后，**并不立即执行**。它将这个 `Put` 操作作为一个“命令”向下传递给本机的 **Raft 库**，请求 Raft 将这个命令同步到所有副本。

3.  **Raft 复制日志（Replication）：**

    - Leader 的 Raft 库将该命令包装成一个新的**日志条目（Log Entry）**，并追加到自己的日志末尾。
    - 接着，Leader 并发地将这个新的日志条目发送给所有 Follower 节点。

4.  **达成共识（Consensus）：**

    - Follower 收到日志条目后，将其存入自己的日志，并向 Leader 回复一个确认消息。
    - 当 Leader 收到**超过半数**（Majority）节点的确认后，它就认为这个日志条目已经被**“提交”（Committed）**了。这意味着该操作已经安全地持久化，不会因后续的故障而丢失。

5.  **Raft 通知应用层执行（Apply）：**

    - 一旦日志条目被提交，Leader 的 Raft 库就会通过一个**向上调用（upcall）**通知其上层的**应用程序**：“日志 X 已经提交，你可以执行它了。”
    - 同时，Leader 在下一次心跳或日志同步时，会通知所有 Follower 节点：“日志 X 已经提交”。Follower 的 Raft 库收到通知后，也会同样地向上通知它们各自的应用程序去执行日志 X。

6.  **所有副本执行操作：**

    - 所有节点的应用程序，都按照 Raft 日志的**严格顺序**，依次执行被提交的操作。
    - 在本例中，所有副本的 Key-Value 数据库都会执行 `Put(key, value)` 操作，更新自己的状态。由于执行顺序完全一致，它们的状态也保持一致。

7.  **响应客户端：** Leader 节点的应用程序在执行完操作后，最终向客户端返回响应。

**关键点：**

- **解耦：** Raft 负责“就操作顺序达成一致”，而应用程序负责“执行这些达成一致的操作”。
- **日志是唯一真相：** 应用程序的状态是 Raft 日志中所有已提交命令的确定性执行结果。
- **过半保证安全，全部执行保证一致：** 只需要**过半**节点确认复制，操作就可被视为安全（Committed）；但为了状态一致，**所有**节点最终都必须按顺序执行这些已提交的操作。

---

## Log 同步时序

### 核心观点

Raft 的日志同步是一个**两阶段**的过程。**第一阶段**是同步的，Leader 将日志复制到**过半**节点以达成“提交”（Commit）共识，完成后即可**立即响应客户端**。**第二阶段**是异步的，Leader 后续通过**捎带（piggyback）**的方式，将“已提交”的状态通知给所有 Follower，以便它们最终将操作应用到自己的状态机上。客户端的延迟仅取决于第一阶段。

---

### 逻辑梳理

以下是基于时序图的一个写请求的完整分解：

#### 阶段一：达成共识并响应客户端（同步过程）

1.  **请求到达与日志分发：**

    - 客户端向 Leader (S1) 发送请求。
    - S1 的 Raft 模块将该请求作为新的日志条目，通过 `AppendEntries` RPC 并发地发送给所有 Follower (S2, S3)。

2.  **等待多数派确认：**

    - S1 等待来自 Follower 的响应。
    - S2 率先响应，表示已成功将日志条目写入本地日志。

3.  **日志“提交”与执行：**

    - 此时，S1 已经收到了**过半**（它自己 + S2，共 2 个，超过 3 的一半）节点的确认。
    - S1 的 Raft 模块现在认为该日志条目是**“已提交”（Committed）**的。这意味着该条目已经安全，不会丢失。
    - S1 的 Raft 模块通知其上层应用程序可以执行该命令。

4.  **响应客户端：**
    - S1 的应用程序执行命令后，立即向客户端返回结果。
    - **关键点：** 客户端的等待到此结束。它无需等待所有节点都完成复制或执行。

#### 阶段二：状态应用与传播（异步过程）

5.  **Follower 的困惑：**

    - 此时，S2 虽然已经存储了日志条目，但它并**不知道**该条目是否已被 Leader 提交。它只是收到了一个请求，但无法确定 Leader 是否已获得多数派确认。因此，S2 **不能**在此时将操作应用到自己的状态机。

6.  **提交状态的传播（Piggybacking）：**

    - Raft 中没有一个专门的“我已提交”的通知消息。
    - 取而代之的是，当 S1 下一次发送 `AppendEntries` RPC 时（无论是为了同步新的客户端请求，还是仅仅发送周期性心跳），它会在消息中包含一个字段 `leaderCommit`，该字段的值是 S1 所知的、已经被提交的最高日志条目的索引。

7.  **Follower 应用状态：**
    - 当 S2 和 S3 收到这个新的 `AppendEntries` 消息后，它们会读取 `leaderCommit` 字段。
    - 它们发现这个值已经更新，覆盖了它们之前收到的那条日志。现在它们确信该日志条目是安全的、已被提交的。
    - S2 和 S3 的 Raft 模块这才通知它们各自的上层应用程序，去执行该日志条目对应的命令，更新自己的状态。

**结论：** 这种设计巧妙地将客户端的延迟与 Follower 的状态应用解耦。客户端只需等待数据在多数派中变得“安全”，而 Follower 的最终状态同步则可以在后台异步完成，这种通过捎带信息进行通知的优化，有效减少了协议的通信开销。

---

## 日志

https://mit-public-courses-cn-translatio.gitbook.io/mit6-824/lecture-06-raft1/6.5-ri-zhi-raft-log

### 核心观点

Raft 中的**日志（Log）**是整个系统的**唯一真相和核心枢纽**。它不仅仅是数据的备份，更是实现**操作定序**、**状态同步**、**故障恢复**和**数据持久化**这四大核心功能的统一载体。可以说，理解了日志的多重角色，就理解了 Raft 的精髓。

---

### 逻辑梳理

日志在 Raft 系统中扮演着四个关键角色：

#### 1. 角色一：为操作定序 (Ordering Mechanism)

- **目的：** 保证所有副本以**完全相同的顺序**执行操作，这是复制状态机的基本要求。
- **如何工作：** Leader 接收到来自不同客户端的并发请求后，通过将这些请求**依次追加**到自己的日志中，为它们确定了一个全局唯一的、不可变的顺序。日志的索引号（`index`）就是这个顺序的体现。
- **本质：** 日志是一个由 Leader 维护的、权威的**全序广播（Total Order Broadcast）**媒介。

#### 2. 角色二：作为 Follower 的暂存区 (Staging Area)

- **目的：** 在操作被最终确认（Committed）之前，Follower 需要一个地方临时存放它们。
- **如何工作：** 当 Follower 收到 Leader 发来的 `AppendEntries` 请求时，它会将日志条目先写入自己的日志中，并向 Leader 回复确认。此时，该条目处于**未提交状态**。Follower **不会**立即执行它，只是将其暂存。
- **重要性：** 这个暂存区是“先复制，后执行”模型的物理体现。同时，这些未提交的日志条目在发生 Leader 变更时，有可能会被新的 Leader 覆盖或丢弃。

#### 3. 角色三：作为 Leader 的重传缓冲区 (Re-transmission Buffer)

- **目的：** 处理 Follower 掉线、网络丢包等情况，确保落后的 Follower 最终能赶上进度。
- **如何工作：** Leader 会在自己的日志中保留所有历史操作记录。当一个 Follower 重新上线或告知 Leader 自己缺少某段日志时，Leader 可以从自己的日志中找到对应的条目并**重传**给该 Follower。
- **本质：** Leader 的日志是整个集群状态的完整记录，是修复任何副本数据不一致的最终依据。

#### 4. 角色四：作为所有节点的持久化存储与恢复基础 (Durable Storage & Recovery)

- **目的：** 使节点能够在崩溃重启后恢复其状态，并重新加入集群。
- **如何工作：** 每个 Raft 节点都必须将接收到的日志条目**持久化**到非易失性存储（如磁盘）中。
- **恢复流程：**
  1.  当一个节点崩溃重启时，它首先从磁盘加载自己的日志。
  2.  此时，它并不知道哪些日志是已提交的，所以它**不会立即执行**任何操作。
  3.  它会以 Follower 的身份重新加入集群，并与当前 Leader 通信。
  4.  Leader 会通过 `AppendEntries` 机制，比对并修复该节点的日志，使其与集群的“唯一真相”保持一致（可能会覆盖该节点本地的部分未提交日志）。
  5.  一旦日志同步完成，该节点就可以像其他 Follower 一样，根据 Leader 的 `commitIndex` 通知，按顺序执行操作，重建其内存中的应用程序状态。

**引申问题与解决方案：**

- **流控问题：** 如果 Follower 执行速度远慢于 Leader 的日志生成速度，会导致 Follower 的日志无限增长，最终耗尽内存。生产级的 Raft 实现需要额外的流控机制，让 Follower 能向 Leader 反馈自己的执行进度，以便 Leader 调节速度。
- **冷启动恢复效率：** 从头执行整个日志来恢复状态非常低效。实际系统会采用**快照（Snapshotting）/ 检查点（Checkpointing）**机制，定期将内存状态完整地保存到磁盘，并丢弃之前的日志，从而大大加快恢复速度。

---

## 应用层接口

### 核心观点

应用程序与 Raft 库之间的接口是一个**解耦的、异步的契约**。应用程序通过一个非阻塞的 `Start()` 函数调用来**提议（propose）**一个操作，Raft 则在后台完成复杂的共识过程；当该操作被安全提交后，Raft 会通过一个专用的 Go channel (`applyCh`) **通知（notify）**应用程序去**执行（apply）**该操作。这种设计将“请求的发起”与“状态的变更”彻底分离开来。

---

### 逻辑梳理

这个接口由一进一出两个核心部分组成：

#### 1. 向下调用：`Start()` - 应用程序发起提议

- **角色：** 应用程序（如 KV 数据库）的入口。
- **时机：** 当 Leader 节点的应用程序收到一个客户端请求时。
- **动作：** 应用程序调用 Raft 库的 `Start(command)` 函数。
  - `command`：客户端的原始请求，如 `Put(k, v)`。
- **Raft 的响应（返回值）：** `Start()` 函数**立即**返回，不等待共识完成。它返回：
  - `index`：Raft 预分配给这个 `command` 的日志索引。
  - `term`：当前的任期号。
  - `isLeader`：一个布尔值，告知调用者它是否仍然是 Leader。
- **关键点：** 应用程序在调用 `Start()` 后，**不会立即响应客户端**。它只是拿到了一个“回执”（`index`），然后等待后续通知。

#### 2. 向上通知：`applyCh` - Raft 通知执行

- **角色：** Raft 库的出口，通往应用程序。
- **实现：** 一个 Go channel，名为 `applyCh`。
- **时机：** 当 Raft 库（在任何一个副本上，无论是 Leader 还是 Follower）确定某个日志条目已经被**提交（Committed）**时。
- **动作：** Raft 库向 `applyCh` 发送一个 `ApplyMsg` 消息。
  - `ApplyMsg` 包含：
    - `Command`：需要被执行的原始命令。
    - `CommandIndex`：该命令在日志中的索引。
- **应用程序的响应：**
  - 所有副本的应用程序都在一个循环中监听 `applyCh`。
  - 收到 `ApplyMsg` 后，它们会执行其中的 `Command`，从而更新自己的状态机（如修改 KV 表）。
  - **对于 Leader 节点**，它会使用 `CommandIndex` 来匹配之前 `Start()` 调用返回的 `index`，从而找到等待该结果的原始客户端请求，并最终向其发送响应。
  - **对于 Follower 节点**，它们只负责执行命令以保持状态同步，`index` 对它们没有特殊用途。

#### 3. 关键洞察：日志不一致性

- **短暂的不一致是常态：** 在 Raft 运行过程中，尤其是在 Leader 切换期间，不同副本的日志末尾部分存在差异是完全正常的。例如，一个旧 Leader 可能在崩溃前只成功将日志复制给了一部分 Follower。
- **Raft 的核心任务之一：** 新当选的 Leader 的一个主要职责就是**强制**所有 Follow-er 的日志与自己保持一致。它会通过 `AppendEntries` RPC 找到与每个 Follower 的日志分歧点，然后用自己的日志覆盖 Follower 的后续部分。这个过程最终会确保整个集群的日志恢复一致。

---

## Leader 选举

### 核心观点

Raft 的 Leader 选举是一个由**超时驱动（Timeout-Driven）**的民主过程，其核心安全保证来自于**过半票决（Majority Vote）**原则的双重应用：首先，候选人必须获得**过半选票**才能赢得选举，这保证了每个任期最多只有一个 Leader；其次，当选的 Leader 必须获得**过半确认**才能提交日志，这保证了被网络分区隔离的旧 Leader 无法造成破坏。

---

### 逻辑梳理

#### 1. 为什么需要 Leader？

- **核心原因：效率与简洁。**
  - **效率：** 有一个公认的 Leader 作为协调者，可以将达成共识所需的消息轮次从（无 Leader 系统通常需要的）两轮减少到一轮，显著提升性能。
  - **简洁：** 将“为操作定序”这一复杂职责集中于一个节点，使得整个协议的逻辑更易于理解和实现。

#### 2. 选举的触发与过程

1.  **任期（Term）：** Raft 将时间划分为连续的、递增编号的“任期”。每个任期是一个独立的“执政周期”，**最多只能有一个 Leader**。
2.  **选举定时器（Election Timer）：** 每个 Follower 节点都维护一个随机化的选举定时器。
3.  **触发选举：** 如果一个 Follower 在定时器超时之前没有收到来自当前 Leader 的任何消息（通常是心跳 `AppendEntries` RPC），它就认为 Leader 已宕机或失联，并决定发起一次新的选举。
4.  **成为候选人（Candidate）：**
    - 该节点立即将自己的当前任期号加一。
    - 切换到 Candidate 状态。
    - 为自己投上一票。
    - 向集群中所有其他节点并行发送 `RequestVote` RPC，请求它们为自己投票。

#### 3. 选举的规则与结果

1.  **投票规则（保证唯一性）：**

    - **一票制：** 在任何一个给定的任期内，每个节点**最多只能投出一张赞成票**。
    - **先到先得：** 它会把票投给第一个向它请求投票、且满足特定条件的合法候选人。

2.  **获胜规则（保证合法性）：**

    - **过半票决：** 一个候选人必须获得集群中**超过半数**节点的赞成票，才能赢得选举成为新的 Leader。
    - **数学保证：** 由于“一票制”和“过半票决”的结合，在同一个任期内，不可能有两个不同的候选人同时获得超过半数的选票。这就从根本上保证了**每个任期最多只有一个 Leader**。

3.  **结果通知（隐式宣告）：**
    - 当选的 Leader 不会广播一个“我赢了”的消息。
    - 取而代之，它会立即开始向所有节点发送常规的 `AppendEntries` 心跳消息。
    - 其他节点收到一个带有**新的、更高的任期号**的 `AppendEntries` RPC 时，它们就**隐式地**得知了新 Leader 的诞生，并自动切换回 Follower 状态，承认新 Leader 的权威。

#### 4. 安全性：如何处理被隔离的旧 Leader？

这是 Leader 选举机制最精妙的地方，也是过半票决的**第二次应用**。

- **场景：** 一个旧 Leader (L_old) 被网络分区隔离在一个少数派分区中，而多数派分区选举出了新 Leader (L_new)。
- **L_old 的困境：**
  - L_old 仍然认为自己是 Leader，并可能继续接收客户端请求。
  - 它会尝试将这些请求作为日志复制给它能联系到的少数几个 Follower。
  - 但是，因为它无法联系到**超过半数**的节点，所以它**永远无法获得多数派的确认**。
  - **结果：** L_old 提交的任何新日志都永远无法达到“已提交”（Committed）状态。因此，它**永远不会执行这些操作，也永远不会向客户端确认操作成功**。
- **结论：** 一个被隔离在少数派分区中的 Leader 虽然“名义上”还活着，但实际上已经被剥夺了所有“实权”，变成了一个无法对系统状态造成任何改变的“僵尸”，从而保证了系统的安全。

---

## 选举定时器

### 核心观点

Raft 的**选举定时器（Election Timer）**是其保证**活性（Liveness）**的关键机制，它通过引入**随机化（Randomization）**来打破选举过程中的对称性，从而极大地降低了因“分裂选票”（Split Vote）导致的选举失败和系统僵死风险。这确保了在 Leader 真正失效后，系统能够快速、大概率地选举出新的 Leader 以恢复服务。

---

### 逻辑梳理

#### 1. 选举定时器的基本作用

- **维持现状（Leader 在线时）：**

  - Leader 会周期性地向所有 Follower 发送心跳（`AppendEntries` RPC）。
  - 任何 Follower 只要收到来自当前 Leader 的 `AppendEntries` 消息，就会**重置**自己的选举定时器。
  - **效果：** 只要 Leader 健康且网络通畅，Follower 的定时器就永远不会超时，从而有效地抑制了不必要的选举。

- **触发选举（Leader 失联时）：**
  - 如果一个 Follower 在其定时器超时之前没有收到 Leader 的心跳，它就认为 Leader 已失效。
  - **效果：** 定时器超时成为发起新一轮选举的唯一合法触发条件。

#### 2. 问题：分裂选票（Split Vote）

- **场景：** 假设 Leader 崩溃，所有 Follower 的定时器都在同一时刻开始计时。如果它们的超时时间完全相同，会发生什么？
  1.  所有 Follower **同时**超时。
  2.  所有 Follower **同时**转为 Candidate，任期号加一，并为自己投票。
  3.  所有 Candidate **同时**向外请求选票。
  4.  当一个 Candidate 收到另一个 Candidate 的投票请求时，因为它已经把票投给了自己，所以它会拒绝该请求。
  5.  **结果：** 每个 Candidate 都只得到一票（来自自己），没有任何人获得过半选票。选举失败。
- **僵局：** 由于没有选出 Leader，也就没有心跳来重置定时器。所有节点会再次启动定时器，并可能再次陷入同样的分裂选票僵局，导致系统永久瘫痪。

#### 3. 解决方案：随机化超时

Raft 的核心洞察是，要打破这种同步导致的僵局，就必须引入不对称性。

- **机制：**

  1.  每个节点在重置其选举定时器时，**不是**设置一个固定的超时时间，而是从一个预设的**时间范围**内（例如 [300ms, 500ms]）**随机选择**一个值作为本次的超时时间。
  2.  **关键：每次重置，都必须重新随机选择一次。**

- **效果：**
  - 由于超时时间是随机的，几乎可以肯定，在 Leader 失效后，**总会有一个 Follower 的定时器会比其他所有节点先超时**。
  - 这个“抢跑”的节点会率先发起选举。当它的 `RequestVote` 请求到达其他节点时，其他节点的定时器还未超时，它们仍然是 Follower，并且尚未投票。
  - 因此，这个抢跑的 Candidate 很有可能在其他节点变成 Candidate 之前，就集齐超过半数的选票，成功当选为 Leader。

#### 4. 超时范围的设定考量

随机范围的选择并非任意，它是一个重要的工程权衡：

- **超时下限（`min_timeout`）：** 必须**显著大于** Leader 的心跳间隔（`heartbeat_interval`），并且最好能容忍一次网络往返（RTT）。这可以防止因网络抖动或单个心跳丢包而导致的误判和不必要的选举。

  - `min_timeout >> heartbeat_interval`

- **超时上限（`max_timeout`）：**
  - **影响故障恢复时间：** 系统在 Leader 崩溃后到新 Leader 选出前的这段时间是不可用的。`max_timeout` 决定了最坏情况下的故障检测延迟。这个值越小，系统恢复越快。
  - **影响分裂选票概率：** `max_timeout - min_timeout` 这个范围越大，两个节点选到相近超时时间的概率就越低，从而分裂选票的概率也越低。这个时间差至少要大于一次选举 RPC 的往返时间，才能给“抢跑者”足够的时间去赢得选举。

---

## 可能的异常情况

### 核心观点

Raft 通过让新当选的 Leader 强制所有 Follower 的日志与自己**完全一致**来解决因故障导致的日志不一致问题。这个“日志修复”过程遵循一个至关重要的安全原则：**任何存在于多数派（Majority）节点上的日志条目，都必须被假定为“可能已提交”，因此必须保留；而任何只存在于少数派（Minority）节点上的日志条目，则被视为“未提交”，可以被安全地覆盖。**

---

### 逻辑梳理

#### 1. 问题根源：故障如何导致日志不一致？

- **正常情况下的铁律：** 只要 Leader 健康，Follower 就必须无条件接受并覆盖自己的日志，以匹配 Leader 的日志。一致性很容易维持。
- **故障打破铁律：** 当 Leader 在复制日志的过程中崩溃时，混乱就产生了。
  - **场景一（部分复制）：** Leader 在将日志条目复制给**部分** Follower 后崩溃。
    - **例子：** S3 (Leader) 将日志写入 S2 后崩溃，但还未来得及写入 S1。结果 S2 和 S3 的日志比 S1 长。
  - **场景二（快速连续故障）：** 连续多任 Leader 在刚当选、只在自己本地写入一条日志后就立即崩溃。
    - **例子：** S2 在任期 4 成为 Leader，在索引 12 写入日志 `T4` 后崩溃。随后 S3 在任期 5 成为 Leader，也在索引 12 写入了不同的日志 `T5` 后崩溃。结果在同一索引 12 上，S2 和 S3 有着不同任期、不同内容的日志。

#### 2. 解决方案：新 Leader 的“强制统一”

当一个新的 Leader 被选举出来后，它不会去尝试“合并”或“协调”各个 Follower 的混乱日志。相反，它会执行一个简单而霸道的流程：

1.  **找到分歧点：** 对于每一个 Follower，新 Leader 会从后向前逐一比对日志条目，找到两者日志开始不一致的那个索引位置。
2.  **强制覆盖：** 从那个分歧点开始，Leader 会用自己的日志**完全覆盖** Follower 的后续所有日志。
3.  **最终结果：** 经过这个过程，所有 Follower 的日志最终都会变成 Leader 日志的一个精确前缀，从而恢复整个集群的日志一致性。

#### 3. 核心安全原则：为什么这样是安全的？

这个强制覆盖的过程之所以安全，完全依赖于 Raft 的选举规则和对“已提交”的定义。

- **“可能已提交”的条目必须保留：**

  - **定义：** 如果一个日志条目存在于**超过半数**的节点上，那么它就**有可能**已经被前一任 Leader 提交了（因为前任 Leader 可能收到了多数派的确认，并响应了客户端，然后才崩溃）。
  - **安全要求：** 我们无法证明它**没有**被提交，所以为了安全，**必须假设它已经被提交**。因此，任何新 Leader 都不能丢弃或覆盖这样的日志条-目。
  - **Raft 的保证：** Raft 的选举规则（我们将在下一节课学习）会确保，任何当选的新 Leader，其自身的日志**必然包含了所有“可能已提交”的条目**。

- **“未提交”的条目可以丢弃：**
  - **定义：** 如果一个日志条目只存在于**少数派**节点上（如前述例子中索引 12 的 `T4` 和 `T5` 条目），那么它**绝对不可能**被提交。
  - **安全允许：** 因为客户端不可能收到对这些操作的确认，所以丢弃或覆盖这些日志条目是完全安全的。新 Leader 在强制统一日志时，就会自然地将这些“废弃的”分支修剪掉。

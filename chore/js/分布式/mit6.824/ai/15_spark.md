## Spark

### 核心观点

Spark 是对 MapReduce 的一次重大进化，它将大数据处理从僵化的两阶段模型（Map -> Reduce）解放出来，引入了以**弹性分布式数据集（RDD）**为核心的、更通用和高效的编程范式。其精髓在于**惰性求值（Lazy Evaluation）**：用户的代码并不直接执行计算，而是在内存中构建一个记录了数据转换步骤的**血缘图（Lineage Graph）**。只有当一个“行动”（Action）操作被调用时，Spark 才会根据这个图进行整体优化，并将计算任务分发执行。这种机制不仅通过内存计算和流水线化（Pipelining）大幅提升了性能，更重要的是，它赋予了系统一种优雅的容错能力：当任何数据分区因节点故障而丢失时，Spark 可以简单地根据血缘图**重新计算**出丢失的部分，而无需依赖昂贵的实时数据复制。

---

### 逻辑梳理

#### 1. Spark vs. MapReduce：从僵化到灵活

MapReduce 模型虽然开创了大数据并行处理的时代，但其自身存在明显局限，而 Spark 正是为了解决这些问题而生。

| 特性         | MapReduce                                                             | Spark                                                                           |
| :----------- | :-------------------------------------------------------------------- | :------------------------------------------------------------------------------ |
| **计算模型** | 僵化的两阶段：Map -> Shuffle -> Reduce。                              | 灵活的**有向无环图 (DAG)**，支持任意多步的复杂数据流。                          |
| **中间数据** | 每个阶段的输出都必须写入分布式文件系统（如 HDFS），引入大量磁盘 I/O。 | 中间数据优先**保存在内存中**，在不同阶段间以流水线方式传递，极大减少 I/O 开销。 |
| **迭代计算** | 极其低效。每次迭代都是一个独立的 MapReduce 作业，反复读写磁盘。       | 非常高效。通过将中间结果**缓存 (cache)** 在内存中，可以被后续迭代反复重用。     |
| **编程模型** | 相对底层和繁琐。                                                      | 提供了更丰富、更具表现力的 API（如 `map`, `filter`, `join`, `groupByKey`）。    |

#### 2. Spark 的核心编程模型：RDD、转换和行动

理解 Spark 的关键在于理解它的三大核心概念。

1.  **RDD (Resilient Distributed Dataset - 弹性分布式数据集):**

    - **是什么：** 一个**不可变的**、被分区到集群中多个节点上的、可并行操作的记录集合。它是 Spark 中最基本的数据抽象。
    - **为何“弹性”：** 它的容错能力不是通过数据复制实现的，而是通过记录“如何计算出它”（即血缘）来实现的。如果一个分区丢失，可以根据血缘重新计算出来。

2.  **转换 (Transformations):**

    - **是什么：** 从一个已有的 RDD 生成一个**新的** RDD 的操作，例如 `map()`, `filter()`, `join()`。
    - **核心特性：惰性求值 (Lazy Evaluation)**。调用一个转换操作时，Spark **不会立即执行计算**。它只是在内部记录下这个操作，并将其添加到血缘图中。

3.  **行动 (Actions):**
    - **是什么：** 触发实际计算，并将结果返回给驱动程序或写入到外部存储的操作，例如 `collect()`, `count()`, `saveAsTextFile()`。
    - **作用：** 当一个行动被调用时，Spark 才会审视整个血缘图，对其进行优化，然后制定执行计划，并将计算任务分发到各个工作节点上真正执行。

**一句话总结编程模型：** 写 Spark 代码就像是在画一张计算蓝图（通过转换操作构建血缘图），直到你下达“执行”命令（调用行动操作）时，工人们（工作节点）才开始照图施工。

#### 3. 执行模型：窄依赖与宽依赖

Spark 根据转换操作对数据分区的依赖关系，将其分为两类，这直接影响执行效率和容错成本。

- **窄依赖 (Narrow Dependency):**

  - **定义：** 父 RDD 的每个分区最多只被子 RDD 的一个分区使用。例如 `map`, `filter`。
  - **优势：**
    - **高效执行：** 可以在单个节点内以流水线（Pipelining）的方式执行。一个父 RDD 分区的数据可以流式地经过多个窄依赖转换，无需落盘。
    - **高效容错：** 如果一个子分区丢失，只需重新计算其对应的父分区即可，影响范围小。

- **宽依赖 (Wide Dependency):**
  - **定义：** 父 RDD 的一个分区可能被子 RDD 的多个分区使用。这通常意味着需要 **Shuffle**——在网络间重新分发数据。例如 `groupByKey`, `reduceByKey`, `join`。
  - **代价：**
    - **昂贵：** Shuffle 涉及大量的网络和磁盘 I/O，是 Spark 应用中最主要的性能瓶颈之一。
    - **容错成本高：** 如果一个子分区丢失，可能需要重新计算其所有父分区，因为无法确定具体是哪个父分区的数据出了问题。

#### 4. 容错机制：基于血缘的重计算

Spark 的容错策略是其设计的核心亮点。

- **基本策略：** 当一个工作节点失败，其上存储的 RDD 分区会丢失。Spark 的主节点（Driver）会检测到这个失败，并查找丢失分区的血缘。然后，它会在其他健康的工作节点上重新执行计算这些分区所需的转换操作，从而恢复数据。
- **检查点 (Checkpointing):** 对于血缘链条非常长或者计算成本极高的 RDD，反复重计算的代价可能很高。为此，用户可以主动设置**检查点**，将某个 RDD 的计算结果物化并持久化到可靠的存储（如 HDFS）中。这样，如果后续发生故障，恢复就可以从最近的检查点开始，而不是从头开始。这是一种在“重计算成本”和“持久化成本”之间的权衡。

## Aurora 背景历史

### 核心观点

Amazon Aurora 的诞生源于其前代架构——在**弹性块存储（EBS）**上运行传统数据库——所暴露出的两大根本性瓶颈：**极高的网络 I/O 开销**和**有限的容错能力**。EBS 通过网络复制（链式复制）解决了单机故障导致数据丢失的问题，但传统数据库在其上运行时会产生大量冗余的网络写流量。同时，EBS 的副本局限于单个可用区（AZ），无法抵御数据中心级别的灾难。Aurora 的设计初衷，正是为了从根本上重新思考数据库与存储层的交互方式，以专门定制的存储系统来解决这两个核心痛点。

---

### 逻辑梳理

#### 1. 演进的起点：EC2 与本地存储

- **服务：** EC2（弹性计算云）提供虚拟机。
- **存储：** 最初，EC2 实例使用其所在物理服务器的**本地硬盘**。
- **问题：**
  - 对**无状态**应用（如 Web 服务器）来说，这很完美。服务器宕机，在别处重启一个新实例即可。
  - 对**有状态**应用（如数据库）来说，这是**灾难性**的。服务器宕机意味着数据永久丢失。

#### 2. 第一代解决方案：EBS (弹性块存储)

为了给数据库等有状态应用提供持久化存储，Amazon 推出了 EBS。

- **抽象：** 对 EC2 实例来说，EBS 看起来就像一个普通的、可挂载的**本地硬盘**。
- **实现：** 实际上，EBS 是一个**网络附加存储**服务。每个 EBS 卷由一对使用**链式复制（Chain Replication）**进行同步的存储服务器在后台提供支持。
  - **写操作：** 数据通过网络发送到链头，再复制到链尾，然后向 EC2 实例确认。
  - **读操作：** 数据直接从链尾读取。
- **解决的问题：**
  - **持久性：** 实现了单机容错。如果数据库所在的 EC2 实例或 EBS 的一个副本服务器宕机，数据不会丢失。可以启动一个新 EC2 实例并重新挂载同一个 EBS 卷。

#### 3. EBS 暴露出的新问题 (Aurora 的动机)

尽管 EBS 是一个巨大的进步，但它作为通用块存储，在承载传统数据库时暴露了两个核心缺陷：

1.  **性能瓶颈：爆炸性的网络 I/O**

    - 传统数据库（如 MySQL）的设计假定存储是本地且廉价的。它会产生多种类型的写操作：数据页、事务日志（redo log）、二进制日志（binlog）、双写缓冲区（doublewrite buffer）等。
    - 当这些写操作全部通过网络发送到 EBS 时，会产生巨大的网络流量放大效应。Aurora 论文指出，一次数据库逻辑写入可能导致多达**六次**网络写入。
    - 这使得**网络**成为整个系统的主要瓶颈，严重限制了数据库的吞吐量。

2.  **可靠性瓶颈：单可用区（AZ）的容错局限**
    - 为了降低延迟和成本，EBS 的两个副本通常位于**同一个可用区（AZ）**，即同一个数据中心。
    - 这意味着 EBS 只能抵御单机或机架级别的故障。
    - 如果整个数据中心发生灾难（如断电、火灾、网络中断），EBS 卷上的数据将**完全丢失**，无法满足高可用性应用的需求。

**结论：** Aurora 的历史背景是一个典型的工程演进故事。EC2 的本地存储无法满足持久化需求，催生了 EBS。而 EBS 作为一个**通用**的块存储解决方案，在承载数据库这一**特定**应用时又显得力不从心，其在性能和可靠性上的局限性，直接为下一代**应用定制**的、深度整合的数据库存储系统——Aurora——的诞生铺平了道路。

---

## 故障可恢复事务(Crash Recoverable Transaction)

### 核心观点

传统数据库通过**预写式日志（Write-Ahead Logging, WAL）**机制来实现可故障恢复的事务。其核心原则是：**在将任何数据变更写入磁盘上的实际数据页（Data Page）之前，必须先将描述该变更的日志记录（Log Record）持久化到磁盘上的日志文件中。** 这个日志文件成为系统恢复的唯一事实来源，确保了事务的原子性（要么全做，要么全不做）和持久性（一旦提交，永不丢失），即使在操作中途发生崩溃。

---

### 逻辑梳理

#### 1. 问题的本质：原子性与持久性

一个事务（如银行转账）包含多个操作，必须满足 ACID 属性，其中最关键的是：

- **原子性 (Atomicity):** 转账操作（一个账户减钱，另一个账户加钱）必须作为一个不可分割的单元执行。不能只发生一半。
- **持久性 (Durability):** 一旦数据库告知客户端事务已“提交”，这个变更就必须是永久的，即使系统立即崩溃。

直接在磁盘上修改数据页无法满足这些要求，因为在修改多个数据页的过程中发生崩溃，会导致数据库状态不一致。

#### 2. 解决方案：预写式日志 (WAL)

数据库引入了一个中介层——WAL，将复杂的、随机位置的数据页写入，转变为简单的、顺序的日志追加。

**事务执行流程：**

1.  **开始事务 & 加锁：** 客户端发起事务。数据库锁定将要被修改的数据行（如账户 X 和 Y），以防止其他并发事务干扰。

2.  **在内存中修改：** 数据库将需要的数据页从磁盘读入内存缓存（Buffer Cache），并在**内存中**执行修改（X+10, Y-10）。此时，磁盘上的实际数据页**尚未改变**。

3.  **写入日志 (Write-Ahead):** 在提交事务之前，数据库生成描述这些变更的日志记录，并**将它们写入磁盘上的 WAL 文件**。这些日志记录包含：

    - 事务 ID。
    - 要修改的数据项（如账户 X）。
    - 旧值（用于回滚/UNDO）。
    - 新值（用于重做/REDO）。

4.  **提交日志：** 数据库在 WAL 文件末尾追加一条特殊的 **"Commit" 记录**，并确保这条记录被**物理写入磁盘**。

5.  **确认客户端：** 一旦 "Commit" 记录成功落盘，数据库就可以安全地向客户端报告“事务已提交”。此时，持久性得到保证。

6.  **懒惰地写入数据页：** 在未来的某个时刻（由数据库自行决定，为了优化性能），内存中被修改过的“脏”数据页才会被慢慢地、异步地写回到磁盘上对应的位置。

#### 3. 故障恢复机制

- **场景：** 数据库在第 5 步之后、第 6 步完成之前崩溃。此时，WAL 中有完整的事务日志和 Commit 记录，但磁盘上的数据页可能还是旧的。

- **恢复过程 (Redo):**
  1.  数据库重启后，恢复程序会扫描 WAL。
  2.  它找到一个事务的所有日志记录，并看到了最后的 "Commit" 记录。
  3.  它意识到这个事务是已经成功提交的，但其变更可能没有完全体现在数据页上。
  4.  于是，恢复程序会根据日志中的**新值**，重新执行这些变更，将新数据写入磁盘上的数据页，确保数据库恢复到一致的、已提交的状态。

**结论：** WAL 机制巧妙地将事务的“提交点”与“数据实际落盘点”解耦。事务的持久性不再依赖于缓慢且危险的随机数据页写入，而是依赖于快速、安全的顺序日志写入。日志成为了一个不可篡改的契约，保证了无论发生何种崩溃，系统总能通过重放（Redo）或撤销（Undo）日志，恢复到一个正确的状态。Aurora 正是抓住了这个核心机制，并对其进行了革命性的优化。

---

## 关系性数据库(Amazon RDS)

### 核心观点

Amazon RDS 通过在另一个可用区（AZ）中同步复制一个完整的、运行在 EBS 上的标准数据库实例，解决了 EBS 无法抵御数据中心级别灾难的问题。然而，这种“镜像”架构是一种**简单粗暴的蛮力方案**：它在对数据库内部逻辑一无所知的情况下，将数据库产生的所有物理写操作（包括巨大的数据页和日志文件）通过网络进行同步复制。这种做法导致了**灾难性的写放大和网络开销**，极大地牺牲了性能，从而为 Aurora 的诞生提供了最直接的动机。

---

### 逻辑梳理

#### 1. RDS 的目标：跨可用区（Cross-AZ）容灾

EBS 提供了单 AZ 内的容错，但如果整个数据中心（AZ）发生故障，数据就会丢失。RDS 的首要目标就是解决这个问题，提供更高一级的可靠性。

#### 2. RDS 的实现架构：同步镜像

RDS 的架构本质上是把一个“数据库 on EBS”的组合，完整地、同步地复制到了另一个 AZ：

1.  **主可用区 (AZ1):**

    - 一个**主数据库实例**（EC2）处理所有客户端请求。
    - 它使用一个 EBS 卷作为其存储。因此，它的每一次物理写入（写 WAL 或写数据页）都会通过网络发送给 AZ1 内的两个 EBS 副本。

2.  **备用可用区 (AZ2):**

    - 一个**备用数据库实例**（EC2）被动地接收来自主实例的所有写操作。
    - 它也拥有自己的 EBS 卷，并将接收到的写操作再写入到 AZ2 内的两个 EBS 副本中。

3.  **同步写流程：**
    - 主数据库发起一个写操作（例如，追加一条 WAL 记录）。
    - 这个写操作被**同时**发送到：
      - AZ1 内的 EBS。
      - AZ2 内的备用数据库实例。
    - 备用实例再将该写操作转发给 AZ2 内的 EBS。
    - 只有当主数据库收到来自**备用实例的确认**后，这个写操作才算完成。

#### 3. 为什么性能如此糟糕？—— 灾难性的写放大

RDS 性能低下的根源在于，它在**物理层面**进行复制，对数据库的**逻辑**一无所知。

- **问题：** 数据库的一个微小逻辑变更（如修改一个 8 字节的整数）可能会触发一个对 8KB 或 16KB 数据页的物理写入。
- **放大效应：** 在 RDS 架构下，这个 16KB 的数据页写入会被放大数倍，并通过网络传输：
  1.  从主数据库到 AZ1 的 EBS（链式复制，两次网络传输）。
  2.  从主数据库**跨越 AZ** 到备用数据库（高延迟、高成本的网络传输）。
  3.  从备用数据库到 AZ2 的 EBS（又是两次网络传输）。
- **结果：** 一次小小的数据库更新，在网络上引发了多次、跨越数据中心的大数据块传输。如论文所述，这导致了极高的网络流量和写延迟，使得整个系统的吞-吐量被严重限制。

**结论：** RDS 成功地用一种直接但低效的方式实现了跨 AZ 容灾，但它付出了巨大的性能代价。它的失败之处在于将数据库视为一个不透明的“黑盒”，只是机械地复制其物理 I/O。这次昂贵的尝试证明了，要想同时实现高性能和高可靠性，必须打破数据库与存储之间的壁垒，进行深度整合与协同设计——这正是 Aurora 所要做的。

---

## Aurora 初探

### 核心观点

Amazon Aurora 通过两大革命性设计，彻底颠覆了传统数据库的复制模型，从而实现了性能与可靠性的巨大飞跃。**第一，它将数据库与存储层深度解耦，不再通过网络复制庞大的数据页，而是只复制轻量的、描述变更的日志记录（Log Record）。第二，它用一个跨越多个可用区（AZ）的、基于 Quorum（法定人数）的存储系统取代了传统的同步复制，从而在获得极高容错能力的同时，避免了被最慢的副本拖慢性能。**

---

### 逻辑梳理

#### 1. 核心洞察：分离计算与存储，只复制“意图”

传统数据库（包括 RDS）最大的性能瓶颈在于，它们将计算（执行事务）和存储（持久化数据页）紧密耦合，并通过网络复制物理层面的结果（数据页）。Aurora 的第一个突破就是打破这个耦合。

- **RDS 的做法（低效）：** 数据库在内存中修改一个 16KB 的数据页，然后将这整个 16KB 的数据页通过网络发送给副本。
- **Aurora 的做法（高效）：** 数据库在内存中修改数据页，但它只将描述这个修改的**日志记录**（例如，“将页面 P 的偏移量 O 处的值从 V1 改为 V2”，可能只有几十个字节）通过网络发送出去。

**结果：**

- **网络流量锐减：** 网络上传输的数据量从庞大的数据页变成了微小的日志记录，数量级下降了数百倍。这是 Aurora 性能提升的最主要来源。
- **存储层智能化：** 这也意味着存储层不再是通用的“块设备模拟器”（如 EBS），而是一个**应用定制的、能理解数据库日志**的智能系统。存储节点自己负责接收日志记录，并在本地“重放”这些日志来构建出最新的数据页。

#### 2. 核心机制：跨三可用区的 6 副本 Quorum

Aurora 的第二个突破是其全新的复制和容错模型，旨在同时实现高可用性和低延迟。

- **架构：**
  - 数据被复制 **6** 份。
  - 这 6 个副本分布在 **3** 个不同的可用区（AZ），每个 AZ 有 2 个副本。
- **Quorum 法则：**
  - **写 Quorum (Vw = 4):** 一个写操作（发送日志记录）只需要得到 **6 个副本中的任意 4 个**确认，就可以被认为是成功的。数据库主节点无需等待全部 6 个副本。
  - **读 Quorum (Vr = 3):** （在需要从存储层读取时）只需要从 **6 个副本中的任意 3 个**读取，就能保证至少读到一个拥有最新数据的版本。
- **优势：**
  - **极高的容错能力：**
    - 可以容忍**整个 AZ 宕机**（损失 2 个副本），系统仍可正常写入（因为还剩 4 个副本）。
    - 可以容忍**整个 AZ 宕机 + 任意另外 1 个副本**的故障，系统仍可正常写入（因为还剩 3 个副本，但读 Quorum 仍可满足）。
  - **极低的写延迟：** 数据库主节点永远只依赖于**最快的 4 个副本**的响应，完全不受最慢的 2 个副本或已宕机的副本的影响。这有效地消除了“慢节点”问题。

**结论：** Aurora 的初探揭示了其设计的精髓：通过**“只写日志”**解决了网络瓶颈，通过**“Quorum 复制”**解决了高可用与低延迟之间的矛盾。这种将数据库逻辑下推到存储层的“日志即数据库”（Log is the Database）哲学，以及对分布式共识的巧妙应用，共同造就了其宣称的 35 倍性能提升，使其成为云原生数据库设计的一个里程碑。

---

## Aurora 存储服务器的容错目标(Fault-Tolerant Goals)

### 核心观点

Aurora 的存储系统设计了一套精密的、分层级的容错目标，其核心是利用 **6 副本 Quorum 机制**来同时应对**大规模灾难（整个可用区 AZ 故障）**和**常规故障（单个副本失效或变慢）**。它通过精心设置的读写 Quorum 值（Vw=4, Vr=3），不仅保证了在损失一个完整 AZ 后仍能无中断地写入，甚至还能在此基础上再容忍一个副本的丢失并维持读取能力。这种设计哲学反映了对云环境下复杂故障模式的深刻理解，并强调了在故障发生后快速恢复（Fast Re-replication）的重要性。

---

### 逻辑梳理

Aurora 的存储容错目标可以分解为四个关键层面，层层递进，共同构成了其强大的可靠性基础：

#### 1. 抵御大规模灾难：容忍整个可用区（AZ）故障

- **目标：** 当一个完整的 AZ（包含 2 个副本）因断电、网络中断等原因彻底离线时，**写操作必须能够继续**。
- **实现 (`Vw = 4`)：** 系统共有 6 个副本。损失一个 AZ 后，还剩下 4 个副本。由于写 Quorum（`Vw`）被设定为 4，这 4 个幸存的副本刚好构成一个有效的写 Quorum。因此，数据库主节点仍然可以成功提交写操作，实现对 AZ 级别故障的无缝容错。

#### 2. 灾难下的再容错：增强的读取鲁棒性

- **目标：** 在一个 AZ 已经离线的情况下（这可能是个长期事件），系统必须还能容忍**额外的、单个副本的故障**，并保证**读操作仍然可以成功**。
- **实现 (`Vr = 3`)：**
  - 一个 AZ 离线，剩下 4 个副本。
  - 此时，如果再有一个副本故障，还剩下 3 个副本。
  - 由于读 Quorum（`Vr`）被设定为 3，这 3 个幸存的副本刚好构成一个有效的读 Quorum。
  - 这意味着即使在“灾难+1”的极端情况下，系统仍然可以从存储层读取到一致的数据。

#### 3. 应对常规扰动：容忍慢副本

- **目标：** 系统的性能不应被少数暂时响应缓慢的副本所拖累。
- **实现 (Quorum 机制的天然属性)：** 由于写操作只需要等待最快的 4 个副本响应，系统自然地忽略了最慢的 2 个副本。这使得 Aurora 对网络抖动、服务器临时高负载等常见问题具有很强的免疫力，保证了稳定且可预测的低延迟。

#### 4. 快速恢复能力：快速再复制 (Fast Re-replication)

- **目标：** 当一个副本被确认永久性丢失后，必须尽快地利用现有副本创建一个新的副本，以迅速将系统的冗余度恢复到完整状态。
- **背景：** 故障往往不是独立事件（“祸不单行”）。一个副本的丢失会显著增加系统在下一个故障面前的脆弱性。
- **实现：** Aurora 的存储系统被设计为能够高效地进行对等（Peer-to-Peer）数据复制。当需要生成新副本时，幸存的副本可以协同工作，快速地将数据同步给新加入的节点，而不是依赖于数据库主节点这个单一的瓶颈。

**结论：** Aurora 的容错设计是一个精心计算的平衡。它没有盲目地追求最高数量的副本，而是通过 `V=6, Vw=4, Vr=3` 这一组“黄金数字”，精确地满足了一系列现实世界中复杂的、多层次的容错需求。这套设计不仅考虑了“如何幸存”，还考虑了“在幸存状态下如何继续幸存”以及“如何快速从幸存状态恢复”，体现了顶级云服务提供商在构建大规模分布式系统时深思熟虑的工程哲学。

---

## Quorum 复制机制简介(Quorum Replication)

### 核心观点

Quorum 复制机制是一种通过**冗余**和**重叠**来构建容错存储系统的经典思想。其核心原则是：**任何一个有效的读操作所接触的副本集合（Read Quorum, R），必须与任何一个有效的写操作所接触的副本集合（Write Quorum, W）至少有一个副本的交集。** 这个由 `R + W > N`（N 为总副本数）保证的重叠，结合**版本号机制**，确保了读操作总能找到并识别出最新的已提交数据，即使部分副本发生故障或暂时不可用。

---

### 逻辑梳理

#### 1. 基本概念与配置

- **N:** 系统中的总副本数量。
- **W (Write Quorum):** 执行一次写操作，必须得到至少 `W` 个副本的成功确认。
- **R (Read Quorum):** 执行一次读操作，必须从至少 `R` 个副本处获取到数据。

#### 2. 核心原则：重叠保证一致性

- **鸽巢原理的应用：** `R + W > N` 是 Quorum 系统的基石。它从数学上保证了，无论你如何选择 `W` 个副本进行写入，以及如何选择 `R` 个副本进行读取，这两个集合中**必然至少有一个成员是相同的**。
- **一致性的保证：** 这个重叠的副本就像一个“信使”，它确保了最新的写操作结果（即使只写入了 `W` 个副本）一定能被后续的任何一次读操作（它会查询 `R` 个副本）所“看到”。

#### 3. 如何识别最新数据：版本号机制

- **问题：** 读操作从 `R` 个副本中可能会收到多个不同的值（因为有些副本可能错过了最近的写操作）。如何确定哪个是正确的最新值？
- **解决方案：**
  1.  **写操作携带版本号：** 每次写操作都必须附带一个**单调递增的版本号**（或时间戳）。
  2.  **读操作比较版本号：** 客户端从 `R` 个副本收集到所有 `(value, version)` 对后，只需简单地**选择版本号最高的那个值**作为最终结果。
- **为什么投票（Voting）不可行：** `R + W > N` 只保证了**至少一个**重叠，不保证多数重叠。因此，最新的数据可能只存在于 `R` 个副本中的一个，投票会选出错误（但数量更多）的旧数据。

#### 4. Quorum 系统的优势

- **容忍慢节点和故障：** 这是 Quorum 系统相比于“全量复制”或“链式复制”的核心优势。
  - 客户端向所有 `N` 个副本发起请求，但**只需等待最快的 `W` 或 `R` 个响应**即可。
  - 系统无需显式地进行故障检测或成员变更。慢节点或故障节点被**隐式地、优雅地**忽略了，只要幸存的节点数量足以构成 Quorum。
- **可调节的读写性能：**
  - **偏重读性能 (`R` 小, `W` 大):** 例如 `N=3, R=1, W=3`。读操作非常快，只需一次响应。但写操作变慢且无容错性。
  - **偏重写性能 (`W` 小, `R` 大):** 例如 `N=3, W=1, R=3`。写操作非常快，但读操作变慢且无容错性。
  - **平衡配置 (Raft 采用):** `N=2f+1, R=W=f+1`（多数派）。读写性能和容错性达到平衡。

#### 5. Aurora 的 Quorum 配置 (`N=6, W=4, R=3`)

Aurora 的配置是其容错目标在 Quorum 原则下的直接体现：

- **`N=6`:** 跨 3 个 AZ，每个 AZ 2 个副本。
- **`W=4`:**
  - **容忍 AZ 故障：** 损失一个 AZ（2 个副本）后，还剩 4 个副本，刚好满足写 Quorum。**写操作可继续。**
  - **`R+W = 3+4 = 7 > 6`:** 满足 Quorum 重叠原则。
- **`R=3`:**
  - **容忍“AZ+1”故障：** 损失一个 AZ（2 个副本）和另一个副本后，还剩 3 个副本，刚好满足读 Quorum。**读操作可继续**，从而可以恢复数据。

**结论：** Quorum 机制为分布式系统提供了一个强大而灵活的框架，用于在性能、可用性和一致性之间进行权衡。Aurora 通过其精心设计的 `(6, 4, 3)` Quorum 模型，不仅实现了对多种复杂故障场景的强大容忍能力，还天然地获得了对慢节点的免疫力，这是其高性能和高可靠性的关键所在。

---

## Aurora 读写存储服务器

### 核心观点

Aurora 对经典 Quorum 机制进行了创造性的改造，形成了一种**非对称的读写模型**。**写操作**遵循严格的 Quorum 法则，将轻量的**日志记录（Log Record）**写入 `W` 个存储副本以保证持久性。而**读操作**则被极大地优化：在正常情况下，数据库主节点通过追踪各副本的日志进度，可以**绕过 Quorum Read**，直接从一个最新的副本读取**数据页（Data Page）**。只有在数据库主节点崩溃恢复时，新的主节点才会执行一次 **Quorum Read** 来发现并截断不完整的日志，从而恢复系统到一致状态。

---

### 逻辑梳理

#### 1. 写路径：只写日志，遵循 Quorum

- **写的是什么：** 数据库主节点**从不**通过网络写入数据页。它只写入描述变更的**日志记录**。
- **如何写：**
  1.  主节点将一条新的日志记录广播给所有 6 个存储副本。
  2.  它等待，直到收到**任意 4 个（Write Quorum）**副本的确认。
  3.  一旦收到 4 个确认，该日志记录就被认为是**持久化的**。主节点可以继续处理下一个操作，并在事务结束时向客户端确认提交。
- **存储节点的行为：**
  - 存储节点接收到日志记录后，并**不立即**将其应用到磁盘上的数据页。
  - 它只是将日志记录追加到内存中一个与该数据页关联的**日志列表**里。这是一种**懒惰更新（Lazy Update）**策略，极大地减少了磁盘 I/O。

#### 2. 读路径（正常情况）：绕过 Quorum，智能选择

- **读的是什么：** 数据库主节点需要读取的是**数据页**，而不是日志。
- **如何读（优化路径）：**
  1.  **进度追踪：** 主节点持续追踪每个存储副本已经成功接收到的连续日志的最高序号（LSN - Log Sequence Number）。
  2.  **智能选择：** 当需要读取一个数据页时，主节点会查看其进度表，找到一个或多个**拥有最新日志的副本**。
  3.  **单副本读取：** 它向其中**一个**最新的副本发送读请求。
  4.  **按需生成页面：** 被请求的存储节点此时才会将内存中缓存的旧数据页和相关的日志列表合并，生成最新的数据页版本，然后返回给主节点。
- **优势：** 这种方式避免了昂贵的 Quorum Read（需要联系 `R` 个副本），将读操作的开销降至最低，只需与单个副本交互。

#### 3. 读路径（崩溃恢复）：执行 Quorum Read，修复日志

- **触发时机：** 当原数据库主节点崩溃，一个新的主节点被启动来接管时。
- **问题：** 新主节点不知道旧主节点崩溃前的确切状态。日志流可能在各个副本上是不完整的、有空洞的。例如，某些副本可能有 LSN 101、102、104，但没有一个副本有 103，因为旧主节点在凑齐 LSN 103 的 Write Quorum 之前就崩溃了。
- **如何恢复（Quorum Read 的用武之地）：**
  1.  **发现不一致：** 新主节点向存储副本发起一次 **Quorum Read**（联系至少 3 个副本），目的是重建完整的日志序列。
  2.  **确定截断点：** 通过比较从 `R` 个副本收集到的日志信息，新主节点可以确定整个集群共同拥有的、无空洞的最高连续日志点（例如 LSN 102）。这个点之后的任何日志都被认为是不完整的、不可信的。
  3.  **截断和清理：** 新主节点向所有 6 个副本广播一条命令：“丢弃所有 LSN > 102 的日志记录”。这确保了所有副本都回滚到一个一致的、已确认的状态。
  4.  **回滚未提交事务 (Undo)：** 在截断后的日志（LSN <= 102）中，可能仍然包含一些属于未提交事务的日志记录。新主节点会扫描这些日志，识别出这些“孤儿”记录，并利用日志中的**旧值**生成并应用 **Undo 操作**，撤销这些未完成的修改。

**结论：** Aurora 的读写模型是其设计的精髓所在。它将 Quorum 机制的强一致性保证用在了“刀刃上”——确保日志的持久化和崩溃后的一致性恢复。而在系统正常运行时，则通过聪明的进度追踪机制，走了性能最优的“捷径”。这种在常规路径上追求极致性能，在异常路径上依赖健壮理论的设计哲学，是 Aurora 取得成功的关键。

---

## 数据分片(Protection Group)

### 核心观点

Aurora 通过将大型数据库**分片（Sharding）**成固定大小（10GB）的逻辑单元——**保护组（Protection Group, PG）**——来突破单机存储容量的限制。每个 PG 都是一个独立的、拥有 6 个副本的 Quorum 系统。这种设计的精髓不仅在于实现了存储的水平扩展，更在于它通过**将故障恢复的压力分散到整个存储集群**，实现了**大规模并行化的快速再复制**。当一个承载了数百个 PG 数据块的物理服务器故障时，恢复工作会被分解成数百个小的、独立的恢复任务，由数百个不同的源和目标服务器并行执行，从而将恢复时间从数小时缩短到数十秒。

---

### 逻辑梳理

#### 1. 问题的根源：单机容量限制

- **问题：** 一个标准的 6 副本 Quorum 系统虽然提供了高可用性，但其总容量受限于单个副本的磁盘大小，因为每个副本都存储了完整的数据拷贝。这无法支持 TB 甚至 PB 级别的数据库。

#### 2. 解决方案：分片成保护组 (PG)

- **分片单位：** 数据库的整个数据空间被切分成连续的 10GB 数据块。
- **保护组 (PG)：** 每个 10GB 的数据块都由一个独立的**保护组**来管理。一个 PG 本质上就是一个小型的、自包含的 Aurora 存储系统，拥有自己的 6 个副本，分布在 3 个 AZ。
- **水平扩展：** 一个大型数据库由多个 PG 组成。当数据库增长超过当前 PG 的容量时，系统会自动分配一个新的 PG。例如，一个 100GB 的数据库会由 10 个 PG 组成。

#### 3. 数据与日志的映射

- **数据页映射：** 数据库的每个数据页根据其页号或其他标识符，被唯一地映射到某一个 PG。例如，奇数页在 PG1，偶数页在 PG2。
- **日志映射：** 这是设计的关键点。日志**不是**全局复制的。当数据库主节点生成一条日志记录时，它会检查该日志记录将要修改哪个数据页，然后**只将这条日志记录发送给管理该数据页的那个 PG** 的 6 个副本。
- **结果：** 每个 PG 只存储了数据库数据页的一个子集，以及**与之相关的日志记录**。

#### 4. 革命性的快速恢复机制

这是 PG 设计带来的最大好处。

- **故障场景：** 一个物理存储服务器通常会承载来自**数百个不同 PG** 的 10GB 数据块（每个数据块都属于一个不同的数据库或同一个数据库的不同部分）。当这个服务器（例如，拥有 10TB 数据）宕机时，数百个 PG 都同时失去了一个副本。

- **传统恢复方式（慢）：** 找一台新的空闲服务器，然后从另一台拥有相同 10TB 数据的服务器上，通过网络将全部数据拷贝过来。这会造成巨大的网络瓶颈，耗时极长（数小时）。

- **Aurora 的并行恢复方式（快）：**
  1.  **任务分解：** 恢复 10TB 数据的任务被分解成**数百个**独立的、恢复 10GB 数据块的任务。
  2.  **压力分散：** 系统不会只找一台新服务器来替换。相反，它会从庞大的服务器池中找出**数百台**不同的服务器，每台服务器只负责成为**一个** PG 的新副本。
  3.  **并行复制：**
      - 对于第一个丢失的 10GB 数据块，系统从其 PG 的 5 个幸存副本中任选一个作为源，将数据复制到第一个新副本上。
      - 对于第二个丢失的 10GB 数据块，系统从其 PG 的 5 个幸存副本中任选一个作为源，将数据复制到**第二个**新副本上。
      - ...以此类推。
  4.  **结果：** 数百个 10GB 的数据复制任务在**数百对不同的源-目标服务器之间并行进行**。整个恢复过程的瓶颈不再是单条网络链路的带宽，而是整个数据中心网络的总吞吐量。这使得原本需要数小时的恢复工作，可以在几十秒内完成。

**结论：** Aurora 的保护组设计是一种将“分而治之”思想运用到极致的典范。它不仅解决了存储容量的扩展性问题，更重要的是，它将单个物理节点的故障影响，巧妙地分散到整个分布式系统的多个维度上（多个 PG、多个源服务器、多个目标服务器），从而实现了前所未有的快速故障恢复能力，这是维持大规模云服务高可用性的核心技术。

---

## 只读数据库

### 核心观点

Aurora 通过引入**只读副本（Read Replicas）**来水平扩展读性能，其架构精髓在于**计算与存储的分离**：所有数据库实例（一个主节点，多个只读副本）共享同一个底层的、基于日志的存储系统。只读副本通过两种方式获取数据：一是直接从共享存储中按需拉取数据页（Data Page），二是通过消费主节点广播的实时日志流（Log Stream）来保持自身缓存的更新。这种设计极大地分担了主节点的读取压力，同时通过将复杂的原子性保证（如微事务）下推到存储层，解决了在共享存储模型下可能出现的数据一致性问题。

---

### 逻辑梳理

#### 1. 问题的根源：读密集型负载

- **场景：** 典型的 Web 应用读写比极高（例如 100:1），单个主数据库实例很快会成为读取操作的瓶颈。
- **目标：** 在不影响写性能的前提下，线性地扩展系统的读吞吐能力。

#### 2. 解决方案：共享存储的只读副本

Aurora 引入了最多 15 个只读数据库实例，它们与主实例共同构成一个数据库集群。

- **写路径（单一写入者）：**

  - 所有写请求**仍然只发送给主数据库实例**。
  - **原因：** 维持全局唯一的、单调递增的日志序列号（LSN）至关重要。让多个节点同时写入会使日志排序变得极其复杂，从而破坏系统的一致性基础。

- **读路径（多点读取）：**
  - 读请求可以被负载均衡到任意一个只读副本。
  - 只读副本**直接与共享存储层交互**来获取它需要的数据页，完全绕过了主数据库实例。

#### 3. 只读副本的数据同步机制

只读副本如何确保自己能读到（相对）新的数据？

1.  **日志流广播：** 主数据库实例在将日志记录发送给存储层的同时，也会将这些日志记录**广播**给所有的只读副本。
2.  **缓存更新：** 只读副本接收到日志流后，用它来更新或失效自己内存中的数据页缓存。它**只更新缓存，不写入存储**。
3.  **按需拉取：** 如果一个读请求需要的页面不在只读副本的缓存中，它会直接向共享存储层发起一个读请求来获取该页面。

**结果：** 这种架构实现了读写分离。主节点专注于处理写操作和生成日志，而大量的读操作被分散到多个只读副本上，每个副本都能独立地服务于客户端。

#### 4. 关键的一致性挑战与解决方案

- **挑战 1：数据延迟。** 只读副本消费日志流存在毫秒级的延迟，因此它们提供的是一种**近实时**的、最终一致性的读取。对于绝大多数 Web 应用，这种微小的延迟是完全可以接受的。

- **挑战 2：读取未提交数据。** 日志流中包含了未提交事务的记录。只读副本必须足够智能，**只有在看到事务的 "Commit" 记录后**，才能将该事务相关的变更应用到自己的缓存中，从而避免脏读。

- **挑战 3：读取中间状态（最精妙的部分）。** 像 B-Tree 分裂这样的复杂操作涉及对多个数据页的原子修改。如果一个只读副本在这些修改的中途去存储层读取数据页，它可能会看到一个损坏的、不一致的数据结构。
  - **解决方案：微事务 (Mini-Transactions) & VDL/VCL。**
    - 主数据库可以将一组必须被原子应用的日志记录打包成一个“微事务”。
    - 这个信息被传递给**存储层**。
    - 当存储节点收到来自只读副本的页面读取请求时，它会检查这个页面是否正处于一个微事务的修改过程中。
    - 存储节点保证，它返回给只读副本的页面版本，**要么是微事务开始之前的状态，要么是微事务完成之后的状态**，绝不会是中间状态。
    - 这巧妙地将保证多页面更新原子性的责任，从数据库计算层**下推**到了智能存储层。

### 总结与启示

- **协同设计的力量：** Aurora 的成功证明，打破应用（数据库）与基础设施（存储）之间的传统抽象壁垒，进行深度整合与协同设计，可以带来数量级的性能提升。
- **云环境的现实：** 设计必须考虑 AZ 级故障、普遍存在的慢节点，并认识到**网络是比 CPU 更宝贵的资源**。Aurora “只写日志”的设计哲学，本质上是用 6 个副本的 CPU 计算（应用日志）来换取宝贵的网络带宽。
- **Quorum 的应用：** Quorum 思想是构建容错系统的基石，Raft 是其一种强一致性的实现，而 Aurora 则展示了其在读写性能和容错能力之间进行灵活权衡的强大能力。

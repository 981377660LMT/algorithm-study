好的，我们来详细讲解 MIT 6.824 的第五讲：**L5 Go Threads and Raft**。

这一讲是整个课程的转折点和核心。在 L4 中，我们看到了 Primary-Backup 模型的致命弱点——“脑裂”（Split-Brain）。Raft 算法的出现，就是为了从根本上解决这个问题。它是一个**共识算法 (Consensus Algorithm)**，旨在让一组服务器在出现故障和网络分区的情况下，依然能像一个统一的、可靠的整体一样工作。

Raft 的设计哲学是**为了可理解性 (Designed for Understandability)**。相比于它的前辈 Paxos，Raft 将共识问题分解为几个相对独立的子问题，使得它更容易学习和实现。这也是为什么 6.824 选择 Raft 作为核心实验的原因。

这一讲通常会结合 Go 语言的并发特性来讲解，因为 Lab 2 就是用 Go 实现 Raft。

---

### 1. Raft 的目标：管理一个复制日志

Raft 的核心目标是管理一个**复制日志 (Replicated Log)**。你可以把它想象成一个可靠的、只能追加的日志文件，这个文件被复制到了集群中的每一台服务器上。

- **状态机复制**: 客户端的每一个写请求（例如 `set x = 3`）都会被作为一个**日志条目 (Log Entry)** 追加到这个日志中。
- **达成共识**: Raft 算法的核心任务，就是保证所有服务器上的这个日志，最终都是**完全一致**的。
- **应用状态**: 一旦一个日志条目被 Raft 确认提交（committed），它就可以被安全地应用到上层的状态机（例如一个 KV 存储），从而改变系统的状态。

Raft 通过选举一个唯一的**领导者 (Leader)**，并由 Leader 全权负责管理日志复制，极大地简化了整个过程。这使得 Raft 的行为非常像一个健壮的、能自动处理故障切换的 Primary-Backup 系统。

---

### 2. Raft 的三大子问题

Raft 将共识问题分解为三个部分：

1.  **领导者选举 (Leader Election)**: 当现有 Leader 宕机时，必须选举出一个新的 Leader。
2.  **日志复制 (Log Replication)**: Leader 必须接受来自客户端的日志条目，并将它们复制到集群中的其他服务器（Followers），同时保证日志的一致性。
3.  **安全性 (Safety)**: 保证系统的正确性。例如，如果一个日志条目在某个任期被提交，那么它必须也存在于所有更高任期的 Leader 的日志中。

---

### 3. 核心概念：任期 (Term)

Raft 将时间划分为一个个连续的**任期 (Term)**。每个任期由一个单调递增的整数标识。

- 每个任期最多只有一个 Leader。
- 任期就像一个逻辑时钟，它允许服务器检测到过期的信息（例如来自旧 Leader 的消息）。如果一个服务器收到的消息中包含的任期号比自己当前的任期号小，它会直接拒绝该消息。

---

### 4. 子问题一：领导者选举 (Leader Election)

Raft 集群中的节点有三种状态：**Leader (领导者)**, **Follower (跟随者)**, **Candidate (候选人)**。

- **正常状态**: 系统中有一个 Leader，其余都是 Followers。Leader 负责处理所有事情，Followers 只是被动地响应 Leader 和 Candidate 的请求。
- **选举过程**:
  1.  **超时**: 每个 Follower 都有一个**选举计时器 (election timer)**，其超时时间是一个随机值（例如 150-300ms）。如果 Follower 在这个时间内没有收到来自 Leader 的任何消息（心跳），它就认为 Leader 可能已经宕机。
  2.  **成为 Candidate**: 选举计时器超时的 Follower 会转变为 Candidate 状态。它会：
      - 增加当前的任期号 (`currentTerm++`)。
      - 为自己投一票。
      - 向集群中的所有其他节点发送 `RequestVote` RPC，请求它们为自己投票。
  3.  **投票与结果**:
      - **成为 Leader**: 如果一个 Candidate 收到了来自**超过半数 (N/2 + 1)** 节点的投票（即获得了**多数派/Quorum**），它就赢得了选举，成为新的 Leader。然后它会立即向所有节点发送心跳，宣告自己的地位。
      - **选举失败 (Split Vote)**: 如果多个 Follower 同时成为 Candidate，它们可能会瓜分选票，导致没有任何一个 Candidate 获得多数派。这种情况下，它们会等待自己的选举计时器再次超时，然后开始新一轮的选举（由于超时时间是随机的，下一次冲突的概率很低）。
      - **遇到新 Leader**: 如果一个 Candidate 在等待投票期间，收到了一个声称自己是 Leader 的节点发来的消息（并且该 Leader 的任期号不小于自己的任期号），它会立即承认失败，转变为 Follower 状态。

**关键点**: **多数派 (Quorum)** 机制从根本上解决了“脑裂”问题。因为不可能有两个节点同时获得多数派的投票，所以在一个任期内，最多只能有一个 Leader。

---

### 5. 子问题二：日志复制 (Log Replication)

一旦选举出 Leader，它就开始负责服务客户端请求和复制日志。

1.  **客户端请求**: 客户端的所有请求都发送给 Leader。
2.  **追加日志**: Leader 收到请求后，将命令作为一个新的日志条目追加到自己的日志中。
3.  **并行复制**: Leader 通过 `AppendEntries` RPC 将这个新的日志条目并行地发送给所有的 Followers。
4.  **提交日志**:
    - 当 Leader 收到**多数派** Followers 对该日志条目的成功响应后，它就认为这个条目是**已提交的 (committed)**。
    - Leader 将这个条目应用到自己的状态机。
    - Leader 响应客户端。
    - 在后续的 `AppendEntries` RPC 中，Leader 会通知 Followers 哪些条目已经被提交了，Followers 收到通知后也将这些条目应用到自己的状态机。

**日志一致性维护**:
Raft 通过一个简单的机制来强制保证所有节点的日志都是一致的。当 Leader 给 Follower 发送 `AppendEntries` RPC 时，会带上新条目**之前**的那个条目的索引 `prevLogIndex` 和任期 `prevLogTerm`。

- Follower 收到 RPC 后，会检查自己的日志在 `prevLogIndex` 位置上的条目，看其任期是否与 `prevLogTerm` 匹配。
- **匹配**: 如果匹配，说明到此为止日志是一致的，Follower 就可以安全地追加新的条目。
- **不匹配**: 如果不匹配，说明 Follower 的日志在这里出现了分歧。Follower 会拒绝该请求，并返回失败。
- **修复日志**: Leader 收到失败响应后，会向前回溯，减小 `prevLogIndex`，然后重试 `AppendEntries` RPC。这个过程会一直持续，直到找到 Leader 和 Follower 日志共同匹配的那个点。一旦找到，Leader 就会将该点之后的所有日志条目（即使 Follower 原来有）强制覆盖为 Leader 的日志。

这个机制保证了**Leader 的日志永远是正确的，所有 Follower 的日志最终都会被强制与 Leader 保持一致**。

---

### 6. 子问题三：安全性 (Safety)

为了保证整个系统的正确性，Raft 增加了两个关键的限制：

1.  **选举限制 (Election Restriction)**:

    - 在选举过程中，一个节点 B 不会投票给一个候选人 A，除非 A 的日志**至少和 B 的一样新 (at-least-as-up-to-date)**。
    - “新”的定义是：比较两者日志中最后一个条目的任期号，任期号大的更新；如果任期号相同，则日志更长的更新。
    - **目的**: 这个限制确保了只有拥有最全的、已提交的日志的节点才有可能成为 Leader。这防止了已提交的日志条目在 Leader 切换后丢失。

2.  **提交规则限制**:
    - Leader **只能提交自己当前任期内**的日志条目。它不能直接提交之前任期的日志条目。
    - 之前任期的日志条目是通过提交当前任期的条目而被**间接提交**的。
    - **目的**: 这个规则避免了一些复杂的边界情况，确保了 Leader 能明确知道哪些日志在被复制到多数派后是可以安全提交的。

### 总结

Raft 通过将共识问题分解为**选举**、**复制**和**安全**三个部分，提供了一个比 Paxos 更易于理解和实现的共识算法。

- 它通过**任期**和**多数派投票**机制，优雅地解决了 Primary-Backup 模型的“脑裂”问题。
- 它通过**强制让 Follower 匹配 Leader 日志**的机制，保证了日志复制的一致性。
- 它通过**选举限制**等安全规则，保证了已提交的数据不会丢失。

掌握 Raft 是理解现代分布式数据库、协调服务和各种高可用系统的关键。在 6.824 的 Lab 2 中，你将亲手实现它，这将是一次极具挑战但收获巨大的经历。

---

好的，我们来深入讲解 MIT 6.824 的第六和第七讲：**L6 & L7 Fault Tolerance in Raft**。

在 L5 中，我们学习了 Raft 的“Happy Path”：如何选举 Leader，以及 Leader 如何复制日志。L6 和 L7 则深入探讨 Raft 的灵魂所在：**在各种故障（节点崩溃、网络分区）下，Raft 如何依然能保证系统的正确性（Safety）和可用性（Liveness）**。这两讲是课程中最具挑战性的部分，也是 Lab 2 实现的难点所在。

我们将这两讲的内容整合起来，围绕几个核心的故障场景来展开。

---

### 核心目标：保证安全性 (Safety First!)

Raft 的所有复杂设计，最终都是为了保证一个核心的**安全属性 (Safety Property)**：一旦一个日志条目被应用到状态机（即被提交），那么之后任何 Leader 都必须包含这个条目，它永远不会被修改或删除。

为了实现这个目标，Raft 引入了 **Leader 完整性原则 (Leader Completeness Property)**：

> 如果一个日志条目在某个任期被提交，那么它必然会出现在所有更高任期的 Leader 的日志中。

下面我们来看 Raft 是如何通过处理各种故障来维护这个原则的。

---

### 场景一：Follower 节点崩溃

这是最简单的故障模式。

- **发生了什么**: 一个 Follower 节点宕机或与网络断开。
- **Raft 如何处理**:
  1.  Leader 会持续向该 Follower 发送 `AppendEntries` RPC（包括心跳）。由于节点已死，Leader 不会收到任何响应。
  2.  Leader 会无限期地重试，但它**不会被阻塞**。
  3.  由于系统中的其他节点（一个多数派）仍然是健康的，Leader 依然可以从它们那里获得对新日志条目的确认。因此，**系统可以继续正常处理客户端请求并提交新的日志**。
  4.  当崩溃的 Follower 恢复后，它会重新加入集群。此时它的日志已经落后了。
  5.  Leader 会通过之前讲过的**日志一致性检查机制**发现该 Follower 的日志不匹配。Leader 会从后往前，逐一递减 `nextIndex`，找到与该 Follower 共同拥有的最后一个日志条目，然后将该点之后的所有日志（Leader 的日志）强制推送给这个 Follower，使其恢复同步。

**结论**: Follower 崩溃不会影响系统的可用性，Raft 的日志修复机制可以自动处理节点恢复后的数据同步问题。

---

### 场景二：Leader 节点崩溃

这是最关键的故障切换场景。

- **发生了什么**: Leader 节点宕机。
- **Raft 如何处理**:

  1.  **故障检测**: 所有 Followers 会因为收不到 Leader 的心跳而导致自己的**选举计时器超时**。
  2.  **选举新 Leader**: 一个或多个 Followers 会转变为 Candidate，发起新一轮选举。由于旧 Leader 已死，某个 Candidate 最终会获得多数派投票，成为新任期的 Leader。
  3.  **保证安全性的关键**: 新选举出的 Leader **必须包含所有已经提交的日志条目**。Raft 是如何保证这一点的？

      - **回顾提交规则**: 一个条目被提交，意味着它已经被复制到了**多数派**的节点上。
      - **回顾选举规则**: 一个 Candidate 要想成为 Leader，必须获得**多数派**的投票。
      - **交集保证 (Quorum Intersection)**: 这两个“多数派”必然有**至少一个重叠的节点**。这意味着，任何一个想成为 Leader 的 Candidate，在它寻求投票的节点中，必然至少有一个节点拥有那条已经提交的日志。
      - **选举限制 (Election Restriction)**: Raft 规定，一个节点不会投票给一个日志没有自己“新”的 Candidate。因此，那个拥有已提交日志的节点，不会投票给一个没有该日志的 Candidate。
      - **推论**: 最终能赢得选举成为 Leader 的节点，其日志必然包含了所有已提交的条目。

  4.  **新 Leader 的工作**: 新 Leader 上任后，它会强制所有 Follower 的日志与自己保持一致。对于旧 Leader 未能成功复制到多数派的、未提交的日志条目，它们可能会被新 Leader 的日志所覆盖。**这是允许的，因为它们从未被提交，客户端也从未收到过成功的确认。**

**结论**: Leader 崩溃会触发一次新的选举。Raft 的选举规则和多数派机制确保了新 Leader 的合法性，并保证了所有已提交的数据都不会丢失。

---

### 场景三：网络分区 (Network Partition) - “脑裂”的终极考验

这是最复杂的场景，也是 Raft 设计精髓的体现。

假设一个 5 节点的集群 (S1, S2, S3, S4, S5)，S1 是 Leader。现在网络发生分区，将集群分成了两个部分：

- **少数派分区**: `{S1, S2}`
- **多数派分区**: `{S3, S4, S5}`

#### 在少数派分区 `{S1, S2}` 中：

- 旧 Leader S1 仍然活着，它可能还认为自己是 Leader。
- 它尝试向所有节点发送心跳和日志，但它只能联系到 S2。它永远无法收到来自多数派（至少 3 个节点）的响应。
- **关键**: 根据 Raft 的提交规则，S1 **无法提交任何新的日志条目**。它可以接收客户端请求，并将它们追加到自己的本地日志中，但这些日志永远处于未提交状态。
- **结论**: 旧 Leader 在少数派分区中“名存实亡”，无法对系统状态做出任何“已提交”的更改。

#### 在多数派分区 `{S3, S4, S5}` 中：

- S3, S4, S5 收不到来自 S1 的心跳，它们的选举计时器会超时。
- 它们会发起一次新的选举。由于它们构成了一个多数派，它们内部可以成功选举出一个新的 Leader（比如 S3）。
- 新的 Leader S3 可以接收客户端请求，并且因为它能联系到多数派（S3, S4, S5），所以它**可以正常提交新的日志条目**。
- **结论**: 系统在多数派分区中继续保持可用，并能正常工作。

#### 当网络分区恢复后：

- 旧 Leader S1 终于能联系到 S3, S4, S5 了。它可能会尝试发送一个 `AppendEntries` RPC。
- S3（新 Leader）会收到这个来自 S1 的 RPC，发现其任期号（Term）低于自己的当前任期号。
- S3 会拒绝 S1 的请求，并回复自己的、更高的任期号。
- S1 收到这个回复后，会意识到自己已经“过时”了，它会立即放弃 Leader 地位，转变为一个 Follower，并更新自己的任期号。
- 然后，S1 会像一个普通的 Follower 一样，开始接收来自新 Leader S3 的日志，并用 S3 的日志来覆盖自己本地那些在分区期间产生的、未提交的日志。

**最终结论**: Raft 通过**任期号 (Term)** 和**多数派 (Quorum)** 这两大武器，完美地解决了“脑裂”问题。在任何时候，系统中最多只有一个功能正常的 Leader（能够提交日志的 Leader），从而保证了数据的一致性。

---

### 补充：持久化与快照 (Persistence & Snapshotting)

这两讲还会涉及 Raft 实现中的两个重要工程实践问题，这通常是 Lab 2C 的内容。

- **持久化 (Persistence)**:

  - 为了在节点崩溃重启后能恢复状态，Raft 的一些关键状态必须被持久化到稳定存储（如磁盘）中。
  - **必须持久化的状态**: `currentTerm`, `votedFor`, `log[]`。
  - **时机**: 这些状态必须在响应对应的 RPC 之前被写入磁盘。例如，在投票给一个 Candidate 后，必须先将 `votedFor` 和 `currentTerm` 持久化，然后再发出投票响应。否则，如果在投票后、持久化前崩溃，重启后可能会在同一个任期内投票给另一个 Candidate，破坏了安全性。

- **快照 (Snapshotting)**:
  - 随着系统运行，Raft 的日志会无限增长，占用大量磁盘空间，并且在节点重启后需要很长时间来重放。
  - **解决方案**: 定期地对系统的当前状态（状态机）进行一次快照，并将其持久化。一旦快照完成，快照点之前的所有日志都可以被安全地丢弃。
  - 当一个新节点加入，或者一个落后很多的节点需要同步时，Leader 可以直接发送最新的快照给它，而不是从头开始发送所有日志，这大大提高了恢复效率。

这两部分内容将理论上的 Raft 算法，变成了一个可以在生产环境中长期稳定运行的、健壮的系统。

---

好的，这是一个非常棒的、结构化的 Raft 学习大纲。它将 Raft 的知识点分成了两个逻辑部分：第一部分是基础机制，第二部分是容错和工程实践。我们来逐一详细讲解。

---

### Lecture 06 - Raft 1: 基础与核心机制

这一讲主要介绍 Raft 为什么被设计出来，以及它的核心工作流程，包括选举和基本的日志复制。

#### 6.1 脑裂（Split Brain）

- **是什么**：这是分布式系统中的一个经典问题，特指在发生网络分区等故障时，一个集群被分割成多个独立的子集，而每个子集都错误地认为自己是唯一的主节点（Leader），并开始独立处理请求。
- **后果**：系统中出现了多个“大脑”，它们各自维护着不同的数据状态。当网络恢复后，这些分叉的数据无法自动合并，导致数据不一致甚至数据丢失。这是 Primary-Backup 模型的主要缺陷。
- **Raft 的目标**：Raft 的核心设计目标之一就是从根本上杜绝脑裂的发生。

#### 6.2 过半票决（Majority Vote）

- **是什么**：也称为“多数派”或“Quorum”。在一个包含 N 个节点的集群中，多数派指的是至少 `(N/2 + 1)` 个节点。
- **如何解决脑裂**：这是 Raft 防止脑裂的数学基石。因为一个集群不可能同时存在两个互不重叠的多数派，所以也就不可能有两个节点同时获得多数派的投票。这就保证了在任何一个任期（Term）内，最多只能有一个 Leader。
- **在 Raft 中的应用**：
  1.  **选举**：一个 Candidate 必须获得多数派的投票才能成为 Leader。
  2.  **日志提交**：一个日志条目必须被成功复制到多数派节点上，Leader 才能将其标记为“已提交”（committed）。

#### 6.3 Raft 初探

- **核心思想**：Raft 是一个用于管理复制日志（Replicated Log）的共识算法。它通过选举一个唯一的 Leader 来简化管理，使其行为类似于一个能自动容错的 Primary-Backup 系统。
- **三大角色**：
  - **Leader (领导者)**：处理所有客户端请求，管理日志复制。
  - **Follower (跟随者)**：被动接收 Leader 的日志和心跳。
  - **Candidate (候选人)**：在选举期间的临时角色。
- **任期 (Term)**：Raft 将时间划分为一个个的“任期”，每个任期有一个单调递增的整数 ID。任期就像朝代更迭，每个任期最多只有一个 Leader（皇帝）。它作为逻辑时钟，用于识别过时的信息。

#### 6.4 Log 同步时序

这是指在正常情况下，一条日志从被客户端请求到被安全提交的完整生命周期：

1.  客户端将命令发送给 Leader。
2.  Leader 将命令作为新条目追加到自己的本地日志中。
3.  Leader 并行地向所有 Follower 发送 `AppendEntries` RPC。
4.  Follower 收到后，将条目追加到自己的本地日志，并向 Leader 回复成功。
5.  Leader 收到**多数派** Follower 的成功回复后，将该条目**提交 (commit)**。
6.  Leader 将该条目应用到自己的状态机，并向客户端返回结果。
7.  在后续的 `AppendEntries` RPC 中，Leader 会通知所有 Follower 该条目已被提交，Follower 收到通知后也将该条目应用到自己的状态机。

#### 6.5 日志（Raft Log）

- **结构**：一个日志条目的数组，每个条目包含：
  - **Index**：整数索引，表示其在日志中的位置。
  - **Term**：该条目被 Leader 创建时的任期号。
  - **Command**：要被状态机执行的命令。
- **作用**：Raft 的核心就是保证所有节点上的这个日志数组最终完全一致。`Term` 和 `Index` 的组合可以唯一地标识一个日志条-目，这在后续的日志一致性检查中至关重要。

#### 6.6 应用层接口

Raft 本身只是一个管理复制日志的算法库，它需要向上层应用（如 KV 存储）提供接口。

- **`Start(command)`**：上层应用调用此接口提交一个新的命令。Raft 负责将此命令复制到多数派并提交。
- **`applyCh` (Apply Channel)**：Raft 库通过这个 Go Channel，将**已提交**的日志条目按顺序发送给上层应用。上层应用从这个 Channel 中取出条目，并将其应用到自己的状态机（例如，执行 `map[k]=v`）。

#### 6.7 Leader 选举（Leader Election）

当 Follower 在一段时间内（选举超时）没有收到 Leader 的心跳时，它会发起选举：

1.  **转为 Candidate**：增加自己的 `currentTerm`，为自己投一票。
2.  **发送 `RequestVote` RPC**：向所有其他节点请求投票。
3.  **等待结果**：
    - **获胜**：收到多数派投票，成为新 Leader。
    - **失败**：收到了来自其他合法 Leader 的消息，转回 Follower。
    - **平票**：选举超时，开始新一轮选举。

#### 6.8 选举定时器（Election Timer）

- **作用**：每个 Follower 都有一个选举定时器。如果在超时时间内没有收到 Leader 的消息，就触发选举。
- **随机化**：这个超时时间必须是**随机的**（例如 150-300ms）。这大大降低了多个 Follower 同时超时、发起选举导致平票（Split Vote）的概率，保证了选举能够快速收敛。

#### 6.9 可能的异常情况

这一讲主要讨论基础的异常，如：

- **Follower 崩溃**：不影响系统，恢复后 Leader 会为其同步日志。
- **Leader 崩溃**：触发新一轮选举。
- **网络分区**：Raft 通过多数派机制保证只有在多数派分区中的节点才能选举出新 Leader 并提交日志，少数派分区中的旧 Leader 会因为无法联系到多数派而失效。

---

### Lecture 07 - Raft 2: 容错、持久化与优化

这一讲深入 Raft 的容错细节和工程实践，确保其在各种复杂场景下的正确性和高效性。

#### 7.1 日志恢复（Log Backup）

- **问题**：当一个 Follower 崩溃恢复后，或者网络分区恢复后，它的日志可能与 Leader 不一致（落后、或者在分区期间跟随了错误的 Leader 导致分叉）。
- **解决方案**：Leader 为每个 Follower 维护一个 `nextIndex`，表示下一个要发给该 Follower 的日志条目索引。
  1.  Leader 发送 `AppendEntries` RPC，其中包含 `prevLogIndex` 和 `prevLogTerm`。
  2.  Follower 检查自己的日志在 `prevLogIndex` 处的条目是否与 `prevLogTerm` 匹配。
  3.  如果不匹配，Follower 拒绝请求。
  4.  Leader 收到拒绝后，将该 Follower 的 `nextIndex` 减一，然后重试。
  5.  这个过程不断重复，直到找到 Leader 和 Follower 日志共同匹配的点。之后，Leader 会强制用自己的日志覆盖 Follower 在该点之后的所有条目，使其恢复一致。

#### 7.2 选举约束（Election Restriction）

- **问题**：如果一个日志落后的 Follower 被选举为 Leader，它可能会用自己不完整的日志覆盖掉其他节点上已经提交的日志，导致数据丢失。
- **解决方案**：Raft 增加了一条选举规则：一个节点 B 不会投票给一个候选人 A，除非 A 的日志**至少和 B 的一样新**。
  - “新”的比较规则：先比较最后一个日志条目的任期号，任期号大的更新；如果任期号相同，则日志更长的更新。
- **保证**：这个约束确保了只有拥有最全的、已提交日志的节点才有可能赢得选举，从而保证了 **Leader 完整性 (Leader Completeness)**，即已提交的日志永不丢失。

#### 7.3 快速恢复（Fast Backup）

这是对 7.1 中日志恢复过程的一个优化。如果 Follower 的日志与 Leader 相差很远，Leader 逐一递减 `nextIndex` 会导致很多轮无效的 RPC。

- **优化方案**：当 Follower 拒绝 `AppendEntries` RPC 时，它可以额外返回一些信息，帮助 Leader 更快地定位到冲突点。例如，返回冲突条目的任期号和该任期号在自己日志中首次出现的索引。Leader 可以利用这些信息直接跳到正确的 `nextIndex`，从而减少恢复所需的网络来回次数。

#### 7.4 持久化（Persistence）

为了防止节点崩溃重启后丢失状态，Raft 的三个关键状态必须在改变后、响应 RPC 前被持久化到稳定存储（如磁盘）中：

1.  `currentTerm`：当前的任期号。
2.  `votedFor`：在当前任期内投票给了谁。
3.  `log[]`：日志条目数组。

- **重要性**：例如，如果一个节点投票后，在响应 RPC 前崩溃，若 `votedFor` 未持久化，它重启后可能会在同一个任期内再次投票给别人，这就破坏了“一个任期只投一票”的原则，可能导致脑裂。

#### 7.5 日志快照（Log Snapshot）

- **问题**：随着时间推移，日志会无限增长，占用大量空间，并且节点重启后重放日志的时间会非常长。
- **解决方案**：
  1.  当日志增长到一定大小时，Raft 对当前的状态机进行一次**快照**，并将快照持久化。
  2.  快照之后，快照点之前的所有日志都可以被安全地丢弃。
  3.  当一个落后很多的 Follower 需要同步时，Leader 可以直接将最新的快照发送给它，然后再发送快照点之后的日志。这比从头发送所有日志要高效得多。

#### 7.6 线性一致（Linearizability）

- **Raft 能否实现线性一致性？** Raft 本身只保证日志的一致性。通过一些额外的措施，可以在 Raft 之上构建一个提供线性一致性的系统。
- **读操作的问题**：如果允许 Follower 处理读请求，可能会读到旧数据（因为 Follower 的状态可能落后于 Leader）。
- **实现线性一致读的方法**：
  1.  **所有读都走 Leader**：这是最简单的方法，但 Leader 会成为瓶颈。
  2.  **Read-Index Read**：Leader 在收到读请求时，记录下自己当前的 `commitIndex`。然后它需要确认自己仍然是 Leader（例如，通过与多数派交换一次心跳）。确认后，它等待自己的状态机至少应用到 `commitIndex`，然后再执行读操作并返回结果。
  3.  **Lease Read**：Leader 获取一个租约（Lease），在租约期内它可以安全地认为自己是 Leader，从而直接响应读请求，无需每次都与多数派通信。这是一种优化，但依赖于时钟的准确性。

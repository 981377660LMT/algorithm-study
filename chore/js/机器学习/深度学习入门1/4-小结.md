1. 普通梯度下降适用于中小规模数据集，具有精确和稳定的优点，但在大规模数据集上效率低下。
2. **随机梯度下降（SGD）**通过每次使用单个样本更新，显著提高了计算效率和训练速度，但引入了梯度估计的不稳定性。
3. Mini-Batch 梯度下降在计算效率和梯度稳定性之间取得了平衡，广泛应用于实际深度学习训练中。
4. 动量方法通过引入动量加速训练过程，减少震荡，提升收敛速度和稳定性，但需要额外的超参数调节和存储开销。

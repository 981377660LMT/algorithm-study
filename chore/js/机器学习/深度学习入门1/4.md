优化算法(Optimization Algorithm)、Optimizer

## SGD(随机梯度下降法, Stochastic Gradient Descent)

随机体现在每次迭代时，随机选择一个样本进行梯度下降。

```python
class SGD:
  def __init__(self, lr=0.01):
    self.lr = lr

  def update(self, params, grads):
    for key in params.keys():
      params[key] -= self.lr * grads[key]

net = TwoLayerNet(input_size=784, hidden_size=100, output_size=10)
optimizer = SGD()

for _ in range(10000):
  x_batch, t_batch = get_mini_batch()  # 128个样本为一批
  grads = net.gradient(x_batch, t_batch)
  optimizer.update(net.params, grads)
```

- 优点：
  实现简单，易于理解。
- 缺点：
  在梯度方向上进行更新，可能会出现震荡(zig-zag)，收敛速度慢。梯度的方向不指向最低点。
  ![alt text](image-5.png)

## Momentum (动量)

优化SGD的方法，引入了“惯性”的概念，即在更新参数时，不仅考虑当前的梯度方向，还考虑之前的更新方向。

```python
class Momentum:
  def __init__(self, lr=0.01, momentum=0.9):
    self.lr = lr  # 学习率
    self.momentum = momentum  # 动量系数
    self.v = None  # 速度

  def update(self, params, grads):
    if self.v is None:
      self.v = {}
      for key, val in params.items():
        self.v[key] = np.zeros_like(val)

    for key in params.keys():
      self.v[key] = self.momentum * self.v[key] - self.lr * grads[key]
      params[key] += self.v[key]
```

- 学习率为什么要衰减 (Learning Rate Decay)

加速初期收敛、稳定后期训练、提高模型性能并防止过拟合

```python
import torch.optim as optim

# 定义优化器
optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)

# 定义学习率调度器，每30个epoch将学习率减少10倍
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)

for epoch in range(num_epochs):
    train(...)
    validate(...)
    # 更新学习率
    scheduler.step()
```

## AdaGrad (Adaptive Gradient Algorithm，自适应梯度算法)

思路：对于每个参数，根据其`历史梯度的平方和`来减小学习率。

```py
class AdaGrad:
  def __init__(self, lr=0.01):
    self.lr = lr
    self.h = None  # 梯度平方和

  def update(self, params, grads):
    if self.h is None:
      self.h = {}
      for key, val in params.items():
        self.h[key] = np.zeros_like(val)

    for key in params.keys():
      self.h[key] += grads[key] * grads[key]
      params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)  # 防止除零
```

缺点：学习率不断减小，梯度消失问题。

## AdaDelta(自适应学习率算法, 是AdaGrad的改进版)

思路：不再使用学习率，而是使用`梯度平方的指数加权平均`来调整学习率。

## Nesterov (Nesterov Accelerated Gradient，NAG，梯度加速法)

`提前刹车`

```py
class Nesterov:
  def __init__(self, lr=0.01, momentum=0.9):
    self.lr = lr
    self.momentum = momentum
    self.v = None

  def update(self, params, grads):
    if self.v is None:
      self.v = {}
      for key, val in params.items():
        self.v[key] = np.zeros_like(val)

    for key in params.keys():
      self.v[key] *= self.momentum
      self.v[key] -= self.lr * grads[key]
      params[key] += self.momentum * self.momentum * self.v[key] - (1 + self.momentum) * self.lr * grads[key]
```

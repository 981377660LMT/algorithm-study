## 感知机

### Sigmoid 函数

**Sigmoid** 函数是常用的激活函数，公式为：

\[
\sigma(x) = \frac{1}{1 + e^{-x}}
\]

特点：

- **输出范围**：将输入映射到 \( (0, 1) \) 区间。
- **用途**：适用于二分类问题，输出可解释为概率值。

### Softmax 函数

**Softmax** 函数用于多分类问题，将输入向量转换为概率分布，公式为：

\[
\sigma(z)_i = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}
\]

其中，\( K \) 为类别数，\( z_i \) 为第 \( i \) 个类别的输入值。

特点：

- **输出范围**：将输入向量映射到 \( (0, 1) \) 区间，且所有输出之和为 1。
- **用途**：用于多分类模型的最后一层，得到每个类别的概率。

### mnist 数据集

## 损失函数

### 均方误差(MSE)

### 交叉熵(Cross Entropy)

### Mini-Batch

**Mini-Batch** 是深度学习中常用的一种训练方式，介于批量梯度下降和随机梯度下降之间。使用 Mini-Batch 方法计算损失函数和更新参数时，每次使用一小部分数据（称为一个批次）来计算梯度。

**优点：**

- **计算效率高**：相比全量数据，Mini-Batch 减少了每次更新的计算量，提升了训练速度。
- **收敛稳定**：相比单个样本，使用 Mini-Batch 可以获得更稳定的梯度估计，有助于模型收敛。
- **利用向量化**：可以充分利用矩阵运算，加速计算过程。

**损失函数计算：**

对于每个 Mini-Batch，损失函数 \( L \) 定义为该批次内所有样本损失的平均值：

\[
L*{\text{mini-batch}} = \frac{1}{m} \sum*{i=1}^{m} L^{(i)}
\]

其中，\( m \) 是 Mini-Batch 的大小，\( L^{(i)} \) 是第 \( i \) 个样本的损失。

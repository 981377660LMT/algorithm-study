这节课的内容是离散特征的处理，包括 one-hot encoding (独热编码) 和 embedding (嵌入)。这节课是为后面几节课的向量召回做准备。

---

这是王树森关于**离散特征处理：One-hot 编码与 Embedding** 的讲解。这节课是为后续学习 **向量召回（Vector Recall/Embedding-based Retrieval）** 打基础的关键前置课程。

以下是逻辑清晰、一针见血的分析：

### 一、 核心问题：如何让计算机理解“离散特征”？

在推荐系统中，处理的数据主要分为两类：

- **连续特征 (Continuous):** 如年龄、价格、阅读时长。这些数字天然有大小含义，计算机能直接处理。
- **离散特征 (Categorical):** 如性别（男/女）、国籍（中/美/印）、ID（用户 ID/物品 ID）。
  - **痛点:** 这些特征本质是符号，没有数学上的大小关系（不能说 中国 < 美国），计算机无法直接运算。
  - **解决:** 必须将这些离散符号映射为**数值向量**。

---

### 二、 基础方案：One-hot 编码 (独热编码)

这是最直观的编码方式。

**1. 原理:**

- 建立一个字典，每个类别分配一个序号（Index）。
- 生成一个长向量，其长度等于类别总数。
- **规则:** 只有该类别序号对应的位置为 1，其余全为 0。

**2. 示例 (国籍):**

- 字典: {中国: 1, 美国: 2, 印度: 3 ...} (共 200 个)
- 中国 $\rightarrow$ `[0, 1, 0, 0, ..., 0]` (维度=200)
- 美国 $\rightarrow$ `[0, 0, 1, 0, ..., 0]` (维度=200)

**3. 局限性 (维度灾难):**

- **稀疏 (Sparse):** 对于性别（2 类）、国籍（200 类）这种**小基数**特征，One-hot 很好用。
- **爆炸:** 对于物品 ID（几亿个）、单词（几万个）这种**大基数**特征，One-hot 向量维度高达几亿。这会导致：
  - 存储空间爆炸。
  - 计算极其低效（绝大多数位置是 0）。
  - **无法表达语义:** 任意两个 One-hot 向量都是正交的（距离恒定），无法体现“动画片与动画片相似”这种语义关系。

---

### 三、 进阶方案：Embedding (嵌入)

这是深度学习推荐系统的核心基石。

**1. 原理:**

- 将高维稀疏的 One-hot 向量映射为**低维稠密 (Low-dimensional Dense)** 向量。
- **本质:** 查表 (Lookup Table)。Embedding 层本质上是一个巨大的参数矩阵 $W$。

**2. 参数规模计算:**

- **公式:** `参数量 = 类别数量 (Vocabulary Size) × 向量维度 (Embedding Size)`
- **示例 (电影推荐):**
  - 电影数量: 10,000 部
  - Embedding 维度: 16
  - 参数矩阵大小: $10,000 \times 16$
  - 参数总量: 160,000 个浮点数。

**3. 数学本质:**

$$
\text{Embedding Vector} = W \times \text{One-hot Vector}
$$

由于 One-hot 向量只有第 $i$ 位是 1，矩阵乘法实际上等价于**取出矩阵 $W$ 的第 $i$ 列**。

**4. 物理意义 (语义空间):**

- Embedding 不仅仅是降维，它还能**学习语义**。
- 经过训练后，相似的物品在向量空间中距离会很近。
  - _例子:_ 所有的“动画片”在 Embedding 空间聚在一起，“间谍片”聚在另一堆。
  - 这一点是 One-hot 绝对做不到的。

---

### 四、 一针见血的总结

| 特征         | One-hot 编码                      | Embedding                                |
| :----------- | :-------------------------------- | :--------------------------------------- |
| **向量性质** | **高维、稀疏** (High-dim, Sparse) | **低维、稠密** (Low-dim, Dense)          |
| **数值表达** | 0 或 1                            | 任意实数 (通常在-1 到 1 之间)            |
| **语义表达** | 无（所有类别距离相等）            | **有**（语义相似的距离近）               |
| **适用场景** | 类别少的特征 (性别, 星期几)       | 类别多的特征 (用户 ID, 物品 ID, 文本)    |
| **工程实现** | 简单逻辑                          | 神经网络的一层，通过反向传播**学习**得到 |

**核心 Takeaway:**
在推荐系统中，处理 ID 类海量特征（几亿级别）时，**Embedding 是唯一可行的方案**。它是连接离散符号世界与连续向量运算世界的桥梁，也是后续所有“向量召回”算法的起点。

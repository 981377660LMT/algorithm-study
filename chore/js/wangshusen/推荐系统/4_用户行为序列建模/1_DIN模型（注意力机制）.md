上节课介绍了用户的 LastN 序列特征。这节课介绍 DIN 模型，它是对 LastN 序列建模的一种方法，效果优于简单的平均。DIN 的本质是注意力机制（attention）。DIN 是阿里在 2018 年提出的，有兴趣的话可以阅读下面的参考文献。

---

这节课介绍了推荐系统序列建模中最经典的模型之一 —— **DIN (Deep Interest Network)**。

DIN 由阿里巴巴在 2018 年提出，它不仅是序列建模的里程碑，更是**Attention 机制在工业界推荐系统落地的开山之作**。它的核心思想直接挑战了上一节课讲的 "Pooling / Averaging" 方法。

以下是对这节课内容的深度逻辑拆解与总结：

### 1. 核心痛点：用户的兴趣是多样的，且是“被激活”的

上节课讲的 **Average Pooling** 有一个致命缺陷：**无差别对待**。

- _场景_：小明既是个吃货（以前点过红烧肉），也是个极客（以前点过显卡）。
- _Pooling_：把红烧肉 Embedding 和显卡 Embedding 取平均，得到一个“半吃货半极客”的混合向量。
- _预测_：假设现在要预测小明对“新款手机”的点击率。
  - Pooling 向量：拿着那个混合向量去预测。显然，“红烧肉”的信息对预测“手机”其实是噪音，反而稀释了“显卡”这个强信号的作用。
- _DIN 的洞察_：**用户对不同候选物品的兴趣，取决于历史记录中相关的部分。** 看手机时，只有历史记录里的“显卡”生效；看零食时，只有历史记录里的“红烧肉”生效。

### 2. DIN 解决方案：Attention (加权平均)

DIN 引入了 **Attention 机制** 来实现“局部激活（Local Activation）”。

#### 结构拆解：

- **Query ($Q$)**: **候选物品 (Candidate Item)**。即模型当前正在打分的那个物品。
- **Keys / Values ($X_1, X_2, \dots, X_N$)**: **用户历史行为序列 (LastN Items)**。
- **Attention Score ($\alpha_i$)**: 候选物品 $Q$ 与历史物品 $X_i$ 的相关性/相似度。
  - $\alpha_i = f(Q, X_i)$。
  - _注意_：这里通常不是简单的点积，而是一个小的 MLP 网络（Activation Unit）来计算，输入是 $[Q, X_i, Q-X_i, Q*X_i]$ 等。
- **Output**: 加权求和。
  - $$ \mathbf{v}_{user_interest} = \sum_{i=1}^{N} \alpha_i \cdot \mathbf{X}\_i $$

#### 物理意义：

当 DIN 预测“新款手机”时：

- History: [红烧肉, 显卡, 连衣裙]
- Attention Weights:
  - $\alpha (\text{手机}, \text{红烧肉}) \approx 0.01$ (完全不相关)
  - $\alpha (\text{手机}, \text{显卡}) \approx 0.9$ (高度相关)
  - $\alpha (\text{手机}, \text{连衣裙}) \approx 0.05$ (不太相关)
- **结果**：最终的用户向量几乎就是“显卡”向量的翻版，去除了“红烧肉”和“连衣裙”的噪音。这使得模型对当前候选物品的感知极其敏锐。

### 3. DIN 的适用范围

这是一个非常重要的工程考量点，王树森老师特别强调了 DIN **只能用于精排 (Ranking)**，而不能用于 双塔召回 或 三塔粗排。

- **为什么不能用于双塔？**
  - 双塔的核心是**用户塔和物品塔解耦**。用户塔输出向量时，根本不知道（也不允许知道）候选物品是谁。
  - 而 DIN 的计算逻辑核心是 $Q$ (候选物品) 去查 $X$ (历史记录)。没有 $Q$，Attention 就算不出来。
- **为什么能用于精排？**
  - 精排阶段是 `Early Fusion`，输入本来就是 (User, Candidate Item) 的 Pair 对，完全具备计算 Attention 的条件。

### 4. 总结与评价

| 特性         | Average Pooling (上节课)            | DIN (这节课)                              |
| :----------- | :---------------------------------- | :---------------------------------------- |
| **计算方式** | $Q$ 无关，恒定平均                  | $Q$ 相关，动态加权                        |
| **用户表征** | **静态的** (同一个用户只有一个向量) | **动态的** (面对不同候选，用户向量会变)   |
| **捕捉能力** | 捕捉全局宽泛兴趣                    | **捕捉局部精准兴趣**                      |
| **适用阶段** | 召回、粗排、精排                    | **仅限精排**                              |
| **计算量**   | 极小                                | 较大 (每次推理都要对序列算一遍 Attention) |

这节课展示了深度学习如何通过 Attention 机制模拟人类“**逛淘宝**”时的心理活动：我们脑子里装着很多兴趣，但看到什么商品，就会自动调取相关的记忆，而忽略不相关的信息。DIN 就是这种心理机制的数学表达。

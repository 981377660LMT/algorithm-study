这节课介绍 LHUC 这种神经网络结构，可以用于精排。LHUC 的起源是语音识别，后来被应用到推荐系统，快手将其称为 PPNet，现在已经在业界广泛落地。

视频中遗漏一个细节：将 LHUC 用于推荐系统，门控神经网络（2 x sigmoid）的梯度不要传递到用户 ID embedding 特征，需要对其做 stop gradient。

---

这节课介绍的 **LHUC (Learning Hidden Unit Contribution)**，在快手的推荐系统中被称为 **PPNet (Parameter Personalized Net)**。这是一种非常巧妙的“**强个性化**”技巧，专门用于精排模型中。

LHUC 的核心思想可以概括为：**“千人千面 (Personalization)” 不仅体现在输入特征上，更应该体现在网络参数（神经元权重）上。**

以下是对这节课逻辑的深度拆解与总结：

### 1. 技术起源：语音识别 (ASR)

- **问题**：语音识别模型如果是通用的，识别不同口音、不同音色的人（例如男女老少、方言）效果会有折扣。
- **想法**：如果我们能让神经网络对每个人都不一样就好了。
- **LHUC 方案**：保留一个通用的主网络，但在每一层输出后，乘上一个与**说话人特质 (Speaker ID)** 相关的系数向量。这就相当于给每个神经元加了一个“个性化放大器”。

### 2. 迁移到推荐系统：PPNet (LHUC)

在推荐系统中，我们把“说话人”换成“用户 (User)”，把“语音信号”换成“物品及其他特征 (All Features)”。

#### 结构拆解：

1.  **Shared Bottom (主网络)**：

    - 这是一个标准的全连接网络（MLP），处理输入的大拼盘特征（User + Item + Context）。
    - 每一层全连接层输出一个特征向量 $\mathbf{x}_{base}$。

2.  **Gate Network (门控网络)**：

    - **独立输入**：只输入**用户 ID / 用户画像特征**（这是 PPNet 的精髓，专门针对 User 进行调控）。
    - **结构**：一个小型的 MLP。
    - **激活函数**：**$2 \times \text{Sigmoid}$**。
      - Sigmoid 输出范围是 $(0, 1)$。
      - 乘以 2 后，输出范围是 **$(0, 2)$**。
      - _为什么是 0 到 2？_
        - 接近 1：保持原样（不干预）。
        - 大于 1：放大该特征（更关注）。
        - 小于 1：缩小该特征（抑制）。
        - 0：完全屏蔽该特征。
    - **输出**：得到一个个性化系数向量 $\mathbf{g}_{user}$。**这也要求 $\mathbf{g}_{user}$ 的维度必须和主网络的 $\mathbf{x}_{base}$ 维度完全一致。**

3.  **Hadamard Product (逐元素相乘)**：
    - 将主网络的输出与门控网络的输出相乘：
      $$ \mathbf{x}_{final} = \mathbf{x}_{base} \odot \mathbf{g}\_{user} $$
    - 这个 $\mathbf{x}_{final}$ 再作为下一层的输入，重复每一层都做这个操作。

### 3. PPNet 的物理意义

- **动态参数**：普通 DNN 对于所有用户，隐层的权重矩阵 $W$ 都是固定的。而 PPNet 相当于动态调整了每一层神经元的激活强度。
- **软掩码 (Soft Attention)**：这是一种针对全连接层通道的 Attention 机制。对于特定的用户（比如“数码宅男”），Gate Network 会自动学会把某些代表“美妆/穿搭”维度的神经元输出给抑制住（乘以 0.1），而把代表“显卡/参数”维度的神经元输出放大（乘以 1.8）。

### 4. 工业界应用 (快手 PPNet)

- **位置**：仅用于**精排 (Fine Ranking)**。因为 Gate Network 需要为每个用户专门计算一次，计算量相对较大，不适合粗排或召回。
- **效果**：显著提升了个性化程度。特别是对于老用户（有丰富画像特征的用户），Gate Network 能非常精准地捕捉其特有的偏好模式。

### 5. 总结

| 模型         | LHUC / PPNet                                                             |
| :----------- | :----------------------------------------------------------------------- |
| **核心操作** | **Hadamard Product (逐元素相乘)**                                        |
| **输入 A**   | 主网络层的输出 (Item + Context + ...)                                    |
| **输入 B**   | **用户特征专用网络** 的输出 (Gate)                                       |
| **关键激活** | $2 \times \text{Sigmoid}$ (范围 0~2)                                     |
| **作用**     | 实现**网络参数级别**的个性化，让同一个模型对不同用户表现出不同的“关注点” |

这节课展示了如何通过简单的结构微调（加个 Gate 乘一下），将通用的神经网络改造成具备高度个性化能力的 Ranking 模型。这是目前大厂精排模型中标配的“独门暗器”之一。

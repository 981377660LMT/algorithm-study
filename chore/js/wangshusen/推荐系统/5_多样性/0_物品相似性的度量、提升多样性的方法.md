这节课和后面几节课的内容是推荐系统中的多样性。如果多样性做得好，可以显著提升推荐系统的核心业务指标。这节课的内容分两部分：

1. 物品相似性的度量。可以用物品标签或向量表征度量物品的相似性。最好的方法是基于图文内容的向量表征，比如 CLIP 方法。
2. 提升多样性的方法。在推荐的链路上，在粗排和精排的后处理阶段，综合排序模型打分和多样性分数做选择。

---

这节课正式开启了 **重排 (Re-ranking)** 和 **多样性 (Diversity)** 的篇章。在推荐系统的流程中，这通常是最后一道关卡，决定了用户最终看到的页面长什么样。

如果前面的召回和精排是想尽办法把用户最可能点的东西找出来（**Accuracy / Precision**），那么重排就是为了防止这些高分物品太像了（**Similarity / Redundancy**），导致用户产生审美疲劳。

以下是对这节课内容的深度逻辑拆解与总结：

### 1. 核心问题：如何定义“相似”？ (Similarity Measurement)

想要提升多样性，首先得知道哪两个东西是相似的。王树森老师介绍了两种方法：

#### A. 基于属性标签 (Attribute-based)

- **方法**：看两个物品有多少标签是重合的。
  - 一级类目 (L1 Category): 美妆 vs 美妆 (Score=1)
  - 二级类目 (L2 Category): 彩妆 vs 香水 (Score=0)
  - 品牌 (Brand): 香奈儿 vs 香奈儿 (Score=1)
- **优点**：解释性极强。业务规则好控制（比如强制这屏不能出现两个“香奈儿”）。
- **缺点**：太粗糙。两个“口红”可能一个是正红一个是粉红，但在标签上它们是一样相似的，这种细微差别体现不出来。

#### B. 基于向量表征 (Embedding-based) —— _推荐_

- **方法**：计算两个物品向量的余弦相似度 (Cosine Similarity)。
- **坑点**：**能不能直接用双塔/精排学出来的 Item ID Embedding？**
  - **结论**：**不太行，慎用。**
  - **原因**：双塔学的是“协同过滤”关系（谁喜欢买这个），而不是“内容相似”关系。
    - 例 1：啤酒和尿布。双塔可能把它们学得很像（因为常被一起买），但在内容上它们完全不相似。
    - 例 2：冷门物品（长尾）。双塔学不好它们的向量（交互少），导致计算出来的相似度全是噪声。
- **最佳实践**：**基于内容 (Content-based) 的 Embedding**。
  - 图片（CNN/ViT）+ 文本（BERT） $\to$ 拼起来。
  - **进阶大杀器：CLIP (Contrastive Language-Image Pre-training)**。
    - 这也是小红书这种图文平台的优势。直接拿笔记的图片和正文做对比学习（Contrastive Learning），学出来的向量天然融合了视觉和语义信息，且不需要人工打标签。
    - _正样本_：同一篇笔记的 (图, 文)。
    - _负样本_：同一 Batch 内其他笔记的 (图, 文) 组合。

### 2. 提升多样性的位置：后处理 (Post-Processing)

王老师强调了一个容易被忽视的观点：**多样性不仅仅是重排的事，粗排也要做。**

- **常规认知**：
  - Recall $\to$ Pre-Rank $\to$ Rank $\to$ **Re-Rank (在这里做去重/打散)**。
- **进阶认知**：
  - Pre-Rank 选出的 Top-N (如 500 个) 如果全是同质化的东西（比如全是美女跳舞），那么精排再厉害，最后出来的 Top-K 也全是美女跳舞，Re-Rank 即使打散了也没得选。
  - **因此**：在 Pre-Rank 的后处理阶段，就应该引入多样性机制（比如 DPP 或 MMR），确保送给精排的那几百个候选本身就是五花八门的，保留了可能性的种子。

### 3. 重排的目标函数

重排不再是简单的 Point-wise 排序（谁分高谁排第一），而是一个 **List-wise 优化问题**。
我们需要最大化一个包含两个目标的函数：

$$ \text{Maximize } \sum\_{i \in S} \text{Relevance}(u, i) + \lambda \cdot \text{Diversity}(S) $$

- **Relevance (相关性)**：精排打出来的分（CTR/CVR）。
- **Diversity (多样性)**：列表中物品两两之间的不相似程度。
- **Constraint (约束)**：最终展示列表 $S$ 的长度为 $K$。

这就引出了下一节课可能会讲到的具体算法，如 MMR (Maximal Marginal Relevance) 或 DPP (Determinantal Point Process)。

**总结**：这一节课是多样性的地基。记住一点：**用 CLIP 做内容向量来算相似度**，并且**要在粗排阶段就开始考虑多样性**，这两点是工业界提升用户留存的实战经验。

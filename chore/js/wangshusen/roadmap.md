# 🧠 AI 领域深度系统讲解

## 📚 目录

```
第一部分：数学基础 (Foundation)
第二部分：机器学习核心 (Machine Learning)
第三部分：深度学习精要 (Deep Learning)
第四部分：大语言模型 (LLM)
第五部分：强化学习 (Reinforcement Learning)
第六部分：AI Agent 与工程实践
第七部分：前沿方向与研究热点
```

---

# 第一部分：数学基础 🔢

> **为什么重要**：数学是 AI 的语言，不懂数学只能调 API，懂了数学才能创造算法。

## 1.1 线性代数 - AI 的骨架

```
核心概念速览：
┌─────────────────────────────────────────────────────────────┐
│  向量 (Vector)        →  数据的基本表示单位                   │
│  矩阵 (Matrix)        →  线性变换、权重存储                   │
│  张量 (Tensor)        →  多维数据容器 (深度学习核心)           │
│  特征值/特征向量       →  PCA降维、谱聚类的理论基础             │
│  奇异值分解 (SVD)      →  推荐系统、降维、矩阵压缩              │
└─────────────────────────────────────────────────────────────┘
```

### 关键理解

**1. 矩阵乘法的本质 = 线性变换**

```
y = Wx + b

W: 权重矩阵 (变换矩阵)
x: 输入向量
b: 偏置向量
y: 输出向量

神经网络的每一层都在做这个操作！
```

**2. 为什么深度学习用张量？**

```python
# 图像数据: [Batch, Channel, Height, Width]
images.shape = (32, 3, 224, 224)  # 32张224x224的RGB图片

# 文本数据: [Batch, Sequence, Embedding]
tokens.shape = (16, 512, 768)     # 16个样本，每个512个token，768维embedding

# 注意力权重: [Batch, Heads, Seq, Seq]
attention.shape = (16, 12, 512, 512)  # Multi-head attention
```

**3. 特征分解的直觉**

```
A = QΛQ⁻¹

矩阵A = 旋转(Q) × 拉伸(Λ) × 反旋转(Q⁻¹)

应用：
- PCA: 找到数据方差最大的方向
- PageRank: 找到网页重要性的稳定分布
- 图神经网络: 图的谱分析
```

## 1.2 概率论与统计 - AI 的灵魂

```
┌──────────────────────────────────────────────────────────────┐
│                     概率论核心公式                            │
├──────────────────────────────────────────────────────────────┤
│  贝叶斯定理:  P(A|B) = P(B|A)P(A) / P(B)                     │
│                                                              │
│  后验 ∝ 似然 × 先验                                          │
│                                                              │
│  这是机器学习的理论基石！                                      │
└──────────────────────────────────────────────────────────────┘
```

### 关键分布

| 分布          | 公式                  | 应用场景              |
| ------------- | --------------------- | --------------------- |
| **伯努利**    | P(x) = p^x(1-p)^(1-x) | 二分类概率            |
| **高斯/正态** | N(μ, σ²)              | 连续值建模、VAE       |
| **Softmax**   | exp(xᵢ)/Σexp(xⱼ)      | 多分类概率输出        |
| **类别分布**  | Cat(π₁,...,πₖ)        | LLM 下一个 token 预测 |

### 信息论基础

```
信息熵:     H(X) = -Σ P(x)log P(x)        # 不确定性度量
交叉熵:     H(P,Q) = -Σ P(x)log Q(x)      # 分类损失函数！
KL散度:     KL(P||Q) = Σ P(x)log(P(x)/Q(x)) # 分布差异度量

关系: H(P,Q) = H(P) + KL(P||Q)

交叉熵损失 = 真实分布的熵 + 预测与真实的差距
```

## 1.3 微积分与优化 - AI 的引擎

### 梯度下降的本质

```
目标: 最小化损失函数 L(θ)

更新规则: θ_{t+1} = θ_t - η∇L(θ_t)

∇L = [∂L/∂θ₁, ∂L/∂θ₂, ..., ∂L/∂θₙ]  # 梯度向量

直觉: 沿着山坡最陡的方向下山
```

### 优化器演进

```
SGD:        θ = θ - η∇L
            ↓ 加入动量
Momentum:   v = βv + ∇L,  θ = θ - ηv
            ↓ 自适应学习率
AdaGrad:    适应稀疏梯度
RMSprop:    指数移动平均
            ↓ 结合两者
Adam:       m = β₁m + (1-β₁)∇L      # 一阶矩估计(动量)
            v = β₂v + (1-β₂)(∇L)²   # 二阶矩估计(自适应)
            θ = θ - η·m̂/√(v̂+ε)

Adam是目前最常用的优化器！
```

### 链式法则 - 反向传播的数学基础

```
y = f(g(x))

dy/dx = dy/dg · dg/dx

多层网络:
∂L/∂W₁ = ∂L/∂y · ∂y/∂h₂ · ∂h₂/∂h₁ · ∂h₁/∂W₁

这就是反向传播！从输出层逐层传递梯度到输入层
```

---

# 第二部分：机器学习核心 🎓

## 2.1 机器学习范式全景

```
                        机器学习
                           │
        ┌──────────────────┼──────────────────┐
        ↓                  ↓                  ↓
    监督学习            无监督学习           强化学习
   (有标签)            (无标签)          (有反馈/奖励)
        │                  │                  │
   ┌────┴────┐       ┌────┴────┐        ┌────┴────┐
   ↓         ↓       ↓         ↓        ↓         ↓
  分类      回归    聚类      降维    策略学习  值函数学习
```

## 2.2 监督学习深度剖析

### 2.2.1 线性模型家族

```python
# 线性回归
y = w₁x₁ + w₂x₂ + ... + wₙxₙ + b = W^T X + b

# 损失函数: 均方误差
L = (1/n)Σ(yᵢ - ŷᵢ)²

# 正则化防止过拟合
L1正则(Lasso): L + λΣ|wᵢ|      → 稀疏解，特征选择
L2正则(Ridge): L + λΣwᵢ²       → 平滑解，防止权重过大
ElasticNet:    L + λ₁Σ|wᵢ| + λ₂Σwᵢ²  → 结合两者
```

### 2.2.2 逻辑回归 - 分类的基石

```
线性回归: y = W^T X + b         → 输出任意实数
                ↓ 加Sigmoid
逻辑回归: P(y=1) = σ(W^T X + b) = 1/(1+e^(-z))  → 输出[0,1]概率

损失函数: 二元交叉熵
L = -[y·log(p) + (1-y)·log(1-p)]

为什么用交叉熵而不是MSE?
→ 交叉熵梯度更大，收敛更快
→ 概率解释更自然
```

### 2.2.3 决策树与集成学习

```
决策树构建过程:
┌─────────────────────────────────────┐
│ 1. 选择最优分裂特征 (信息增益/基尼系数) │
│ 2. 递归分裂子节点                      │
│ 3. 达到停止条件 (深度/样本数)           │
│ 4. 剪枝防止过拟合                      │
└─────────────────────────────────────┘

信息增益: IG = H(parent) - Σ(|child|/|parent|)H(child)
基尼系数: Gini = 1 - Σpᵢ²
```

**集成学习的三种范式：**

```
1. Bagging (Bootstrap Aggregating)
   ┌─────┐ ┌─────┐ ┌─────┐
   │模型1│ │模型2│ │模型3│  ← 并行训练，随机采样
   └──┬──┘ └──┬──┘ └──┬──┘
      └───────┼───────┘
              ↓
          投票/平均           ← 减少方差

   代表: Random Forest

2. Boosting
   模型1 → 模型2 → 模型3      ← 串行训练，关注错误样本
     ↓       ↓       ↓
   残差1   残差2   残差3      ← 每个模型修正前一个的错误

   代表: XGBoost, LightGBM, CatBoost

3. Stacking
   ┌─────┐ ┌─────┐ ┌─────┐
   │模型1│ │模型2│ │模型3│  ← 基学习器
   └──┬──┘ └──┬──┘ └──┬──┘
      └───────┼───────┘
              ↓
         元学习器           ← 学习如何组合
```

### 2.2.4 支持向量机 (SVM)

```
核心思想: 找到最大间隔超平面

目标函数:
min (1/2)||w||²  subject to  yᵢ(w·xᵢ + b) ≥ 1

核技巧 (Kernel Trick):
┌─────────────────────────────────────────────────┐
│ 原始空间线性不可分 → 映射到高维空间 → 线性可分     │
│                                                 │
│ K(x,y) = φ(x)·φ(y)  不需要显式计算φ！            │
│                                                 │
│ 常用核函数:                                      │
│ - 线性核: K(x,y) = x·y                          │
│ - RBF核: K(x,y) = exp(-γ||x-y||²)              │
│ - 多项式核: K(x,y) = (x·y + c)^d               │
└─────────────────────────────────────────────────┘
```

## 2.3 无监督学习

### 2.3.1 聚类算法对比

| 算法         | 原理                  | 优点                     | 缺点                   | 适用场景 |
| ------------ | --------------------- | ------------------------ | ---------------------- | -------- |
| **K-Means**  | 最小化类内距离        | 简单快速                 | 需指定 K，对初始值敏感 | 球形簇   |
| **DBSCAN**   | 密度可达              | 不需指定 K，发现任意形状 | 对密度变化敏感         | 噪声数据 |
| **层次聚类** | 自底向上/自顶向下合并 | 可视化好                 | O(n²)复杂度            | 小数据集 |
| **GMM**      | 高斯混合概率模型      | 软聚类，概率输出         | 需指定 K               | 重叠簇   |

### 2.3.2 降维算法

```
PCA (主成分分析):
1. 数据中心化: X' = X - mean(X)
2. 计算协方差矩阵: C = X'^T X' / n
3. 特征分解: C = VΛV^T
4. 选取前k个特征向量
5. 投影: X_reduced = X' × V_k

保留方差比例 = Σλ_selected / Σλ_all

t-SNE / UMAP:
- 非线性降维
- 保持局部结构
- 主要用于可视化
- 不能用于新数据投影（需要重新训练）
```

## 2.4 模型评估与选择

### 2.4.1 偏差-方差权衡

```
总误差 = 偏差² + 方差 + 不可约误差

             高偏差              低偏差
           (欠拟合)             (过拟合)
              │                    │
高方差        │      最优区域      │
              │        ↓          │
低方差     ───┴────────●──────────┴───→ 模型复杂度

欠拟合: 训练误差高，测试误差高 → 增加模型复杂度
过拟合: 训练误差低，测试误差高 → 正则化/减少复杂度/更多数据
```

### 2.4.2 交叉验证

```
K-Fold Cross Validation (k=5为例):

Fold 1: [Test][Train][Train][Train][Train]
Fold 2: [Train][Test][Train][Train][Train]
Fold 3: [Train][Train][Test][Train][Train]
Fold 4: [Train][Train][Train][Test][Train]
Fold 5: [Train][Train][Train][Train][Test]

最终得分 = mean(5次得分)
标准差   = std(5次得分)  ← 衡量稳定性
```

### 2.4.3 评估指标详解

**分类指标：**

```
              预测正例    预测负例
实际正例        TP          FN
实际负例        FP          TN

准确率 Accuracy = (TP+TN)/(TP+TN+FP+FN)  ← 类别不平衡时无意义
精确率 Precision = TP/(TP+FP)            ← 预测为正的有多少真正
召回率 Recall = TP/(TP+FN)               ← 真正的正例找回多少
F1 = 2×P×R/(P+R)                         ← 精确率和召回率的调和平均

AUC-ROC:
- ROC曲线: TPR vs FPR 在不同阈值下的表现
- AUC: ROC曲线下面积，越大越好
- AUC=0.5 等于随机猜测
- AUC=1.0 完美分类器
```

**回归指标：**

```
MSE  = (1/n)Σ(y-ŷ)²      ← 对大误差敏感
RMSE = √MSE              ← 与y同量纲
MAE  = (1/n)Σ|y-ŷ|       ← 对离群点鲁棒
R²   = 1 - SS_res/SS_tot ← 解释方差比例，1最好，可为负
```

---

# 第三部分：深度学习精要 🧠

## 3.1 神经网络基础

### 3.1.1 神经元与激活函数

```
        x₁ ──w₁──┐
        x₂ ──w₂──┼──→ Σ(wᵢxᵢ) + b ──→ σ(·) ──→ y
        x₃ ──w₃──┘        ↑
                        线性组合    激活函数    输出

激活函数的作用: 引入非线性！
没有激活函数: 多层网络 = 单层线性变换 (因为线性的组合还是线性)
```

**激活函数对比：**

| 函数           | 公式                      | 优点                 | 缺点               | 使用场景    |
| -------------- | ------------------------- | -------------------- | ------------------ | ----------- |
| **Sigmoid**    | 1/(1+e^(-x))              | 输出(0,1)            | 梯度消失，非零中心 | 二分类输出  |
| **Tanh**       | (e^x-e^(-x))/(e^x+e^(-x)) | 零中心               | 梯度消失           | RNN         |
| **ReLU**       | max(0,x)                  | 计算快，缓解梯度消失 | 死亡 ReLU          | 隐藏层默认  |
| **Leaky ReLU** | max(αx,x)                 | 解决死亡 ReLU        | 需要调 α           | 替代 ReLU   |
| **GELU**       | x·Φ(x)                    | 平滑，性能好         | 计算稍慢           | Transformer |
| **SiLU/Swish** | x·σ(x)                    | 平滑，性能好         | 计算稍慢           | 现代架构    |
| **Softmax**    | e^xᵢ/Σe^xⱼ                | 输出概率分布         | 需要所有类别       | 多分类输出  |

### 3.1.2 反向传播算法

```
前向传播 (Forward Pass):
输入x → h₁=σ(W₁x+b₁) → h₂=σ(W₂h₁+b₂) → ŷ=σ(W₃h₂+b₃) → L(ŷ,y)

反向传播 (Backward Pass):
∂L/∂W₃ ← ∂L/∂ŷ × ∂ŷ/∂W₃
   ↑
∂L/∂W₂ ← ∂L/∂h₂ × ∂h₂/∂W₂
   ↑
∂L/∂W₁ ← ∂L/∂h₁ × ∂h₁/∂W₁

计算图 + 链式法则 = 自动微分
PyTorch的autograd就是干这个的！
```

### 3.1.3 训练技巧

```
1. 权重初始化
   - Xavier: W ~ N(0, 2/(n_in+n_out))  ← 用于Tanh/Sigmoid
   - He:     W ~ N(0, 2/n_in)          ← 用于ReLU

2. 批归一化 (Batch Normalization)
   x̂ = (x - μ_batch) / √(σ²_batch + ε)
   y = γx̂ + β  ← 可学习参数

   作用: 加速训练，允许更大学习率，有正则化效果

3. Dropout
   训练时: 以概率p随机置零
   测试时: 所有神经元激活，权重乘以(1-p)

   作用: 防止过拟合，集成学习效果

4. 学习率调度
   - Step Decay: 每n个epoch降低
   - Cosine Annealing: 余弦曲线下降
   - Warmup: 开始时逐渐增加
   - OneCycleLR: 先升后降
```

## 3.2 卷积神经网络 (CNN)

### 3.2.1 核心操作

```
卷积操作:
┌───┬───┬───┬───┐     ┌───┬───┐     ┌───┬───┬───┐
│ 1 │ 2 │ 3 │ 4 │     │ 1 │ 0 │     │19 │25 │...│
├───┼───┼───┼───┤  *  ├───┼───┤  =  ├───┼───┼───┤
│ 5 │ 6 │ 7 │ 8 │     │ 0 │ 1 │     │...│...│...│
├───┼───┼───┼───┤     └───┴───┘     └───┴───┴───┘
│ 9 │10 │11 │12 │      Kernel        Feature Map
└───┴───┴───┴───┘
     Input

池化操作 (Max Pooling 2x2):
┌───┬───┬───┬───┐     ┌───┬───┐
│ 1 │ 2 │ 5 │ 6 │     │ 4 │ 8 │
├───┼───┼───┼───┤  →  ├───┼───┤
│ 3 │ 4 │ 7 │ 8 │     │12 │16 │
├───┼───┼───┼───┤     └───┴───┘
│ 9 │10 │13 │14 │
├───┼───┼───┼───┤
│11 │12 │15 │16 │
└───┴───┴───┴───┘

作用: 减少参数，增大感受野，平移不变性
```

### 3.2.2 经典架构演进

```
LeNet (1998) → AlexNet (2012) → VGG (2014) → GoogLeNet (2014)
                    ↓
              ResNet (2015) ← 残差连接革命！
                    ↓
          DenseNet, EfficientNet, ConvNeXt...

ResNet残差连接:
┌─────────────────────────────┐
│     x ──→ F(x) ──→ F(x)+x   │  ← 恒等映射
│           ↑                 │
│           └───────┘         │
└─────────────────────────────┘

为什么残差连接有效?
1. 梯度高速公路，缓解梯度消失
2. 学习残差比学习完整映射更容易
3. 允许网络自适应深度
```

## 3.3 循环神经网络 (RNN)

### 3.3.1 基础 RNN

```
         h₀ ──→ h₁ ──→ h₂ ──→ h₃ ──→ h₄
          ↑     ↑     ↑     ↑     ↑
         x₀    x₁    x₂    x₃    x₄

hₜ = tanh(Wₕₕ·hₜ₋₁ + Wₓₕ·xₜ + b)

问题:
- 梯度消失/爆炸 (长序列)
- 难以捕捉长期依赖
```

### 3.3.2 LSTM - 长短期记忆

```
┌────────────────────────────────────────────┐
│                                            │
│  遗忘门: fₜ = σ(Wf·[hₜ₋₁, xₜ] + bf)       │  ← 决定丢弃什么
│  输入门: iₜ = σ(Wi·[hₜ₋₁, xₜ] + bi)       │  ← 决定更新什么
│  候选值: C̃ₜ = tanh(Wc·[hₜ₋₁, xₜ] + bc)    │  ← 新信息
│  细胞状态: Cₜ = fₜ⊙Cₜ₋₁ + iₜ⊙C̃ₜ           │  ← 记忆更新
│  输出门: oₜ = σ(Wo·[hₜ₋₁, xₜ] + bo)       │  ← 决定输出什么
│  隐状态: hₜ = oₜ⊙tanh(Cₜ)                  │  ← 输出
│                                            │
└────────────────────────────────────────────┘

门控机制允许梯度无阻碍流动 → 解决长期依赖问题
```

## 3.4 Transformer - 革命性架构 ⭐

### 3.4.1 自注意力机制

```
核心公式:
Attention(Q, K, V) = softmax(QK^T / √dₖ)V

┌─────────────────────────────────────────────────────────────┐
│  Q (Query): 查询向量，"我在找什么"                            │
│  K (Key):   键向量，"我有什么特征"                            │
│  V (Value): 值向量，"我包含什么信息"                          │
│                                                             │
│  QK^T: 计算每个位置与其他位置的相关性得分                      │
│  √dₖ: 缩放因子，防止softmax梯度消失                          │
│  softmax: 归一化为注意力权重                                 │
│  × V: 加权求和得到输出                                       │
└─────────────────────────────────────────────────────────────┘

为什么自注意力革命性?
1. O(1)路径长度 vs RNN的O(n) → 捕捉长距离依赖
2. 可并行计算 vs RNN的串行 → 训练更快
3. 可解释性 → 可视化注意力权重
```

### 3.4.2 Multi-Head Attention

```
MultiHead(Q, K, V) = Concat(head₁, ..., headₕ)Wᴼ

where headᵢ = Attention(QWᵢQ, KWᵢK, VWᵢV)

直觉:
- 单头注意力只能关注一种模式
- 多头可以同时关注: 语法结构、语义相似、位置关系等
- 类似CNN的多通道/多滤波器
```

### 3.4.3 完整 Transformer 结构

```
                    ┌─────────────────┐
                    │   Output Embed  │
                    └────────┬────────┘
                             │
              ┌──────────────┼──────────────┐
              │              │              │
         ┌────┴────┐   ┌────┴────┐   ┌────┴────┐
         │ Linear  │   │ Linear  │   │ Linear  │
         │  + Soft │   │         │   │         │
         │   max   │   │         │   │         │
         └────┬────┘   └────┬────┘   └────┬────┘
              │              │              │
              └──────────────┼──────────────┘
                             │
                    ┌────────┴────────┐
                    │ Add & LayerNorm │←──────┐
                    └────────┬────────┘       │
                             │                │
                    ┌────────┴────────┐       │
                    │       FFN       │       │
                    └────────┬────────┘       │
                             │                │
                    ┌────────┴────────┐       │
                    │ Add & LayerNorm │←──┐   │
                    └────────┬────────┘   │   │
                             │            │   │
                    ┌────────┴────────┐   │   │
                    │  Cross-Attention│   │   │
                    │    (Q from Dec, │   │   │
                    │   K,V from Enc) │   │   │
                    └────────┬────────┘   │   │
                             │            │   │
                    ┌────────┴────────┐   │   │
                    │ Add & LayerNorm │←──┼───┤
                    └────────┬────────┘   │   │
                             │            │   │
                    ┌────────┴────────┐   │   │
                    │ Masked Self-Attn│───┘   │
                    └────────┬────────┘       │
                             │                │
                    ┌────────┴────────┐       │
                    │ Input + Pos Enc │───────┘
                    └─────────────────┘
                         DECODER

Encoder: Self-Attention → FFN (可并行看全文)
Decoder: Masked Self-Attn → Cross-Attn → FFN (逐步生成)
```

### 3.4.4 位置编码

```
为什么需要?
→ 自注意力对位置不敏感，需要显式注入位置信息

原始Transformer (正弦位置编码):
PE(pos, 2i)   = sin(pos / 10000^(2i/d))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d))

现代方法:
- 可学习位置编码 (BERT)
- 相对位置编码 (Transformer-XL)
- 旋转位置编码 RoPE (LLaMA) ← 目前主流

RoPE优势:
- 外推性好，可处理比训练更长的序列
- 相对位置信息自然融入注意力计算
```

## 3.5 其他重要架构

### 3.5.1 生成对抗网络 (GAN)

```
┌─────────────────────────────────────────────────┐
│                                                 │
│   噪声z ──→ [生成器G] ──→ 假样本 ──┐             │
│                                   ↓             │
│                              [判别器D] ──→ 真/假 │
│                                   ↑             │
│   真实数据 ────────────────────────┘             │
│                                                 │
│   G的目标: 骗过D，max_G E[log D(G(z))]          │
│   D的目标: 区分真假，max_D E[log D(x)] + E[log(1-D(G(z)))]│
│                                                 │
│   对抗训练 = 零和博弈 → 纳什均衡                   │
│                                                 │
└─────────────────────────────────────────────────┘

变体: DCGAN, StyleGAN, CycleGAN, BigGAN...
```

### 3.5.2 变分自编码器 (VAE)

```
编码器                   解码器
x ──→ [Encoder] ──→ μ, σ ──→ z = μ + σ⊙ε ──→ [Decoder] ──→ x̂
                         ↑
                    重参数化技巧
                   (ε ~ N(0,1))

损失函数:
L = 重构损失 + KL散度正则
  = E[log p(x|z)] - KL(q(z|x) || p(z))

与GAN对比:
- VAE: 显式概率模型，训练稳定，但输出模糊
- GAN: 隐式模型，输出清晰，但训练不稳定
```

### 3.5.3 扩散模型 (Diffusion Models)

```
前向过程 (加噪):
x₀ → x₁ → x₂ → ... → xₜ → ... → xₜ (纯噪声)
        逐步添加高斯噪声

反向过程 (去噪):
xₜ → xₜ₋₁ → ... → x₁ → x₀ (生成图像)
        神经网络预测噪声

训练目标: 学习预测每一步添加的噪声
L = E[||ε - ε_θ(xₜ, t)||²]

代表: DDPM, Stable Diffusion, DALL-E 2, Midjourney

为什么强大?
- 训练稳定 (不像GAN)
- 生成质量高
- 可控性强 (条件生成)
```

---

# 第四部分：大语言模型 (LLM) 🚀

## 4.1 LLM 发展历程

```
2017: Transformer诞生 (Attention Is All You Need)
2018: GPT-1 (解码器架构) + BERT (编码器架构)
2019: GPT-2 (1.5B参数，Zero-shot能力涌现)
2020: GPT-3 (175B参数，Few-shot学习)
2022: ChatGPT (RLHF对齐) + InstructGPT
2023: GPT-4 (多模态) + LLaMA (开源生态)
2024: Claude 3, Gemini, Qwen2, DeepSeek
2025: GPT-4o, Claude Sonnet, 推理模型...

关键转折点:
- GPT-3: 规模定律 (Scaling Law) 验证
- ChatGPT: RLHF使模型对齐人类偏好
- LLaMA: 开源推动整个生态
```

## 4.2 LLM 核心架构

### 4.2.1 GPT 架构 (Decoder-Only)

```
GPT = Stack of Transformer Decoder Blocks

┌──────────────────────────────────────┐
│  Token Embedding + Position Encoding │
└────────────────┬─────────────────────┘
                 ↓
┌──────────────────────────────────────┐
│      Masked Self-Attention           │ ← 因果mask，只看过去
│              ↓                       │
│         Add & Norm                   │
│              ↓                       │
│            FFN                       │
│              ↓                       │
│         Add & Norm                   │
└────────────────┬─────────────────────┘
                 ↓  (× N layers)
┌──────────────────────────────────────┐
│           LM Head                    │
│    (Linear → Vocab Size Logits)      │
└────────────────┬─────────────────────┘
                 ↓
         Next Token Prediction

P(xₜ|x₁,...,xₜ₋₁) = softmax(LM_Head(h_t))
```

### 4.2.2 现代 LLM 改进

```
┌────────────────────────────────────────────────────────────────┐
│ 改进点              │  技术                    │ 效果           │
├────────────────────────────────────────────────────────────────┤
│ 位置编码            │ RoPE (旋转位置编码)       │ 更好的外推性    │
│ 归一化              │ Pre-Norm + RMSNorm       │ 训练更稳定     │
│ 激活函数            │ SwiGLU                   │ 性能提升       │
│ 注意力              │ GQA (分组查询注意力)      │ 推理加速       │
│ 上下文长度          │ ALiBi, NTK-aware Scaling │ 支持更长上下文  │
│ 稀疏注意力          │ Sliding Window, Mixture  │ 效率提升       │
└────────────────────────────────────────────────────────────────┘
```

### 4.2.3 GQA (Grouped-Query Attention)

```
MHA (Multi-Head Attention):
Q: [h heads] K: [h heads] V: [h heads]
每个head有独立的K,V → KV Cache大

MQA (Multi-Query Attention):
Q: [h heads] K: [1 head] V: [1 head]
所有Q head共享1个K,V → 极大减少KV Cache

GQA (Grouped-Query Attention):
Q: [h heads] K: [g groups] V: [g groups]
每g个Q head共享1个K,V → 折中方案

例: 32个Q heads, 8个KV heads
→ 每4个Q head共享1个K,V
→ KV Cache减少4x，性能损失极小
```

## 4.3 训练范式

### 4.3.1 预训练 (Pre-training)

```
目标: 在海量文本上学习语言模式

数据规模:
- GPT-3: 300B tokens
- LLaMA: 1.4T tokens
- LLaMA 2: 2T tokens
- Qwen2: 7T+ tokens

训练目标: Next Token Prediction (自回归)
L = -Σ log P(xₜ|x₁,...,xₜ₋₁)

预训练学到了什么?
- 语法和语义
- 世界知识
- 推理模式
- 代码理解
```

### 4.3.2 Scaling Laws (规模定律)

```
Kaplan定律 (OpenAI):
L(N, D) ≈ (Nₒ/N)^αₙ + (Dₒ/D)^αᴅ

L: 损失
N: 模型参数量
D: 数据量

核心发现:
1. 增加模型规模 → 损失下降 (幂律关系)
2. 增加数据规模 → 损失下降
3. 增加计算量 → 损失下降
4. 三者需要平衡，存在最优配比

Chinchilla定律 (DeepMind):
最优配置: 数据tokens ≈ 20 × 参数量
→ 很多模型其实数据不够！

Emergent Abilities (涌现能力):
模型超过某个规模后，突然获得新能力
- In-context learning
- Chain-of-thought
- 复杂推理
```

### 4.3.3 SFT (Supervised Fine-Tuning)

```
目标: 将预训练模型调整为对话/指令跟随模式

数据格式:
{
  "instruction": "解释什么是机器学习",
  "input": "",
  "output": "机器学习是人工智能的一个分支..."
}

或对话格式:
[
  {"role": "user", "content": "什么是AI?"},
  {"role": "assistant", "content": "AI是..."}
]

常用方法:
1. 全参数微调 (Full Fine-tuning)
2. LoRA (Low-Rank Adaptation) ← 主流
3. QLoRA (量化 + LoRA)
4. Prefix Tuning
5. Adapter
```

### 4.3.4 LoRA 详解

```
原始权重: W₀ ∈ R^(d×k)

LoRA: W = W₀ + ΔW = W₀ + BA
其中: B ∈ R^(d×r), A ∈ R^(r×k), r << min(d,k)

例如: d=4096, k=4096, r=8
原始参数: 16M
LoRA参数: 65K (减少250倍!)

前向传播:
h = W₀x + BAx = W₀x + B(Ax)
              ↑
         先降维再升维

为什么有效?
- 预训练权重的更新矩阵通常是低秩的
- 只学习关键的"差异"，而非全部
```

### 4.3.5 RLHF (人类反馈强化学习)

```
三阶段流程:

Stage 1: SFT
预训练模型 → 监督微调 → SFT模型

Stage 2: 训练奖励模型 (RM)
收集人类偏好数据: (prompt, response_好, response_差)
训练RM: r(x, y) = 打分

Stage 3: PPO优化
max E[r(x,y)] - β·KL(π||π_ref)
    ↑               ↑
  最大化奖励      不要偏离太远

直觉:
让模型输出人类喜欢的回答
同时不要"崩坏"(过度优化奖励)
```

### 4.3.6 DPO (Direct Preference Optimization)

```
RLHF的简化版本，无需训练单独的奖励模型

损失函数:
L_DPO = -E[log σ(β(log π(y_w|x)/π_ref(y_w|x)
                  - log π(y_l|x)/π_ref(y_l|x)))]

y_w: 人类偏好的回答 (winner)
y_l: 人类不偏好的回答 (loser)

直接从偏好数据学习，更简单高效
```

## 4.4 推理优化

### 4.4.1 KV Cache

```
问题: 自回归生成时，每个token都要计算之前所有token的K,V

解决: 缓存已计算的K,V

生成第t个token时:
- 不缓存: 计算K₁,V₁,...,Kₜ₋₁,Vₜ₋₁ → O(t²)
- 有缓存: 只计算Kₜ,Vₜ，复用之前的 → O(t)

内存消耗:
KV Cache = 2 × num_layers × seq_len × hidden_size × batch × precision

例: LLaMA 70B, seq=4096, batch=1, fp16
≈ 2 × 80 × 4096 × 8192 × 2 bytes ≈ 10GB
```

### 4.4.2 量化 (Quantization)

```
目标: 减少模型存储和计算开销

FP16 → INT8 → INT4
16位   8位    4位

类型:
- PTQ (Post-Training Quantization): 训练后量化
- QAT (Quantization-Aware Training): 量化感知训练

常用方法:
- GPTQ: 逐层量化，考虑激活分布
- AWQ: 保护重要权重
- GGML/GGUF: llama.cpp格式
- bitsandbytes: HuggingFace集成

量化对性能的影响:
FP16 → INT8: 基本无损
INT8 → INT4: 略有下降，但可接受
```

### 4.4.3 推理加速技术

```
1. Flash Attention
   - 分块计算，减少内存访问
   - 不存储完整注意力矩阵
   - 速度提升2-4x

2. Speculative Decoding (投机采样)
   ┌─────────────────────────────────────────────┐
   │ 小模型 (Draft): 快速生成k个候选token        │
   │ 大模型 (Target): 并行验证k个token           │
   │ 接受: 与大模型输出一致的token               │
   │ 拒绝: 从拒绝位置用大模型重新采样            │
   └─────────────────────────────────────────────┘

3. Continuous Batching
   动态管理batch，不等最长序列完成

4. PagedAttention (vLLM)
   KV Cache分页管理，减少内存碎片

5. Tensor Parallelism / Pipeline Parallelism
   多GPU推理
```

## 4.5 Tokenization

```
目的: 文本 → 数字序列

演进:
Word-level → Char-level → Subword (BPE/WordPiece/SentencePiece)

BPE (Byte-Pair Encoding):
1. 初始: 每个字符是一个token
2. 统计最频繁的相邻token对
3. 合并为新token
4. 重复直到达到词表大小

例:
"lower" + "lowest" → ['low', 'er', 'low', 'est']
                    → ['low'] 被识别为common subword

你的笔记里提到:
"一个token通常对应约4个字符，约等于¾个单词"
→ 100 tokens ≈ 75 words (英文)
→ 中文约 1 token = 1-2 字
```

## 4.6 Prompt Engineering

```
Zero-shot:
"翻译成英文: 你好世界"

Few-shot:
"翻译例子:
早上好 → Good morning
晚安 → Good night
你好世界 → "

Chain-of-Thought (CoT):
"让我一步步思考这个问题...
第一步: ...
第二步: ...
因此答案是: ..."

Self-Consistency:
生成多个推理路径，投票选最常见答案

Tree of Thoughts (ToT):
探索多个推理分支，回溯搜索

ReAct:
Reasoning + Acting，结合思考和工具调用
```

---

# 第五部分：强化学习 (RL) 🎮

## 5.1 核心概念

```
┌─────────────────────────────────────────────────────────────┐
│                    强化学习框架                              │
│                                                             │
│                    ┌──────────┐                             │
│         action aₜ  │          │  state sₜ₊₁                │
│        ─────────→  │  环境 E   │ ─────────→                 │
│                    │          │                             │
│                    └────┬─────┘                             │
│                         │ reward rₜ                         │
│                         ↓                                   │
│                    ┌──────────┐                             │
│         state sₜ   │          │                             │
│        ←─────────  │ 智能体 A  │                             │
│                    │          │                             │
│                    └──────────┘                             │
│                                                             │
│  目标: 最大化累积奖励 G = Σγᵗrₜ                              │
│  γ: 折扣因子 (0-1)，表示对未来奖励的重视程度                   │
└─────────────────────────────────────────────────────────────┘
```

## 5.2 马尔可夫决策过程 (MDP)

```
MDP = (S, A, P, R, γ)

S: 状态空间
A: 动作空间
P: 转移概率 P(s'|s,a)
R: 奖励函数 R(s,a,s')
γ: 折扣因子

马尔可夫性质:
P(sₜ₊₁|sₜ,aₜ,sₜ₋₁,...,s₀) = P(sₜ₊₁|sₜ,aₜ)
未来只依赖当前，与历史无关

策略 π(a|s): 给定状态选择动作的概率分布
```

## 5.3 价值函数

```
状态价值函数 V^π(s):
"从状态s出发，按策略π行动，期望获得的累积奖励"
V^π(s) = E_π[Σγᵗrₜ | s₀=s]

动作价值函数 Q^π(s,a):
"从状态s采取动作a，然后按策略π行动，期望获得的累积奖励"
Q^π(s,a) = E_π[Σγᵗrₜ | s₀=s, a₀=a]

关系:
V^π(s) = Σ_a π(a|s)Q^π(s,a)
Q^π(s,a) = R(s,a) + γΣ_s' P(s'|s,a)V^π(s')
```

## 5.4 贝尔曼方程

```
贝尔曼期望方程:
V^π(s) = Σ_a π(a|s)[R(s,a) + γΣ_s' P(s'|s,a)V^π(s')]

贝尔曼最优方程:
V*(s) = max_a [R(s,a) + γΣ_s' P(s'|s,a)V*(s')]
Q*(s,a) = R(s,a) + γΣ_s' P(s'|s,a) max_a' Q*(s',a')

最优策略:
π*(s) = argmax_a Q*(s,a)
```

## 5.5 经典算法

### 5.5.1 动态规划 (已知模型)

```
策略迭代 (Policy Iteration):
1. 策略评估: 计算当前策略的V^π
2. 策略改进: π'(s) = argmax_a Q^π(s,a)
3. 重复直到收敛

值迭代 (Value Iteration):
1. 直接迭代最优值函数:
   V(s) ← max_a [R(s,a) + γΣ_s' P(s'|s,a)V(s')]
2. 从V*提取最优策略

限制: 需要完整知道P和R，实际中很少满足
```

### 5.5.2 蒙特卡洛方法

```
思想: 通过采样估计价值

流程:
1. 生成完整episode: s₀,a₀,r₁,s₁,a₁,r₂,...,sₜ
2. 计算每个状态的回报: G = Σγᵗrₜ
3. 更新: V(s) ← average(所有访问s的回报)

优点: 无需模型，可处理大状态空间
缺点: 需要完整episode，方差大
```

### 5.5.3 时序差分学习 (TD Learning)

```
TD(0):
V(sₜ) ← V(sₜ) + α[rₜ₊₁ + γV(sₜ₊₁) - V(sₜ)]
                 └────────────────────────┘
                        TD误差 δ

Q-Learning (Off-policy):
Q(s,a) ← Q(s,a) + α[r + γmax_a' Q(s',a') - Q(s,a)]

SARSA (On-policy):
Q(s,a) ← Q(s,a) + α[r + γQ(s',a') - Q(s,a)]

区别:
- Q-Learning: 用max更新，学习最优策略
- SARSA: 用实际动作更新，学习当前策略
```

## 5.6 深度强化学习

### 5.6.1 DQN (Deep Q-Network)

```
核心改进:
1. 用神经网络逼近Q函数: Q(s,a;θ)
2. 经验回放 (Experience Replay): 打破样本相关性
3. 目标网络 (Target Network): 稳定训练

损失函数:
L = E[(r + γmax_a' Q(s',a';θ⁻) - Q(s,a;θ))²]
                    ↑
              目标网络参数 (定期更新)

改进版本:
- Double DQN: 解耦动作选择和评估
- Dueling DQN: 分离V和A
- Prioritized Replay: 优先回放重要样本
- Rainbow: 集合多种改进
```

### 5.6.2 策略梯度方法

```
思想: 直接优化策略参数

目标:
J(θ) = E_π[Σγᵗrₜ]

策略梯度定理:
∇J(θ) = E_π[∇log π(a|s;θ) · Q^π(s,a)]

REINFORCE算法:
∇J(θ) ≈ Σₜ ∇log π(aₜ|sₜ;θ) · Gₜ

问题: 方差大

改进: 引入Baseline
∇J(θ) ≈ Σₜ ∇log π(aₜ|sₜ;θ) · (Gₜ - b(sₜ))
                               ↑
                          通常用V(s)
```

### 5.6.3 Actor-Critic

```
结合策略梯度和价值函数

Actor: 策略网络 π(a|s;θ)
Critic: 价值网络 V(s;w) 或 Q(s,a;w)

A2C (Advantage Actor-Critic):
Advantage: A(s,a) = Q(s,a) - V(s) ≈ r + γV(s') - V(s)
Actor更新: θ ← θ + α∇log π(a|s;θ) · A(s,a)
Critic更新: w ← w - β∇(A(s,a))²

A3C: 异步并行版本，多个worker同时采样
```

### 5.6.4 PPO (Proximal Policy Optimization)

```
目标: 稳定的策略更新

核心思想: 限制策略更新幅度

目标函数:
L^CLIP(θ) = E[min(rₜ(θ)Aₜ, clip(rₜ(θ), 1-ε, 1+ε)Aₜ)]

rₜ(θ) = π(aₜ|sₜ;θ) / π(aₜ|sₜ;θ_old)  # 重要性采样比率

clip机制:
- rₜ > 1+ε 且 A > 0: 被裁剪 (防止过度增加概率)
- rₜ < 1-ε 且 A < 0: 被裁剪 (防止过度减少概率)

为什么PPO流行?
1. 实现简单
2. 超参数鲁棒
3. 效果好
4. RLHF的标准选择
```

### 5.6.5 SAC (Soft Actor-Critic)

```
最大熵强化学习:
目标不仅是最大化奖励，还要最大化策略熵

J(π) = Σ E[r(sₜ,aₜ) + αH(π(·|sₜ))]
                      ↑
                    熵正则化

好处:
1. 鼓励探索
2. 避免过早收敛到局部最优
3. 更鲁棒

SAC是连续动作空间的SOTA算法之一
```

## 5.7 探索与利用

```
困境:
- 利用 (Exploitation): 选择当前最优动作
- 探索 (Exploration): 尝试新动作以发现更好策略

方法:
1. ε-greedy: 以ε概率随机探索
2. Boltzmann (Softmax): P(a) ∝ exp(Q(s,a)/τ)
3. UCB: 选择 argmax [Q(s,a) + c√(ln t / N(s,a))]
4. 熵正则化: 内在鼓励多样性
5. 内在奖励: 好奇心驱动探索 (ICM)
```

## 5.8 RL 在 LLM 中的应用

```
RLHF流程中的RL:
┌─────────────────────────────────────────────────┐
│ State:   prompt + 已生成的tokens                │
│ Action:  下一个token                            │
│ Reward:  奖励模型打分 (仅在生成结束时)           │
│ Policy:  LLM本身                               │
│                                                 │
│ 算法: PPO                                       │
│ 目标: max E[R(prompt, response)]               │
│       - β·KL(π || π_ref)                       │
│                ↑                               │
│           防止偏离太远                          │
└─────────────────────────────────────────────────┘
```

---

# 第六部分：AI Agent 与工程实践 🤖

基于你收集的资料(Building effective agents, Context Engineering 等)，这是当前最热门的领域。

## 6.1 Agent 基础架构

```
┌────────────────────────────────────────────────────────────┐
│                      AI Agent 架构                         │
│                                                            │
│  用户输入 ──→ ┌─────────────────────────────────────────┐  │
│              │           Agent Core                     │  │
│              │  ┌───────────────────────────────────┐   │  │
│              │  │         LLM (大脑)                 │   │  │
│              │  │    - 理解意图                      │   │  │
│              │  │    - 规划步骤                      │   │  │
│              │  │    - 生成响应                      │   │  │
│              │  └───────────────────────────────────┘   │  │
│              │                  ↕                       │  │
│              │  ┌───────────────────────────────────┐   │  │
│              │  │       Memory (记忆系统)            │   │  │
│              │  │    - 短期: 对话历史                │   │  │
│              │  │    - 长期: 向量数据库              │   │  │
│              │  └───────────────────────────────────┘   │  │
│              │                  ↕                       │  │
│              │  ┌───────────────────────────────────┐   │  │
│              │  │       Tools (工具调用)             │   │  │
│              │  │    - 代码执行                      │   │  │
│              │  │    - 网页搜索                      │   │  │
│              │  │    - API调用                       │   │  │
│              │  │    - 文件操作                      │   │  │
│              │  └───────────────────────────────────┘   │  │
│              └─────────────────────────────────────────┘  │
│                             ↓                              │
│  响应输出 ←─────────────────┘                              │
└────────────────────────────────────────────────────────────┘
```

## 6.2 Agentic 设计模式

```
1. ReAct (Reasoning + Acting)
   ┌─────────────────────────────────────────┐
   │ Thought: 我需要搜索这个问题的答案        │
   │ Action: search("什么是强化学习")         │
   │ Observation: 强化学习是...              │
   │ Thought: 我现在可以回答了               │
   │ Answer: 强化学习是一种机器学习范式...    │
   └─────────────────────────────────────────┘

2. Plan-and-Execute
   ┌─────────────────────────────────────────┐
   │ 规划阶段:                               │
   │   1. 分析需求                           │
   │   2. 搜索资料                           │
   │   3. 整合信息                           │
   │   4. 生成报告                           │
   │                                         │
   │ 执行阶段:                               │
   │   依次执行每个步骤，根据结果调整         │
   └─────────────────────────────────────────┘

3. Reflection / Self-Critique
   生成 → 反思 → 改进 → 再反思 → 最终输出

4. Multi-Agent
   多个专业Agent协作:
   - Researcher Agent
   - Coder Agent
   - Reviewer Agent
   - Coordinator Agent
```

## 6.3 Context Engineering (上下文工程)

```
你收集的Anthropic文章 "Effective context engineering for AI agents" 的核心:

Context = 给LLM的所有输入信息

┌────────────────────────────────────────────────────────────┐
│                    Context组成                             │
│                                                            │
│  System Prompt (系统提示)                                   │
│  ├── 角色定义                                              │
│  ├── 能力边界                                              │
│  ├── 行为准则                                              │
│  └── 输出格式                                              │
│                                                            │
│  Retrieved Context (检索上下文)                             │
│  ├── RAG结果                                               │
│  ├── 工具输出                                              │
│  └── 相关文档                                              │
│                                                            │
│  Conversation History (对话历史)                            │
│  ├── 用户消息                                              │
│  ├── AI回复                                                │
│  └── 工具调用记录                                          │
│                                                            │
│  Current Input (当前输入)                                   │
│  └── 用户最新请求                                          │
└────────────────────────────────────────────────────────────┘

Context管理策略:
1. 压缩: 总结长对话
2. 检索: 只获取相关信息
3. 优先级: 最重要的信息放在最前/最后
4. 结构化: 清晰的格式便于模型理解
```

## 6.4 RAG (检索增强生成)

```
┌──────────────────────────────────────────────────────────┐
│                     RAG Pipeline                         │
│                                                          │
│  离线索引阶段:                                            │
│  文档 → 分块 → Embedding → 向量数据库                     │
│                                                          │
│  在线检索阶段:                                            │
│  Query → Embedding → 相似度搜索 → Top-K文档               │
│                          ↓                               │
│                    拼接Context                           │
│                          ↓                               │
│                   Prompt + Context → LLM → 回答          │
└──────────────────────────────────────────────────────────┘

关键组件:
1. Chunking策略: 固定长度 / 语义分割 / 递归分割
2. Embedding模型: OpenAI / BGE / Jina
3. 向量数据库: Pinecone / Milvus / Chroma / FAISS
4. 检索策略: 相似度 / BM25混合 / 重排序

进阶技术:
- HyDE: 先生成假设答案，再检索
- Multi-Query: 生成多个查询扩展
- Self-Query: 从问题提取结构化过滤条件
- Contextual Compression: 压缩检索结果
```

## 6.5 Tool Use (工具使用)

```
Function Calling流程:
1. 定义工具schema (JSON格式)
2. LLM决定是否调用工具
3. 解析调用参数
4. 执行工具
5. 将结果返回给LLM

{
  "name": "search_web",
  "description": "搜索互联网获取最新信息",
  "parameters": {
    "type": "object",
    "properties": {
      "query": {"type": "string", "description": "搜索关键词"}
    },
    "required": ["query"]
  }
}

MCP (Model Context Protocol):
你收集的资料里有"一文讲透MCP的原理及实践"
- Anthropic提出的工具标准化协议
- 统一的工具描述和调用格式
- 支持多种传输方式 (stdio, HTTP)
```

## 6.6 实践框架

```
主流框架:
┌─────────────────────────────────────────────────────────┐
│ LangChain: 最流行的LLM应用框架                          │
│ ├── Chains: 链式调用                                   │
│ ├── Agents: ReAct等模式                               │
│ ├── Memory: 对话记忆                                  │
│ └── Retrievers: RAG组件                               │
│                                                        │
│ LangGraph: 基于图的Agent编排                           │
│ ├── 状态机模式                                        │
│ ├── 复杂工作流                                        │
│ └── 人机交互节点                                      │
│                                                        │
│ LlamaIndex: 专注RAG的框架                              │
│                                                        │
│ AutoGPT/BabyAGI: 自主Agent                            │
│                                                        │
│ CrewAI: 多Agent协作                                    │
└─────────────────────────────────────────────────────────┘
```

---

# 第七部分：前沿方向与研究热点 🔮

## 7.1 推理模型 (Reasoning Models)

```
o1/o3, DeepSeek-R1, Claude的extended thinking

核心思想:
- 测试时计算 (Test-time Compute): 推理时花更多时间思考
- 长思维链: 展开详细的推理过程
- 强化学习训练: 奖励正确的推理路径

技术细节:
1. 训练大量数学/代码推理数据
2. 使用RL优化推理质量
3. 可能使用MCTS等搜索算法
4. 验证器判断推理正确性
```

## 7.2 多模态模型

```
GPT-4V, Claude Vision, Gemini

架构:
┌─────────────────────────────────────────────────────┐
│                                                     │
│  图像 → Vision Encoder → 投影层 → 文本空间          │
│                               ↓                    │
│  文本 → Text Embedding ──────→ LLM → 输出          │
│                                                     │
└─────────────────────────────────────────────────────┘

技术路线:
1. CLIP-style: 对比学习对齐视觉和文本
2. Flamingo-style: 交叉注意力融合
3. LLaVA-style: 简单投影 + 指令微调
```

## 7.3 长上下文

```
从4K → 8K → 32K → 128K → 1M+ tokens

技术:
1. 位置编码改进: RoPE外推, NTK-aware
2. 稀疏注意力: Sliding Window, Longformer
3. 线性注意力: RWKV, Mamba
4. 压缩记忆: MemGPT, Memorizing Transformer
5. 外部记忆: RAG增强
```

## 7.4 高效架构

```
Mamba / State Space Models (SSM):
- 线性时间复杂度 vs Transformer的O(n²)
- 隐式记忆机制
- 适合超长序列

Mixture of Experts (MoE):
- 参数量大，但激活参数少
- 例: Mixtral 8x7B, 实际只激活2x7B
- 训练挑战: 负载均衡
```

## 7.5 模型安全与对齐

```
问题:
- 有害内容生成
- 虚假信息
- 隐私泄露
- 能力滥用

技术:
- RLHF: 对齐人类偏好
- Constitutional AI: AI自我监督
- Red Teaming: 对抗测试
- Interpretability: 可解释性研究
```

---

# 📖 学习路线建议

```
阶段1: 基础 (1-2个月)
├── 线性代数、概率论
├── Python编程 + NumPy
├── 机器学习基础 (sklearn)
└── 动手实现: 线性回归、逻辑回归、决策树

阶段2: 深度学习 (2-3个月)
├── 神经网络原理
├── PyTorch框架
├── CNN、RNN实战
└── 动手实现: 图像分类、文本分类

阶段3: Transformer与LLM (2-3个月)
├── Transformer架构
├── BERT/GPT原理
├── HuggingFace生态
└── 动手: 微调预训练模型

阶段4: 强化学习 (1-2个月)
├── MDP、价值函数
├── DQN、Policy Gradient
├── PPO、SAC
└── 动手: 游戏AI、RLHF理解

阶段5: 前沿应用 (持续)
├── Agent开发
├── RAG系统
├── 多模态
└── 跟进最新论文

推荐资源 (结合你的收藏):
- 课程: CS336, CS 194/294-196
- 视频: 慢学AI, AI异教徒
- 文章: Anthropic Engineering Blog
- 实践: 扣子, LangChain
```

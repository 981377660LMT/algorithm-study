# 机器学习概览

机器学习基础。本部分简要介绍了基础 ML 问题——回归、分类、降维和聚类——以及用于解决这些问题的传统 ML 模型和数值算法。

---

### 1. 机器学习的核心逻辑

课件开篇强调了机器学习的本质：**Learning from Data**。

- **目标**：寻找一个函数 $f$。
- **输入**：过去的数据（Past Data，通常带标签）。
- **输出**：对未来数据（Future Data）进行预测。
  - _课件图示：通过训练数据，将一辆车的图片输入模型，输出 "car" 这个标签。_

---

### 2. 四大核心任务 (Tasks)

课件将机器学习任务主要划分为四大类，并区分了监督学习与无监督学习。

#### A. 监督学习 (Supervised Learning)

利用**带标签**的数据训练函数。

1.  **回归 (Regression)**
    - **目标**：预测**连续且有序 (Continuous & Ordered)** 的数值。
    - **核心特征**：数值之间有大小关系（如 $324K < \$1.2M$）。
    - **课件案例**：房价预测（Housing Price），根据房屋特征预测具体价格。
2.  **分类 (Classification)**
    - **目标**：预测**类别 (Categorical)**。
    - **核心特征**：类别之间通常**无序**（不能说 "Class 1 < Class 4"）。
    - **课件案例**：
      - **二分类 (Binary)**：垃圾邮件检测 (Spam Detection)。
      - **多分类 (Multi-class)**：人脸识别 (Face Recognition)。
      - **多标签 (Multi-label)**：图像识别中一张图既有 "Dog" 也有 "Shiba Inu" 标签。

#### B. 无监督学习 (Unsupervised Learning)

从**无标签**的数据中推断结构。 3. **聚类 (Clustering)**
_ **目标**：根据数据的相似性，将数据分组。
_ **输入**：向量 $x_1, \dots, x_n$ 和簇的数量 $k$。
_ **输出**：每个数据所属的类别 $y_i \in \{1, \dots, k\}$。 4. **降维 (Dimensionality Reduction)**
_ **目标**：将高维向量转化为低维向量，同时保留重要信息。
_ **应用**：数据可视化（Visualization）、数据预处理（以提高下游任务效率）。
_ **方法**：PCA, Autoencoder (课件中展示了用 Autoencoder 将 MNIST 手写数字降维并将相似数字聚在一起的可视化)。

---

### 3. 三层知识架构：Task vs Method vs Algorithm

这是该课件最精彩的总结部分，它帮初学者理清了容易混淆的概念：

| 层级                  | 含义                       | 例子                                                                                                                   |
| :-------------------- | :------------------------- | :--------------------------------------------------------------------------------------------------------------------- |
| **Tasks (问题)**      | 我们想解决什么问题？       | 回归、分类、聚类、降维                                                                                                 |
| **Methods (模型)**    | 我们用什么数学模型来解决？ | 神经网络 (Neural Networks)<br>支持向量机 (SVM)<br>逻辑回归 (Logistic Regression)<br>决策树 (Decision Tree)             |
| **Algorithms (求解)** | 我们如何计算出模型的参数？ | **梯度下降 (Gradient Descent, GD)**<br>随机梯度下降 (SGD)<br>坐标下降 (Coordinate Descent)<br>牛顿法 (Newton's Method) |

**深度理解这个架构：**

- 同一个 **Task**（如分类），可以用不同的 **Method**（如 SVM 或 神经网络）来解决。
- 同一个 **Method**（如逻辑回归），可以用不同的 **Algorithm**（如 SGD 或 牛顿法）来优化参数。
- **深度学习 (Deep Learning)** 处于 **Method** 这一层（神经网络），通常使用 **SGD** 及其变体作为 **Algorithm** 来解决分类或回归等 **Task**。

### 总结

这份 ML Basics 课件不在于罗列复杂的公式，而在于不仅介绍了各类**任务**的区别（特别是有序 vs 无序标签），更重要的是建立了 **Task-Method-Algorithm** 的思维导图。这为后续学习深度学习（Deep Learning）打下了基础——深度学习就是一种强大的 **Method**，主要通过 **SGD Algorithm** 来解决各种复杂的 **Tasks**。

---

基于 **CS583: Deep Learning** 课程的第二份课件 `1_Models.pdf`（Modeling & Computation），我们从“任务”深入到**“建模 (Modeling)”**与**“计算 (Computation)”**这两个核心支柱。

如果说上一部分是在决定“**我们要去哪里**”（定义任务），这一部分就是在解决“**我们怎么去**”（构建模型）以及“**用什么交通工具去**”（计算与优化）。

---

### 第二部分：建模与计算 (Modeling & Computation)

#### 1. 建模 (Modeling)：从线性到深度

建模的核心是寻找一个函数 $f(x)$ 来拟合数据。课件展示了模型能力的进化之路。

**A. 线性模型 (Linear Models) —— 起点与局限**

- **形式**：$f(x) = x^T w$（特征加权求和）。
- **求解**：通过最小二乘法 (Least Squares) 最小化均方误差损失 $L(w) = \sum (x_i^T w - y_i)^2$。
- **致命弱点**：**表达能力不足 (Not Expressive)。**
  - _课件案例_：给定一张人脸照片预测年龄。线性模型假设年龄只是像素点的加权平均，这显然无法捕捉复杂的面部特征（如皱纹、皮肤纹理），因此效果极差。

**B. 传统机器学习 (Traditional Approaches) —— 特征工程时代**
在深度学习爆发前，为了解决线性模型能力不足的问题，人们手动设计特征提取器。

- **流程**：原始图像 $\to$ **人工特征 (SIFT, HOG, LBP)** $\to$ 线性分类器 $\to$ 结果。
- **局限**：特征的好坏完全依赖专家的经验，如果不工作很难调整。

**C. 深度学习模型 (Deep Models) —— 特征学习时代**
深度学习不再依赖人工特征，而是让神经网络自己学习数据的**表示 (Representation)**。课件介绍了三大主流架构：

1.  **CNN (卷积神经网络)**

    - **专长**：图像数据 (Image Data)。
    - **核心能力**：将原始像素转换为有效的特征表示 (Feature Extraction)。
    - **应用**：
      - **医疗诊断**：如皮肤癌检测（精度匹敌人类专家）。
      - **自动驾驶**：识别行人、路标、车辆。

2.  **RNN (循环神经网络)**

    - **专长**：序列数据 (Sequence Data)，如时间序列、文本、语音。
    - **应用**：机器翻译 (Machine Translation)、语音识别。

3.  **DRL (深度强化学习)**
    - **专长**：决策与控制。
    - **应用**：电子游戏、机器人控制（如 Boston Dynamics 的 Atlas 机器人）。

---

#### 2. 计算 (Computation)：让模型动起来

有了模型结构（如 CNN）和目标（Loss Function），我们还需要一种方法算出具体的参数 $w$。这就是**计算**。

**A. 核心算法：梯度下降 (Gradient Descent)**
数学上，这被称为优化问题：$\theta^* = \min_\theta L(\theta)$。

- **直观理解**：甚至最复杂的神经网络训练，本质上也是在重复以下步骤：
  1.  初始化参数 $w$。
  2.  计算当前的梯度 $\nabla L(w)$（告诉我们要往哪个方向走才能让误差变小）。
  3.  更新参数：$w \leftarrow w - \alpha \cdot \text{gradient}$ （$\alpha$ 是步长）。
- **变体**：为了更快更稳，我们使用 **SGD (随机梯度下降)**、**Momentum**、**RMSProp** 或 **ADAM**。

**B. 计算挑战 (Computational Challenges)**
现代深度学习面临“双重巨大”：

1.  **Big Data**：如 ImageNet 有 1400 万张图片。
2.  **Big Model**：如 ResNet-50 有 2500 万个参数。

- _现实代价_：用单块 GPU 训练 ResNet-50 可能需要 14 天。这迫使我们需要更高效的算法和分布式系统（如 TensorFlow, PyTorch 的底层优化）。

---

#### 3. 实践中的“两条腿走路”

课件最后总结了成为一名 ML 专家的双重修养：

- **侧重 Modeling 的能力 (Data Scientist 视角)**：
  - 理解数据和问题。
  - 设计网络结构、选择激活函数和损失函数。
  - **目标**：提高预测精度 (Accuracy)。
- **侧重 Computation 的能力 (ML Engineer 视角)**：
  - 设计高效算法，实现分布式系统。
  - 代码优化 (Optimize Code)。
  - **目标**：提高运行效率和可行性 (Efficiency)。

接下来的课件（根据文件名判断）应该会深入到具体的数学方法，如 `Regression`（回归）和 `Classification`（分类）的细节。如果需要继续讲解下一部分，请告诉我。

---

### 附录：机器学习核心算法体系解构

为了更深入理解 **"Methods (模型)"** 这一层，我们将常见的机器学习算法进行系统分类和通俗解构。你可以把这些算法想象成不同的“工具钳”，每一把都有其最擅长的场景。

#### 1. 线性家族 (The Linear Family) —— 简单直观的基石

这一类模型假设世界是简单的、线性的（或者可以通过变换变得线性）。

- **线性回归 (Linear Regression)**

  - **核心思想**：画一条直线（或平面）穿过所有数据点，使得点到直线的距离之和最小。
  - **比喻**：根据过去几个月的电费账单，画一条线来预测下个月的电费。
  - **适用场景**：预测房价、温度等连续数值。

- **逻辑回归 (Logistic Regression)**
  - **注意**：尽管名字叫“回归”，但它实际上是一个**分类**算法。
  - **核心思想**：通过一个“S 型函数”（Sigmoid）将线性回归的结果压缩到 0 到 1 之间，代表概率。
  - **比喻**：将考试分数（数值）转化为“及格”或“不及格”的概率。如果结果 > 0.5 判为是，否则判为否。
  - **适用场景**：二分类问题，如广告是否被点击、用户是否流失。

#### 2. 树模型家族 (The Tree Family) —— 模拟人类决策逻辑

这一类模型最接近人类“如果不……就……”的思考方式，可解释性极强，也是目前表格型数据竞赛（如 Kaggle）中的霸主。

- **决策树 (Decision Tree)**

  - **核心思想**：不断的通过“它是红色的吗？”、“它是圆的吗？”这样的二选一问题，把数据切分成不同的类别。
  - **比喻**：“20 个问题”游戏（Akinator）。
  - **优点**：可视化非常直观，白盒模型。

- **随机森林 (Random Forest)**

  - **核心思想**：**Bagging (人多力量大)**。种很多棵决策树，每棵树只看通过随机采样得到的一部分数据和特征。最后大家投票决定结果。
  - **比喻**：由多个普通专家组成的委员会，少数服从多数，避免单个人（单棵树）犯错（过拟合）。

- **梯度提升树 (GBDT / XGBoost / LightGBM)**
  - **核心思想**：**Boosting (知错能改)**。不是大家一起投票，而是一个接一个地学。第一棵树预测错了的部分，由第二棵树重点去学，以此类推。
  - **比喻**：做错题本。一直在死磕那些做错的题目，直到分数学满分为止。目前处理表格数据的 SOTA (State of the Art) 模型。

#### 3. 距离与几何家族 (Distance & Geometry) —— 空间派

这一类模型通过计算样本在数学空间中的距离或位置来分类。

- **K-近邻 (K-Nearest Neighbors, KNN)**

  - **核心思想**：物以类聚。看你周围最近的 K 个邻居主要是谁，你就是谁。
  - **比喻**：孟母三迁。如果你住在书香门第，你大概率也是读书人。
  - **缺点**：计算量大，必须存下所有的历史数据。

- **支持向量机 (Support Vector Machines, SVM)**
  - **核心思想**：在两类数据中间找一条路（分界线），尽量让这条路修得最宽，离两边的数据都最远。如果线性不可分，就升到高维空间去切分（Kernel Trick）。
  - **比喻**：楚河汉界。不仅要划清界限，还要在界限两边留出足够的缓冲地带。
  - **地位**：在深度学习流行之前（2000-2012），SVM 是分类算法的王者。

#### 4. 概率家族 (The Probabilistic Family) —— 统计派

- **朴素贝叶斯 (Naive Bayes)**
  - **核心思想**：基于条件概率公式。假设所有特征之间相互独立（虽然现实中不一定成立，所以叫“朴素”），直接计算某特征出现时属于某类的概率。
  - **适用场景**：早期的垃圾邮件过滤（只要出现“发票”、“中奖”词汇，大概率是垃圾邮件）、文本分类。

#### 5. 神经网络家族 (The Neural Network Family) —— 仿生学派

深度学习 (Deep Learning) 的大本营。通过模拟人脑神经元的连接，自动学习特征。

- **多层感知机 (MLP / DNN)**

  - **核心思想**：逻辑回归的超级叠加版，每一层都在进行特征变换。
  - **适用场景**：基础的拟合任务。

- **卷积神经网络 (CNN)**

  - **核心结构**：卷积层 (Convolution) + 池化层 (Pooling)。
  - **核心思想**：像人眼一样，先看局部特征（线条、边缘），再组合成形状，最后组合成物体。具有**平移不变性**。
  - **适用场景**：图像识别、计算机视觉。

- **循环神经网络 (RNN / LSTM / Transformer)**
  - **核心思想**：有记忆功能。今天的输出不仅取决于今天的输入，还取决于昨天的状态。
  - **适用场景**：NLP（自然语言处理）、语音识别、股票预测等序列数据。

# 降维

这份文档名为《Singular Value Decomposition (SVD)》，是 Shusen Wang 教授关于深度学习中降维（Dimensionality Reduction）技术的第一部分讲义。

文档深入浅出地讲解了奇异值分解（SVD）的核心数学原理、截断 SVD（Truncated SVD）在数据压缩中的应用，以及如何使用幂迭代（Power Iteration）高效计算 SVD。

以下是对文档内容的深度解构：

### 1. 核心数学架构：SVD 分解

SVD 是线性代数中最重要的分解之一，它将任意矩阵分解为三个部分。

- **基本定义**:
  对于任意矩阵 $\mathbf{A} \in \mathbb{R}^{m \times n}$，若其秩（Rank）为 $r$，SVD 将其分解为：
  $$ \mathbf{A} = \sum\_{i=1}^r \sigma_i \mathbf{u}\_i \mathbf{v}\_i^T $$
    这可以理解为将矩阵 $\mathbf{A}$ 拆解为 $r$ 个**秩-1 矩阵**的加权和。

- **三个关键组件**:
  1.  **奇异值 ($\sigma_i$)**: 标量，且 $\sigma_1 \ge \sigma_2 \ge \dots \ge \sigma_r > 0$。它们代表了对应分量的重要性（能量）。
  2.  **左奇异向量 ($\mathbf{u}_i$)**: $\in \mathbb{R}^m$，构成一组标准正交基（Orthonormal Basis）。
  3.  **右奇异向量 ($\mathbf{v}_i$)**: $\in \mathbb{R}^n$，也构成一组标准正交基。
  - *标准正交基*意味着向量模长为 1（Unit $\ell_2$-norm），且两两正交（点积为 0）。

### 2. 工程应用：截断 SVD (Truncated SVD)

全量 SVD 保留了所有信息，但往往我们只需要“主要特征”。

- **定义**:
  只保留前 $k$ 个最大的奇异值及其对应的向量（$k < r$）：
  $$ \mathbf{A}_k = \sum_{i=1}^k \sigma_i \mathbf{u}\_i \mathbf{v}\_i^T $$
- **物理意义**: $\mathbf{A}_k$ 是 $\mathbf{A}$ 的**最佳秩-$k$ 近似**。
  - 优化目标：$\mathbf{A}_k = \mathop{\arg\min}_{\mathbf{B}} \|\mathbf{A} - \mathbf{B}\|_F^2 \quad \text{s.t. rank}(\mathbf{B}) \le k$
  - 这里的 $\|\cdot\|_F$ 是 **Frobenius 范数**（矩阵元素的平方和开根号，类似于向量的欧几里得距离）。
- **误差分析**:
  舍弃的信息量可以用未被选中的奇异值的平方和来衡量：
  $$ \|\mathbf{A} - \mathbf{A}_k\|\_F^2 = \sum_{i=k+1}^r \sigma_i^2 $$
  这意味着如果尾部的奇异值非常小，截断后的损失就微乎其微。
- **压缩案例**:
  文档用一张 $1000 \times 1500$ 的图片举例。
  - 原图数据量：$1.5 \times 10^6$ 个数值。
  - Rank-100 SVD：只需要存储 $100$ 个 $\sigma$， $100 \times 1000$ 个 $\mathbf{U}$ 和 $100 \times 1500$ 个 $\mathbf{V}$。总计约 $0.25 \times 10^6$ 个数值。
  - **效果**：仅用 **17%** 的存储空间，就能重建出极高质量的图像（仅损失了极小的细节）。

### 3. 如果高效计算：幂迭代 (Power Iteration)

对于超大矩阵，计算全量 SVD 极其缓慢。如果只需要前 $k$ 个主成分，可以使用幂迭代法。

- **数学转换**:
  计算 $\mathbf{A}$ 的 SVD 等价于计算 $\mathbf{A}^T\mathbf{A}$ 的特征值分解：
  $$ \mathbf{A}^T\mathbf{A} = \sum\_{i=1}^r \sigma_i^2 \mathbf{v}\_i \mathbf{v}\_i^T $$
    这里的特征值是 $\sigma_i^2$，特征向量是右奇异向量 $\mathbf{v}_i$。

- **算法流程 (求 Top-1)**:

  1.  随机初始化向量 $\mathbf{x}$。
  2.  重复迭代：$\mathbf{x} \leftarrow \mathbf{A}^T\mathbf{A}\mathbf{x}$，然后归一化 $\mathbf{x} \leftarrow \mathbf{x} / \|\mathbf{x}\|$。
  3.  最终 $\mathbf{x}$ 会收敛到 $\mathbf{v}_1$。

- **收敛原理**:
  任意向量 $\mathbf{x}$ 都可以表示为基向量的线性组合：$\mathbf{x} = \sum \alpha_i \mathbf{v}_i$。
  每次乘以 $\mathbf{A}^T\mathbf{A}$，相当于给分量乘以 $\sigma_i^2$。
  经过 $t$ 次迭代，第 $i$ 个分量变为 $\sigma_i^{2t}$。
  由于 $\sigma_1 > \sigma_2$，比值 $(\frac{\sigma_i}{\sigma_1})^{2t}$ 会随着 $t$ 增大迅速趋近于 0（对于所有 $i > 1$）。
  因此，主成分 $\mathbf{v}_1$ 会在迭代中“幸存”下来并占据主导。

- **求 Top-k (Block Power Iteration)**:
  如果是求前 $k$ 个，则初始化一个 $n \times k$ 的矩阵 $\mathbf{X}$，在每次迭代 $\mathbf{X} \leftarrow \mathbf{A}^T\mathbf{A}\mathbf{X}$ 后，增加一步 **正交化 (Orthogonalization)** 操作（如 QR 分解），防止所有列向量都收敛到同一个 $\mathbf{v}_1$ 上。

### 总结

这就讲义构建了一个完整的闭环：

1.  **理论**: 用 SVD 拆解矩阵。
2.  **应用**: 用 Truncated SVD 抛弃次要信息进行降维/压缩。
3.  **实现**: 用 Power Iteration 快速算出所需的 Truncated SVD，避免了昂贵的全量计算。

---

这份文档名为《Principal Component Analysis (PCA)》，是 Shusen Wang 教授关于深度学习中降维技术的第二部分讲义。文档从统计学基础出发，详细解构了 PCA 的几何直观、数学推导、算法步骤，并结合海洋温度数据的真实案例进行了深入分析。

以下是对文档内容的深度解构：

### 1. 理论基石：从统计学到几何变换

PCA 的本质是寻找一个新的坐标系，使得数据在这个坐标系下的**方差（Variance）**最大化。

- **统计学基础**:

  - **标量**: 均值 $\mu$ 衡量中心，方差 $\sigma^2$ 衡量离散程度。
  - **向量**: 均值向量 $\boldsymbol{\mu} \in \mathbb{R}^d$ 定义了数据的中心位置；**协方差矩阵 (Covariance Matrix)** $\mathbf{C} \in \mathbb{R}^{d \times d}$ 描述了不同维度间的相关性以及各个方向上的离散程度。

- **PCA 的几何直观**: 也就是从这组数据中提取特征的过程，可以被拆解为两个简单的几何变换：
  1.  **平移 (Translation)**: 将数据整体移动，使重心（样本均值）落在原点。这就是步骤中的“去均值”（Subtract the mean）。
  2.  **旋转 (Rotation)**: 旋转坐标轴，使得：
      - 第一坐标轴（第 1 主成分）对准数据方差最大的方向。
      - 第二坐标轴（第 2 主成分）对准第二大方差的方向（且与第一轴正交）。
      - 以此类推。

### 2. 核心算法：基于 SVD 的实现

文档揭示了 PCA 的计算过程实际上就是对**去均值后的数据矩阵**进行奇异值分解（SVD）。

- **算法流程**:

  1.  **输入**: 数据集 $\{\mathbf{x}_1, \dots, \mathbf{x}_n\}$，以及目标维度 $k$。
  2.  **去均值**: 计算样本均值 $\hat{\boldsymbol{\mu}} = \frac{1}{n}\sum \mathbf{x}_i$，令 $\tilde{\mathbf{x}}_i = \mathbf{x}_i - \hat{\boldsymbol{\mu}}$。
  3.  **构造矩阵**: 将去均值后的向量堆叠成矩阵 $\tilde{\mathbf{X}} \in \mathbb{R}^{n \times d}$（这里 $n$ 是样本数，$d$ 是特征维数）。
  4.  **计算 SVD**: 对 $\tilde{\mathbf{X}}$ 进行截断 SVD 分解：
      $$ \tilde{\mathbf{X}} \approx \sum\_{i=1}^k \sigma_i \mathbf{u}\_i \mathbf{v}\_i^T $$
        其中 $\mathbf{v}_1, \dots, \mathbf{v}_k$ 就是我们寻找的旋转矩阵（即主成分方向）。
  5.  **投影/降维**: 将原数据投影到 $\mathbf{V} = [\mathbf{v}_1, \dots, \mathbf{v}_k]$ 定义的空间中。

- **为什么是 SVD? (数学原理)**:
  - 样本协方差矩阵定义为 $\hat{\mathbf{C}} \propto \tilde{\mathbf{X}}^T \tilde{\mathbf{X}}$。
  - 将 $\tilde{\mathbf{X}}$ 的 SVD 代入，可以证明 $\tilde{\mathbf{X}}^T \tilde{\mathbf{X}}$ 的特征向量正是 $\mathbf{v}_i$。
  - 最大奇异值 $\sigma_1$ 对应的 $\mathbf{v}_1$ 正是最大化 $\mathbf{v}^T \hat{\mathbf{C}} \mathbf{v}$ 的方向，即最大方差方向。

### 3. 特征工程视角：信号与噪声

为什么我们只需要 Top-$k$ 个主成分？

- **信号 (Signal)**: Top 奇异值对应的方向包含了数据中的主要变化模式（Features）。
- **噪声 (Noise)**: 尾部的奇异值对应的方向通常包含测量噪声或不重要的细节。
- 通过截断 SVD（只取前 $k$ 个），我们不仅实现了降维，还默认执行了去噪操作。

### 4. 实战案例：海洋温度数据分析

文档展示了一个极其生动的 PCA 应用案例，用于分析 1979-2011 年全球海洋表面温度数据。

- **数据规模**: 高维张量（时间 $\times$ 经度 $\times$ 纬度）。
  - 样本数 $n \approx 4.6$ 万（时间点）。
  - 特征数 $d \approx 25.9$ 万（地理位置网格）。
- **PCA 分析结果**:
  仅提取前 4 个主成分，就能发现极具物理意义的模式，而无需任何气象学先验知识：
  1.  **PC1 & PC2**: 捕捉到了**年度周期 (Annual Cycle)**，即春夏秋冬的温度变化规律。
  2.  **PC3**: 捕捉到了**厄尔尼诺现象 (El Niño)**，太平洋赤道海域水温异常升高的模式。
  3.  **PC4**: 捕捉到了**拉尼娜现象 (La Niña)**，与厄尔尼诺相反的降温模式。

### 5. 总结

- **PCA 的本质**: 去均值 + 旋转。
- **核心工具**: 主要依赖 SVD 来寻找旋转矩阵。
- **价值**: 它是无监督学习中最强大的工具之一，既可以用于数据压缩和加速训练，也可以用于探索性数据分析（EDA），帮助我们从海量杂乱数据中“看见”隐藏的物理规律（如气候变化）。

---

这份文档名为《Singular Value Decomposition (SVD) and Principal Component Analysis (PCA)》，是王树森（Shusen Wang）教授深度学习课程的讲义。它严谨地从线性代数底层推导了 SVD 的原理，介绍了如何通过幂迭代（Power Iteration）计算 SVD，并最终引出 PCA（主成分分析）作为 SVD 的典型应用。

以下是对该文档的**深度分析与解构**：

### 1. 核心视角：基变换 (Change of Basis)

文档首先建立了一个高阶视角：**PCA 本质上是一次标准正交基（Orthonormal Basis）的变换**。

- **原始数据**：存在于某个基（通常是标准基 $e_1, ..., e_d$）下，数据可能分散且相关性强。
- **目标**：找到一组新的基 $v_1, ..., v_d$，使得数据在这组基下的表示 $x'$ 具有良好的性质——即“红色项（主要成分）”很大，“蓝色项（次要成分）”很小。
- **数学本质**：旋转（Rotation）。如果丢弃次要成分，就是降维。

### 2. 数学引擎：奇异值分解 (SVD)

SVD 是实现上述变换的核心工具。文档强调了两种形式：

- **向量形式**：$A = \sum_{i=1}^r \sigma_i u_i v_i^T$。这种形式揭示了矩阵可以拆解为一系列 Rank-1 矩阵的和。
- **矩阵形式**：$A = U\Sigma V^T$。

**关键性质解析**：

- **最佳 Rank-k 近似**：这是 SVD 最重要的物理意义。截断 SVD（只取前 $k$ 项）是 Frobenius 范数意义下对原矩阵 $A$ 最好的 Rank-k 近似。这是图像压缩、降噪和 PCA 的理论基石。
- **矩阵范数**：文档建立了奇异值与矩阵能量的联系。
  - Frobenius 范数（整体能量）：$\|A\|_F^2 = \sum \sigma_i^2$
  - 谱范数（最大拉伸能力）：$\|A\|_2 = \sigma_1$

### 3. 计算方法：从原理到实践

仅仅知道 SVD 存在是不够的，文档深入探讨了**如何高效计算**大规模矩阵的 SVD，特别是只关注前 $k$ 个特征值时（Truncated SVD）。

- **朴素方法：幂迭代 (Power Iteration)**
  - 原理：利用矩阵多次乘以随机向量，结果会自然向最大特征值对应的特征向量收敛。
  - 缺点：只能求 Top-1。虽然可以通过剥离（Deflation，$A \leftarrow A - \sigma_1 u_1 v_1^T$）求后续分量，但**误差会指数级累积**（第 1 个算的稍微不准，第 2 个就会更不准，以此类推）。
- **工业级方法：块幂迭代 (Block Power Iteration)**
  - 改进：不同于每次迭代一个向量，而是迭代一个矩阵 $X$（包含 $k$ 个列）。
  - 关键步骤：每次乘法后做正交化（Orthogonalization, e.g., QR 分解）。
  - 优势：数值稳定性高，误差不会像 Deflation 那样失控，收敛速度取决于谱间隙（Spectral Gap, $\sigma_k / \sigma_{k+1}$）。

### 4. 深度学习中的 PCA：不仅仅是统计学

文档从线性代数角度重构了 PCA 的四个步骤：

1.  **中心化 (Centering)**：$x' = x - \mu$。这是为了让协方差矩阵的形式简化为 $X^T X$。
2.  **SVD 分解**：直接对中心化后的数据矩阵 $X'$ 做 SVD，得到右奇异向量 $V$。
    - **关键点**：在统计学中，我们通常是对协方差矩阵做特征值分解；但在计算科学中，直接对数据矩阵做 SVD 数值上更稳定，因为避免了构造 $X^TX$ 带来的精度损失（条件数平方问题）。
3.  **旋转 (Rotation)**：$z = V^T x'$。这步将最大方差方向对齐到轴上。
4.  **降维**：取前 $k$ 维。

**为什么是 $v_1$？**
文档附录通过最大化瑞利商（Rayleigh Quotient）$\frac{w^T C w}{w^T w}$ 证明了：协方差矩阵 $C$ 的最大特征向量（即 $X'$ 的最大右奇异向量 $v_1$）就是数据方差最大的方向。这就从数学上闭环解释了“为何 SVD 找到的基就是 PCA 想要的基”。

### 5. 总结与启示

这份讲义的独到之处在于：

1.  **打破黑盒**：没有直接调用 `sklearn.pca`，而是手写了基于幂迭代的求解器原理，解释了底层库（如 ARPACK, cuSOLVER）的工作机制。
2.  **效率优先**：明确指出计算 Truncated SVD（$O(ndk)$）远比 Full SVD（$O(nd^2)$）快，指导工程选型。
3.  **直观与严谨并重**：用“最大化红色项，最小化蓝色项”直观解释降维，同时在附录中给出了严谨的收敛性证明。

这不仅是关于 SVD/PCA 的数学课，更是一份关于**大规模数值计算**的入门指南。

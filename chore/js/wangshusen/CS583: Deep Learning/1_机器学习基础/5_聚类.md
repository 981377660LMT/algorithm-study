# 聚类

这份 Slide 是王树森教授“深度学习”课程关于**聚类（Clustering）** 的章节。虽然标题是 Clustering，但核心内容聚焦在 **K-Means 聚类**及其求解算法 **Lloyd's Algorithm**。

以下是对这份 Slide 的分析与解构：

### 1. 聚类任务定义 (Clustering Task)

- **输入**: $n$ 个特征向量 $\mathbf{x}_1, \dots, \mathbf{x}_n \in \mathbb{R}^d$，以及一个预设的簇数量 $k$ (通常 $k \ll n$)。
- **输出**: 每个样本的类别标签 $y_1, \dots, y_n \in \{1, \dots, k\}$。
- **数学定义**: 将数据集 $N$ 划分为 $k$ 个**不相交（Disjoint）** 的子集 $S_1, \dots, S_k$。
  - 并集为全集：$S_1 \cup \dots \cup S_k = N$
  - 交集为空：$S_i \cap S_j = \emptyset, \forall i \neq j$

### 2. K-Means 模型 (K-Means Model)

PPT 强调了**模型（Model）**与**算法（Algorithm）**的区别。K-Means 首先是一个优化模型。

- **核心概念**: 质心（Centroid）。每个簇 $S_j$ 有一个质心 $\boldsymbol{\mu}_j$。
- **目标函数**: 最小化所有点到其所属簇质心的**欧几里得距离平方和**（即最小化簇内方差）。
  $$ \min*{S_1, \dots, S_k} \sum*{j=1}^k \sum\_{\mathbf{x}\_i \in S_j} \|\mathbf{x}\_i - \boldsymbol{\mu}\_j\|^2 $$
    其中 $\boldsymbol{\mu}_j$ 通常定义为簇 $S_j$ 中所有点的均值。
- **难度**: 这是一个**组合优化问题（Combinatorial Optimization Problem）**，已被证明是 **NP-hard** 的。因此无法求得全局最优解，只能求近似解。

### 3. Lloyd's Algorithm (常说的 K-Means 算法)

这是求解 K-Means 模型最经典的启发式算法。

- **步骤 1: 初始化 (Initialization)**

  - 随机选择 $k$ 个点作为初始质心 $\boldsymbol{\mu}_1, \dots, \boldsymbol{\mu}_k$。
  - **进阶初始化** (Slide 18): 类似于 **K-Means++** 的思想。
    - 第一个质心随机选。
    - 后续质心选择距离当前已有质心**最远**的点。这样可以分散质心，避免局部最优。

- **步骤 2: 迭代 (Repeat until convergence)**

  - **分配 (Assignment)**: 固定质心，更新簇分配。
    - 对每个数据点 $\mathbf{x}_i$，计算它与所有 $k$ 个质心的距离。
    - 将其分配给**最近**的那个质心所属的簇。
    - 这一步本质上是构建 Voronoi 图（Voronoi Partition）。
  - **更新 (Update)**: 固定簇分配，更新质心。
    - 计算新簇的均值：$\boldsymbol{\mu}_j^* = \frac{1}{|S_j|} \sum_{\mathbf{x} \in S_j} \mathbf{x}$。
    - **PPT 的一个特殊细节 (Slide 21)**: 这里的更新步骤写道：“New centroid $\boldsymbol{\mu}_j$: the feature vector (among $\mathbf{x}_1 \dots \mathbf{x}_n$) closest to $\boldsymbol{\mu}_j^*$”。
    - _注意_：这实际上是 **K-Medoids** 或者 **Discrete K-Means** 的变体（要求质心必须是真实存在的数据点）。标准的 K-Means 更新通常直接使用均值 $\boldsymbol{\mu}_j^*$ 作为新质心，而不强制它是数据点。这种变体通常用于保持稀疏性或解释性。

- **收敛**: 当质心不再移动或数据点的分配不再变化时，算法停止。

### 4. 总结与澄清 (Summary)

- **"没有所谓的 K-Means 算法"**: Slide 45 提出了一个有趣的观点。严格来说，K-Means 是一个**优化问题/模型**。通常大家所说的“K-Means 算法”实际上是指 **Lloyd's Algorithm**（当然还有 Forgy, MacQueen, Hartigan 等其他求解算法）。
- **应用**: 可以用于构建 **KNN 的空间划分**（加速最近邻搜索）。
- **评估指标**:
  - K-Means 目标函数值（越小越好）。
  - Accuracy（如果有真实标签）。
  - NMI (Normalized Mutual Information)。

### 深度解构：为什么讲 K-Means？

在“深度学习”课程中讲 K-Means (Slide 46 提到 KNN 加速) 通常有几个目的：

1.  **无监督学习基石**: 理解聚类思想。
2.  **向量量化 (Vector Quantization)**: K-Means 可以看作是一种对向量空间的压缩。将连续的无限空间压缩为 $k$ 个离散的码本（Codebook/Centroids）。这在压缩模型、加速检索（如 Faiss）中非常重要。
3.  **RBF 网络/注意力机制的基础**: K-Means 的分配步骤涉及计算距离和 Soft/Hard Assignment，这与 RBF 神经网络和 Attention 机制中的 Key-Query 匹配有数学上的联系。

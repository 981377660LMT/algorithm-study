# 分类

这份 PPT 是王树森教授“深度学习”课程（CS583）关于**二分类（Binary Classification）**的第一部分。它主要涵盖了从数学基础（矩阵求导）到逻辑回归（Logistic Regression）模型的推导、优化算法（GD/SGD）以及评估指标（ROC 曲线）。

以下是对这份 Slide 内容的深入讲解：

### 第一部分：数学基础 - 向量与矩阵求导 (Slides 2-13)

这部分是深度学习中反向传播的基础。PPT 列举了几种关键的求导规则，用于后续推导逻辑回归的梯度。

1.  **标量对向量求导 (Scalar w.r.t Vector)**:

    - 如果 $y = \sum x_i^2 = \|x\|^2$，则 $\frac{\partial y}{\partial x} = 2x$。
    - 如果 $y = w^T x$（线性函数），则 $\frac{\partial y}{\partial x} = w$。
    - **关键公式**: 逻辑回归中会用到 $y = \log(1 + e^{-x})$ 的求导形式。

2.  **向量对向量求导 (Vector w.r.t Vector)**:

    - 结果是一个**雅可比矩阵 (Jacobian Matrix)**。
    - 如果 $y = Ax$（线性变换），则 $\frac{\partial y}{\partial x} = A^T$（注意排版习惯，这是分母布局 derivative by denominator layout 的结果，有时也记为 $A$ 取决于定义）。

3.  **链式法则 (Chain Rule)**:
    - 这是神经网络的核心：$\frac{dz}{dx} = \frac{\partial y}{\partial x} \frac{dz}{dy}$（矩阵乘法形式）。

### 第二部分：二分类问题定义 (Slides 14-25)

1.  **任务**:

    - 输入：特征向量 $x \in \mathbb{R}^d$。
    - 标签：$y \in \{-1, +1\}$。
    - 目标：学习一个函数 $f(x)$ 来预测 $y$。

2.  **线性分类器 (Linear Classifier)**:

    - 决策边界是一个超平面：$w^T x + b = 0$。
    - 为了简化符号，通常将 $b$ 并入 $w$（intercept trick），即 $x \leftarrow [x, 1]^T$，$w \leftarrow [w, b]^T$。
    - **判别规则**:
      - 如果 $y=+1$，希望 $w^T x > 0$。
      - 如果 $y=-1$，希望 $w^T x < 0$。
      - 合并起来：**希望 $y_i w^T x_i > 0$**。

3.  **损失函数的演变**:
    - **直观想法**: 直接最小化分类错误率（Classification Error）。即统计 $y_i w^T x_i < 0$ 的个数。
    - **问题**: 这种函数是不连续的、非凸的（阶跃函数），很难求导优化（NP-hard）。
    - **解决方案 (Logistic Regression)**: 使用连续、光滑且凸的 **Logistic Loss** 来近似。
    - **公式**:
      $$ \min*w \sum*{i=1}^n \log(1 + e^{-y_i w^T x_i}) $$
      - 这里的 $\log(1 + e^{-z})$ 叫做 softplus 函数，当 $z = y w^T x$ 越大（分类越正确），损失越接近 0；当 $z$ 越小（分类严重错误），损失线性增加。

### 第三部分：优化算法 (Optimization) (Slides 26-42)

如何找到最优的 $w$？PPT 对比了四种算法。

1.  **梯度计算**:

    - 利用链式法则，推导出 Loss 对 $w$ 的梯度：
      $$ g = \sum\_{i=1}^n \frac{-y_i x_i}{1 + e^{y_i w^T x_i}} $$

2.  **梯度下降 (Gradient Descent, GD)**:

    - **做法**: 每一步都用**所有** $n$ 个样本的梯度总和来更新 $w$。
    - **复杂度**: 每次迭代 $O(nd)$。当 $n$ 很大时非常慢。

3.  **随机梯度下降 (Stochastic Gradient Descent, SGD)**:

    - **做法**: 每次迭代随机抽取**一个**样本 $j$，仅用它的梯度来更新 $w$。
    - **复杂度**: 每次迭代 $O(d)$。速度极快，但梯度有噪声，路径震荡。
    - **收敛性**: 按“Epoch”（遍历一次数据）来看，SGD 收敛通常比 GD 快，因为它更新频率高。

4.  **加速算法 (Momentum)**:
    - **AGD (Accelerated GD)** 和 **Accelerated SGD**: 引入“动量” (Momentum) $v$。
    - $v \leftarrow \beta v + g$
    - $w \leftarrow w - \alpha v$
    - 动量项可以帮助因梯度的方向一致而加速，或在震荡处减小摆动。

### 第四部分：模型评估 (Evaluation) (Slides 43-59)

如何衡量分类器好不好？

1.  **准确率 (Accuracy)**:

    - $1 - ErrorRate$。
    - **缺陷**: 在**类别不平衡 (Class Imbalanced)** 的情况下没有意义。例如：癌症检测中 99% 样本是健康的，如果你全部预测“健康”，准确率高达 99%，但这毫无用处。

2.  **混淆矩阵指标**:

    - **TP (True Positive)**: 真的正例（预测对的癌症）。
    - **FN (False Negative)**: 漏报（有病没查出来）。
    - **FP (False Positive)**: 误报（没病说有病）。
    - **TN (True Negative)**: 真的负例（预测对的健康）。

3.  **关键比率**:

    - **TPR (True Positive Rate)**: $\frac{TP}{TP + FN}$。也就是**召回率 (Recall)**。所有真病人里你找出了多少？
    - **FPR (False Positive Rate)**: $\frac{FP}{FP + TN}$。所有健康人里你误判了多少？

4.  **ROC 曲线 (Receiver Operating Characteristic Curve)**:
    - 横轴：FPR；纵轴：TPR。
    - 对角线 $(0,0) \to (1,1)$ 代表**随机猜测**（Random Guess）。
    - **好的分类器**: 曲线越靠近**左上角** $(0,1)$ 越好（即 TPR 高且 FPR 低）。
    - 通过调整分类阈值（Threshold），可以在 TPR 和 FPR 之间做权衡，形成 ROC 曲线。

### 总结

这份 Slide 完整地介绍了逻辑回归的闭环：

1.  **建模**: 用 sigmoid/logistic loss 替代 0/1 loss。
2.  **训练**: 推导梯度，使用 SGD 或 Momentum SGD 进行高效训练。
3.  **评估**: 不仅仅看准确率，要关注 ROC 曲线，特别是在样本不平衡的场景下。

---

这份文档名为《Logistic Regression: Model, Algorithms, and Implementation》（逻辑回归：模型、算法与实现），作者是 Shusen Wang (Stevens Institute of Technology)。 它不仅仅是一篇关于逻辑回归的介绍，更像是一个完整的机器学习微型课程，涵盖了从数据处理、数学建模、数值优化算法到模型评估和超参数调优的全流程。

以下是因为文档内容的深度分析与解构：

### 1. 核心架构解构

文档按照机器学习的标准工作流组织，逻辑非常清晰：
**数据 (Data) $\rightarrow$ 模型 (Model) $\rightarrow$ 训练/优化 (Training) $\rightarrow$ 预测 (Prediction) $\rightarrow$ 评估与调优 (Evaluation & Tuning)**

### 2. 深度分析：各模块详解

#### A. 数据处理 (Data)

- **输入定义**: 特征向量 $x \in \mathbb{R}^d$，标签 $y \in \{+1, -1\}$。注意这里特别强调如果原始标签是 $\{0, 1\}$，需要转换为 $\{-1, +1\}$ 以适配后续的数学推导。
- **特征缩放 (Feature Scaling)**: 文档详细介绍了两种常见方法，并提供了 Python (`numpy`) 实现代码：
  1.  **归一化 (Min-max Normalization)**: 将特征缩放到 $[0, 1]$ 区间。
      $$ x' = \frac{x - \min(x)}{\max(x) - \min(x)} $$
  2.  **标准化 (Standardization)**: 将特征转换为均值为 0，方差为 1 的分布。
      $$ x' = \frac{x - \text{mean}(x)}{\text{std}(x)} $$
- **工程实践**: 极其重要的一点是，**测试集必须使用训练集的统计量（均值、方差、极值）进行缩放**，严禁在测试集上重新计算统计量，以防止信息泄露。

#### B. 模型构建 (Model)

- **目标函数 (Objective Function)**:
  采用了带有 **L2 正则化** 的逻辑回归模型。
  $$ \min*w Q(w; X, y) = \frac{1}{n} \sum*{i=1}^n \log(1 + \exp(-y_i w^T x_i)) + \frac{\lambda}{2} \|w\|\_2^2 $$
  - 第一部分是损失函数（Loss Function），用于衡量预测误差。
  - 第二部分是正则化项（Regularization），$\lambda$ 是超参数，用于防止过拟合。
- **推导逻辑 (Derivations)**:
  文档对比了两种推导视角（尽管最终专注于 Variant 2）：
  - **Variant 1 ($y \in \{0,1\}$)**: 基于 Sigmoid 函数 $f(x) = \frac{1}{1+e^{-w^T x}}$ 和二元交叉熵 (Binary Cross-Entropy)。这是深度学习中常见的形式。
  - **Variant 2 ($y \in \{-1,1\}$)**: 令 $\tilde{x} = yx$，试图让 $yx^T w$ 尽可能大（即 $y$ 与 $w^Tx$ 同号）。这推导出了上述公式 (2.1)。

#### C. 数值优化 (Numerical Optimization) - **文档的核心**

文档不仅给出了公式，还提供了 `gradient` 计算和完整的 `grad_descent`、`sgd` Python 代码。

1.  **梯度 (Gradient)**:
    利用链式法则推导出了目标函数 $Q$ 关于 $w$ 的梯度：
    $$ \nabla Q(w) = \frac{1}{n} \sum\_{i=1}^n \frac{-y_i x_i}{1 + \exp(y_i w^T x_i)} + \lambda w $$

2.  **算法演进**:

    - **梯度下降 (GD)**: 使用全量数据计算梯度。每一步最准，但计算量大 ($O(nd)$)。
      $$ w \leftarrow w - \alpha \nabla Q(w) $$
    - **随机梯度下降 (SGD)**: 每次随机抽取一个样本 $(x_i, y_i)$ 更新。虽然梯度有噪声，但期望是无偏估计。计算极快 ($O(d)$)。
      $$ w \leftarrow w - \alpha \nabla Q_i(w) $$
        *注意*: 文档中 SGD 的实现包含了一个细节——每个 Epoch 对数据进行 Shuffle（洗牌），然后按顺序遍历，这比纯随机采样效果更好。且 SGD 需要随时间衰减学习率 $\alpha$。
    - **小批量 SGD (Mini-batch SGD)**: 折中方案，每次采样 $b$ 个样本。兼顾了梯度的稳定性和计算效率，是深度学习的标准配置。
    - **动量加速 (Momentum)**: 针对病态（ill-conditioned）问题（即条件数 $\kappa$ 很大，导致优化路径呈 "之" 字形），引入动量项 $z$ 来加速收敛。

3.  **算法比较的陷阱**:
    - 不要用“运行时间”做横轴（受编程语言实现影响大）。
    - 不要用“迭代次数 (Iterations)”做横轴（GD 一次迭代是全量，SGD 一次迭代是单样本，不可比）。
    - **正确做法**: 使用 **Epochs**（遍历一次所有数据）作为横轴来对比 Objective Value 的下降曲线。

#### D. 评估与调优 (Evaluation & Tuning)

- **预测**: $f(x') = \text{sgn}(w^T x')$。
- **评估指标**:
  - **类别平衡**: 准确率 (Accuracy)、错误率 (Error Rate)。
  - **类别不平衡 (Class-imbalanced)**: 准确率失效。应使用 Precision, Recall, F-measure, ROC 曲线, AUC。
- **交叉验证 (Cross-Validation)**:
  - 区分了**模型参数 ($w$)** 和 **超参数 ($\lambda, \alpha, \beta, b, T$)**。
  - 严禁使用测试集调参。
  - 介绍了 **k-fold Cross-Validation**：通过多次划分训练/验证集来选择最佳超参数 $\lambda$，虽然计算成本高（需训练 $k \times c$ 次），但是最稳健的方法。

### 3. 文档价值总结

这份文档是一份非常优秀的**工程导向型理论教程**。它的特点在于：

1.  **理论与代码结合**: 每个数学公式后几乎紧跟 Python (`numpy`) 实现，消除了从公式到代码的鸿沟。
2.  **细节考究**: 提到了很多教科书容易忽略的工程细节（如 Scaling 时测试集的处理、SGD 的 Shuffle 和学习率衰减、算法比较的正确画图姿势）。
3.  **练习题设计**: 文档末尾留了 6 个问题（如推导 Hessian 矩阵、实现动量 SGD），引导读者深入思考。

### 4. 关键 Python 代码片段解构 (示例)

**目标函数计算**体现了向量化编程思想：

```python
def objective(w, x, y, lam):
    # 利用矩阵乘法一次性计算所有样本的线性部分
    yx = numpy.multiply(y, x)
    yxw = numpy.dot(yx, w)
    # 向量化计算 logistic loss
    vec1 = numpy.exp(-yxw)
    vec2 = numpy.log(1 + vec1)
    loss = numpy.mean(vec2) # 均值
    reg = lam / 2 * numpy.sum(w * w) # 正则项
    return loss + reg
```

这段代码避免了显式的 `for` 循环，利用 `numpy` 的广播和矩阵运算机制，极大提高了执行效率。

---

这份 PPT 是王树森教授“深度学习”课程关于**分类（Classification）**的第二部分，核心主题是 **支持向量机（Support Vector Machine, SVM）**。

SVM 是机器学习中最经典的算法之一，这份 Slide 非常清晰地从几何角度推导了 SVM 的原理，从线性可分（Hard Margin）过渡到线性不可分（Soft Margin / Hinge Loss）。

以下是详细的内容讲解：

### 1. 几何基础：点到超平面的距离 (Slides 2-6)

这是理解 SVM “间隔（Margin）”概念的数学基础。

- **问题**: 给定一个点 $\mathbf{z}$ 和一个超平面 $\mathbf{w}^T\mathbf{x} + b = 0$，如何求点到平面的距离？
- **推导**:
  - 这就是一个约束优化问题：最小化 $\|\mathbf{z} - \mathbf{x}\|^2$，约束条件是 $\mathbf{x}$ 在平面上。
  - 利用拉格朗日乘子法（KKT 条件）解得投影点 $\mathbf{x}^*$。
- **结论公式**:
  - 点 $\mathbf{z}$ 到超平面的垂直距离为：
    $$ \text{distance} = \frac{|\mathbf{w}^T\mathbf{z} + b|}{\|\mathbf{w}\|\_2} $$
  - 这个公式告诉我们要最大化距离（Margin），可以通过减小分母 $\|\mathbf{w}\|_2$ 来实现（假设分子被归一化固定）。

### 2. 硬间隔 SVM (Hard Margin SVM) (Slides 7-18)

这部分讨论当数据**线性可分**（完全可以用一条直线分开）的情况。

- **直观思想**:
  - 能把数据分开的超平面有无数个，选哪一个？
  - 选最“安全”的那个，也就是离最近的数据点越远越好。这个最近的距离叫做 **Margin**。
- **数学建模**:
  1.  **定义 Margin**: 所有数据点 $(y_i, \mathbf{x}_i)$ 到超平面的最小距离。
      $$ \text{Margin} = \min_i \frac{|y_i(\mathbf{w}^T\mathbf{x}\_i + b)|}{\|\mathbf{w}\|\_2} $$
  2.  **缩放技巧 (Canonical Hyperplane)**:
      - $\mathbf{w}$ 和 $b$ 同时放大缩小不改变超平面位置。
      - 为了简化计算，我们将 $\mathbf{w}, b$ 缩放，使得最近点的函数距离 $|y_i(\mathbf{w}^T\mathbf{x}_i + b)| = 1$。
      - 此时，$\text{Margin} = \frac{1}{\|\mathbf{w}\|_2}$。
  3.  **优化目标**:
      - 最大化 Margin $\iff$ 最大化 $\frac{1}{\|\mathbf{w}\|_2}$ $\iff$ **最小化 $\|\mathbf{w}\|_2^2$**。
      - **约束条件**: 所有点都要被正确分类且在 Margin 之外，即 $y_i (\mathbf{w}^T\mathbf{x}_i + b) \geq 1$。

### 3. 软间隔 SVM (Soft Margin SVM) (Slides 19-23)

现实世界的数据往往是**不可分**的（有噪声或重叠）。

- **问题**: 如果数据不可分，上述约束 $y_i (\mathbf{w}^T\mathbf{x}_i + b) \geq 1$ 无法对所有点成立，优化问题无解。
- **解决方案 (Relaxation)**:
  - 允许某些点“犯错”，即允许它们跑到 Margin 里面，甚至跑到错误的一侧。
  - 把这些错误量化为损失，加到目标函数里去。
- **引入松弛变量**:
  - 原来的约束是 $1 - y_i(\mathbf{w}^T\mathbf{x}_i + b) \leq 0$（完美分类）。
  - 现在我们看 $1 - y_i(\mathbf{w}^T\mathbf{x}_i + b)$ 这个值：
    - 如果 $\le 0$，说明分类正确且在间隔外，损失为 0。
    - 如果 $> 0$，说明犯错了（在间隔内或分错），这个正数值就是**Hinge Loss**。
- **最终目标函数**:
  $$ \min*{\mathbf{w}, b} \underbrace{\|\mathbf{w}\|^2}*{\text{Regularization}} + \lambda \sum*{i=1}^n \underbrace{\max(0, 1 - y_i(\mathbf{w}^T\mathbf{x}\_i + b))}*{\text{Hinge Loss}} $$
  - 第一项：希望 Margin 尽可能大（模型更简单，防止过拟合）。
  - 第二项：希望分类错误尽可能少。
  - $\lambda$：权衡两者的超参数。

### 4. 损失函数对比 (Slides 24-27)

最后，PPT 将 SVM 放在广义线性模型的框架下进行了对比。

- **三种 Loss**:

  1.  **Hinge Loss (SVM)**: $g(z) = \max(0, 1-z)$。
      - 特点：是凸函数（Convex），但**不光滑**（在 $z=1$ 处不可导）。
  2.  **Logistic Loss (Logistic Regression)**: $l(z) = \log(1 + e^{-z})$。
      - 特点：是凸函数，且**光滑**（处处可导）。
  3.  **Hard Thresholding (0/1 Loss)**:
      - 特点：非凸，不连续，难以优化（NP 難）。

- **结论**:
  - SVM 和 逻辑回归（LR）在数学本质上非常相似，都是添加了正则项的线性分类器，只是 Loss Function 不同。
  - **优化难度**: LR 的 Loss 光滑，梯度下降（GD）收敛更快（线性收敛）；SVM 的 Hinge Loss 不光滑，需要次梯度下降（Sub-gradient Descent），收敛较慢（亚线性收敛）。
  - **稀疏性**: 虽然 PPT 没细说，但 SVM 的 Hinge Loss 有个特点，对于分类“足够好”（$y \mathbf{w}^T\mathbf{x} > 1$）的点，损失完全为 0，梯度也为 0。这意味着 SVM 的解由这里“犯错”或“刚好在边界上”的少数点（支持向量）决定，而 LR 的解受所有点影响。

### 总结

这一节的核心逻辑链是：
**几何距离 $\to$ 最大化间隔 (Hard Margin) $\to$ 处理不可分数据 (Soft Margin) $\to$ 导出 Hinge Loss $\to$ 与 Logistic Regression 对比。**

---

这份 PPT 是王树森教授“深度学习”课程关于**分类（Classification）**的第三部分，核心主题是**多类分类（Multi-Class Classification）**，特别是**Softmax 分类器（Softmax Classifier）**。

以下是对这份 Slide 的深入讲解：

### 1. 为什么不能用线性回归做分类？ (Slides 5-8)

PPT 首先通过手写数字识别的例子（识别 3, 5, 8）反驳了使用线性回归（Linear Regression）进行分类的可行性。

- **问题**：如果我们强行定义标签 $y$ 为数字的值（如 3, 5, 8），线性回归试图拟合 $f(x) \approx y$。
- **矛盾**：
  - 假设模型训练得很好，看到“3”输出 3，看到“8”输出 8。
  - 那么对于一个长得像“3”和“8”中间状态的数字（实际上可能是“5”），模型会输出 $(3+8)/2 = 5.5$ 左右的值。
  - 但这在分类任务中没有意义，数字之间没有这种数值上的连续过度关系。
- **结论**：分类问题需要输出的是**属于每个类别的“概率”或“分数”**，而不是一个回归值。

### 2. 核心组件：Softmax 与 Cross-Entropy (Slides 9-13)

为了构建多类分类器，引入了三个数学工具：

1.  **One-Hot Encoding (独热编码)**:
    - 将离散的类别标签转换为向量。例如 $K=10$ 类，标签 $y=3$ 对应的向量是 $[0, 0, 0, 1, 0, \dots, 0]^T$。
2.  **Softmax Function**:
    - 将线性计算得到的“分数”向量 $\boldsymbol{\phi}$ 转换为“概率”分布 $\mathbf{p}$。
    - 公式：$p_k = \frac{e^{\phi_k}}{\sum_{j=1}^K e^{\phi_j}}$
    - 作用：保证所有 $p_k > 0$ 且 $\sum p_k = 1$。
3.  **Cross-Entropy Loss (交叉熵损失)**:
    - 衡量两个概率分布（真实标签 $\mathbf{y}$ 和预测概率 $\mathbf{p}$）之间的距离。
    - 公式：$H(\mathbf{y}, \mathbf{p}) = - \sum_{k=1}^K y_k \log p_k$。
    - 由于 $\mathbf{y}$ 是 One-Hot 向量，实际上 $H(\mathbf{y}, \mathbf{p}) = -\log(p_{\text{target}})$，即只关心正确类别的预测概率是否足够大。

### 3. Softmax 分类器模型 (Slides 14-22)

这是整个 PPT 的核心模型部分。

- **输入**: 特征向量 $\mathbf{x} \in \mathbb{R}^d$。
- **参数**: 权重矩阵 $\mathbf{W} \in \mathbb{R}^{K \times d}$（$K$ 是类别数，$d$ 是特征维度）。
- **前向传播过程**:
  1.  **线性变换**: $\boldsymbol{\phi} = \mathbf{W}\mathbf{x}$。这一步给每个类别打分。
  2.  **概率归一化**: $\mathbf{p} = \text{Softmax}(\boldsymbol{\phi})$。
  3.  **计算损失**: Loss $= H(\mathbf{y}, \mathbf{p})$。
- **优化目标**: 最小化所有样本的交叉熵损失之和。

### 4. 梯度推导与 SGD (Slides 23-42)

这部分详细推导了如何更新参数 $\mathbf{W}$。这是实现 Softmax 回归最关键的数学步骤。

- **目标**: 求 $\frac{\partial \text{Loss}}{\partial \mathbf{W}}$。
- **推导结果**: 这里的公式非常简洁优雅：
  $$ \frac{\partial \text{Loss}}{\partial \mathbf{W}} = (\mathbf{p} - \mathbf{y}) \mathbf{x}^T $$
  - $\mathbf{p} - \mathbf{y}$：预测概率与真实标签的误差（Shape: $K \times 1$）。
  - $\mathbf{x}^T$：输入特征（Shape: $1 \times d$）。
  - 结果矩阵 Shape：$K \times d$，与 $\mathbf{W}$ 相同。
- **直观理解**: 如果预测 $\mathbf{p}$ 和真实 $\mathbf{y}$ 很接近，误差接近 0，梯度就很小，参数就不怎么更新；如果误差很大，梯度就大，参数通过 SGD（随机梯度下降）迅速调整。

### 5. 推理与局限性 (Slides 43-49)

- **测试/推理 (Test)**:
  - 训练好 $\mathbf{W}$ 后，来一个新样本 $\mathbf{x}'$。
  - 计算 $\boldsymbol{\phi} = \mathbf{W}\mathbf{x}'$。
  - 不需要算 Softmax，直接看 $\boldsymbol{\phi}$ 哪个元素最大，就是哪个类别（因为 $e^x$ 是单调增函数）。
- **局限性 (Limitations)**:
  - **参数量爆炸**: 参数数量 = 特征维数 $\times$ 类别数 ($d \times K$)。
  - **例子**:
    - 数字识别 ($K=10$): 参数很少，没问题。
    - ImageNet ($K=1000$): 参数约 1M，还能接受。
    - 人脸识别 ($K=1,000,000$甚至更多): 参数量达到 10 亿级别 (1G floats = 4GB 显存仅存权重)，计算和存储成本过高。
  - 这也引出了后续课程可能涉及的主题：如何处理超大规模分类（例如使用 Embedding 或 Hierarchical Softmax 等，虽然此 PPT 未涉及）。

### 总结

这份 PPT 完整地构建了 **Softmax Regression (Multinomial Logistic Regression)** 的理论框架：从**线性打分**到**Softmax 概率**，再到**交叉熵损失**，最后通过**梯度下降**求解。这是深度学习中处理分类任务的最基础模块。

---

这份 PPT 是王树森教授深度学习课程关于**分类（Classification）**的第四部分，主要讲解了**最近邻方法（Nearest Neighbor Methods）**，特别是 **K-近邻算法（K-Nearest Neighbor, KNN）**。

与前面的 Softmax 分类器不同，KNN 是一种**非参数（Non-parametric）**或**基于实例（Instance-based）**的学习方法。

以下是对这份 Slide 的深入讲解：

### 1. K-近邻分类器 (KNN) 的核心思想 (Slides 3-6)

- **基本原理**：给定一个测试样本 $\mathbf{x}$，在训练集中找到与它最相似（距离最近）的 $k$ 个样本。让这 $k$ 个样本“投票”来决定 $\mathbf{x}$ 的类别。
- **直观理解**：“物以类聚，人以群分”。如果你周围的 $k$ 个邻居大多是某一类，那你大概率也属于这一类。
- **$k$ 的选择**：
  - $k$ 是一个**超参数（Hyper-parameter）**。
  - 不能通过梯度下降学习，通常需要通过**交叉验证（Cross-Validation）**来选取最优的 $k$ 值。

### 2. 核心组件：相似度与投票 (Slides 7-10)

KNN 的效果高度依赖于两个设计选择：

#### A. 如何衡量相似度 (Similarity Measure)?

PPT 列举了三种常用的衡量方式：

1.  **余弦相似度 (Cosine Similarity)**:
    - $\text{sim}(\mathbf{x}, \mathbf{x}_i) = \frac{\mathbf{x}^T \mathbf{x}_i}{\|\mathbf{x}\| \|\mathbf{x}_i\|}$
    - 主要衡量向量方向的一致性，常用于文本或归一化后的特征。
2.  **高斯核 (Gaussian Kernel)**:
    - $\text{sim}(\mathbf{x}, \mathbf{x}_i) = \exp(-\|\mathbf{x} - \mathbf{x}_i\|_2^2)$
    - 基于欧几里得距离，距离越近，分数越高（指数级衰减）。
3.  **拉普拉斯核 (Laplacian Kernel)**:
    - $\text{sim}(\mathbf{x}, \mathbf{x}_i) = \exp(-\|\mathbf{x} - \mathbf{x}_i\|_1)$
    - 基于曼哈顿距离（L1 距离）。

#### B. 如何投票 (Voting)?

1.  **均匀权重 (Uniform Weight)**:
    - 前 $k$ 个邻居，每人一票，票数最多的类别胜出。
2.  **加权投票 (Weighted Voting)**:
    - 距离越近的邻居权重越大。
    - 例如：$\text{Weight} = \text{sim}(\mathbf{x}, \mathbf{x}_i)$。这样可以缓解 $k$ 值较大的时候，远处的邻居干扰决策的问题。

### 3. 计算复杂度与优化 (Slides 12-22)

这是 KNN 在实际应用中最大的瓶颈。

- **朴素算法 (Naïve Algorithm)**:

  - **训练**: 不需要训练（Lazy Learning）。
  - **测试/推理**: 对于每一个查询样本，都要计算它与所有 $n$ 个训练样本的距离，然后排序。
  - **复杂度**: $O(nd)$。当样本量 $n$ 很大（如数百万）或特征维度 $d$ 很高时，速度非常慢，不可接受。

- **高效算法 (Efficient Algorithms)**:
  为了解决速度问题，PPT 引入了 **向量量化 (Vector Quantization)** 的思想：
  - **类比**: 你要找最近的邮局，不需要在这个国家所有 30,000 个邮局里逐一计算距离。你只需要在同一个城市或同一个区的几个邮局里找。
  - **预处理 (Training Phase)**:
    1.  构建“地标 (Landmarks)”（可以通过聚类如 K-Means 得到）。
    2.  将所有训练样本分配到最近的地标下（将空间划分区域，类似 Voronoi 图）。
  - **查询 (Test Phase)**:
    1.  先比较查询点与各个地标的距离，找到最近的几个地标。
    2.  只在这些地标所管辖的样本中进行精确的 KNN 搜索。
  - **其他方法**: PPT 还提到了 **KD-Tree** 和 **LSH (局部敏感哈希)** 作为加速手段。

### 4. KNN 与 Softmax 分类器的对比 (Slide 23)

这是一个非常重要的总结，解释了为什么深度学习有了 Softmax 还需要 KNN。

- **Softmax 分类器**:
  - 需要学习一个权重矩阵 $W \in \mathbb{R}^{d \times K}$。
  - 当类别数 $K$ 非常巨大时（例如人脸识别，$K$ 可能有数百万个 ID），参数量爆炸，Softmax 计算分母归一化时也非常慢。
- **KNN 分类器**:
  - 不需要维护一个固定的类别权重矩阵。
  - 非常适合 **海量类别 (Extreme Classification)** 场景。
  - 例如：人脸识别系统。注册新用户时，只需要把新用户的人脸特征存入数据库（增加样本），不需要重新训练整个网络。

### 总结

这份 PPT 讲解了 KNN 从理论到工程实现的路径。虽然 KNN 原理简单，但在大规模系统中，**如何快速检索（近似最近邻搜索 ANN）**才是核心挑战。在深度学习时代，KNN 常与深度特征提取结合使用（Deep Metric Learning），即：用神经网络提取特征，用 KNN 进行检索分类。

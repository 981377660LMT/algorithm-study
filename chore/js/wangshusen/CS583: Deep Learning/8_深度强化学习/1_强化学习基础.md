# 强化学习基础

https://www.bilibili.com/video/BV12o4y197US

这份 PDF 是 Shusen Wang 教授关于 **深度强化学习（Deep Reinforcement Learning, DRL）** 系列课程的第 13 讲第一部分，主题为 **《Reinforcement Learning Basics》（强化学习基础）**。

这份讲义非常系统地构建了强化学习的底层逻辑，从最基础的概率论回顾，到 RL 的核心术语，再到价值函数的数学定义。以下是对这份材料的深入解构与解读：

### 1. 理论基石：概率论回顾 (Probability Theory Review)

在进入 RL 之前，讲义先铺垫了必要的数学基础，因为 **RL 本质上是对随机过程的建模与控制**。

- **随机变量 (Random Variable)**：强调了我们面对的“状态”和“动作”本质上都是随机变量。
  - **大写 $X$** 表示随机变量（未知的，待发生的）。
  - **小写 $x$** 表示观测值（已知的，已发生的）。
- **PDF vs PMF**：
  - **PDF (概率密度函数)**：用于连续变量（如高斯分布）。在 RL 中，对应的场景是**连续控制**（Continuous Control），例如控制机器人的关节角度或自动驾驶的方向盘角度。
  - **PMF (概率质量函数)**：用于离散变量。在 RL 中对应**离散控制**（Discrete Control），例如在超级马里奥中选择“左、右、跳”。
- **期望 (Expectation)**：这是 RL 所有的核心，因为我们无法预知确切的未来，只能追求“平均意义上”的最好结果（即期望最大化）。

---

### 2. 强化学习核心世界观 (Terminologies)

RL 的世界是由五个核心元素构成的元组。

#### (1) 核心组件

- **State ($s$)**：当前世界的样子（例如：马里奥游戏的一帧画面）。
- **Action ($a$)**：智能体能做的事情（例如：向左走、向右走、跳）。
- **Reward ($R$)**：环境给的反馈分数。
  - **正反馈**：吃到金币 (+1)，赢了游戏 (+10000)。
  - **负反馈**：碰到敌人 Goomba 挂了 (-10000)。
  - **零反馈**：什么都没发生 (0)。
  - _解读：Reward 是引导 Agent 行为的唯一信号。_

#### (2) 两个核心函数 (函数的角度看世界)

这部分是讲义的重点，明确了**谁控制什么**。

1.  **Policy Function (策略函数) $\pi(a|s)$**

    - **控制者**：Agent (智能体)。
    - **定义**：$\pi(a|s) = P(A=a | S=s)$
    - **含义**：在看到状态 $s$ 时，采取动作 $a$ 的概率。
    - **例子**：看到前方有坑（State），跳（Action）的概率是 0.9，直接走过去（Action）的概率是 0.1。这就是策略。

2.  **State Transition Function (状态转移函数) $p(s'|s, a)$**
    - **控制者**：Environment (环境)。
    - **定义**：$p(s'|s, a) = P(S'=s' | S=s, A=a)$
    - **含义**：在状态 $s$ 做出了动作 $a$，下一时刻变成状态 $s'$ 的概率。
    - _解读：这是物理法则或游戏规则。例如你按了“跳”，但地面湿滑，你可能跳起来了，也可能摔倒了。这是环境决定的，Agent 无法直接控制，只能适应。_

#### (3) 两个随机性来源 (Sources of Randomness)

这是一个非常深刻的见解，讲义特意将其区分开：

1.  **动作的随机性**：来自 **Actor (Agent)**。因为 Policy $\pi$ 通常是一个概率分布，即使面对同样的情况，Agent 也可能通过抽样做出不同的选择（Exploration, 探索）。
2.  **状态的随机性**：来自 **Environment**。即使你做出了同样的动作，环境的反馈（下一个状态）也可能不同。

---

### 3. 核心目标：回报与价值 (Rewards, Returns and Value)

RL 的目标不仅是拿当前的奖励，而是拿“未来的总奖励”。

#### (1) Return (回报 / 累积未来奖励) $U_t$

$$U_t = R_t + R_{t+1} + R_{t+2} + \dots$$
但讲义指出这里有个问题：现在的奖励比未来的奖励更值钱（类似于经济学中的通货膨胀或利息）。

#### (2) Discounted Return (折扣回报)

引入折扣因子 **$\gamma$ (Gamma)**，通常 $\gamma \in [0, 1]$。
$$U_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \dots$$

- **数学作用**：保证无穷级数收敛（如果游戏无限进行下去，总分不会变成无穷大）。
- **直观作用**：$\gamma$ 越小，Agent 越短视（只看眼前）；$\gamma$ 越大，Agent 越有远见。

#### (3) 随机性的传递

$$U_t \text{ 是随机变量}$$
因为 $U_t$ 依赖于未来的所有 $R$，而未来的 $R$ 依赖于未来的 $S$ 和 $A$。由于 $S$ 和 $A$ 都是随机的，所以 $U_t$ 也是随机的。

---

### 4. 价值函数 (Value Functions) —— RL 的灵魂

既然未来是随机的，我们怎么知道现在的状态好不好？答案是：**算期望 (Expectation)**。

#### (1) Action-Value Function (动作价值函数) $Q_\pi(s, a)$

- **定义**：$Q_\pi(s, a) = \mathbb{E}[U_t | S_t=s, A_t=a]$
- **解读**：假设我现在处于状态 $s$，并且**非要**做动作 $a$（哪怕这个动作很蠢），然后从此以后都按策略 $\pi$ 玩下去，我平均能拿多少分？
- **作用**：$Q$ 函数直接指导 Agent 选动作。哪个 $a$ 对应的 $Q$ 值大，就选哪个。

#### (2) State-Value Function (状态价值函数) $V_\pi(s)$

- **定义**：$V_\pi(s) = \mathbb{E}_{A \sim \pi}[Q_\pi(s, A)]$
- **数学关系**：$V_\pi(s) = \sum_{a} \pi(a|s) \cdot Q_\pi(s, a)$
- **解读**：$V$ 函数是对 $Q$ 函数关于动作分布的加权平均。它评价的是**当前局势本身好不好**。
- **例子**：
  - 围棋棋盘上现在的局势，$V(s)$ 很高，说明胜率很大。
  - 具体的某一步棋，$Q(s, a)$ 很高，说明这步棋是妙手。

---

### 5. 交互流程图解 (Agent-Environment Interaction Loop)

讲义最后用图示总结了 RL 的标准工作流：

1.  **观察**：Agent 看到 State $s_t$。
2.  **决策**：Agent 根据 Policy $\pi$ 采样出 Action $a_t$。
3.  **执行**：Agent 执行 $a_t$。
4.  **反馈**：Environment 根据状态转移 $p$，返回新的 State $s_{t+1}$ 和 Reward $r_t$。
5.  **循环**：时间推移到 $t+1$，重复上述过程。

### 总结

这份讲义清晰地定义了强化学习的数学框架：**马尔可夫决策过程 (MDP)**（虽然讲义中未显式使用此术语，但内容即是 MDP）。

- **世界观**：Agent 与 Environment 交互。
- **不确定性**：由 $\pi$（策略）和 $p$（环境）共同产生的随机性。
- **目标**：最大化 Discounted Return 的期望。
- **工具**：利用 $Q(s, a)$ 和 $V(s)$ 函数来评价策略的好坏，从而改进策略。

---

这节课是王树森老师深度强化学习（Deep Reinforcement Learning, DRL）系列的**第一课**。这节课至关重要，因为它铺设了后续所有算法（DQN, Policy Gradient, Actor-Critic）的基石——**基本概念与术语**。强化学习难入门的主要原因就是概念多且容易混淆。

以下是对这节课的深度分析与逻辑梳理：

---

### 第一部分：数学基础回顾（概率论）

在进入 RL 之前，王老师首先复习了概率论，这是描述不确定性的语言。

1.  **随机变量 (Random Variable, $X$) vs. 观测值 (Observation, $x$)**

    - **大写 $X$**：未知的、随机的量（例如：硬币还没扔，结果未知）。
    - **小写 $x$**：已知的、确定的数值（例如：硬币扔完了，是正面）。
    - _RL 中的映射_：在游戏每一帧结束前，动作和下一帧状态都是 **随机变量**；一旦发生，就记录为 **观测值**。

2.  **概率密度函数 (PDF) 与 期望 (Expectation)**

    - **PDF**：描述随机变量在某点取值的可能性。离散分布求和=1，连续分布积分=1。
    - **期望 ($E$)**：加权平均值。$E[f(x)] = \sum p(x)f(x)$ 或 $\int p(x)f(x)dx$。
    - _RL 中的映射_：价值函数（Value Function）本质上就是一种 **数学期望**。

3.  **随机抽样 (Random Sampling)**
    - 根据概率分布随机选择一个样本。
    - _例子_：摸彩球。
    - _应用_：Agent 选择动作、环境生成下一个状态，本质上都在做随机抽样。

---

### 第二部分：核心概念 (The Cast)

这是强化学习舞台上的“角色”和“道具”。

#### 1. 基础要素

- **Agent (智能体)**：动作的发出者（如：马里奥、自动驾驶汽车）。
- **Environment (环境)**：与 Agent 交互的对象（如：游戏程序、物理世界）。
- **State (状态, $s$)**：Agent 看到的当前局势（如：屏幕画面）。
- **Action (动作, $a$)**：Agent 做出的行为（如：向左走、跳跃）。

#### 2. 两大核心函数 (Brain & Physics)

- **Policy (策略, $\pi$) —— Agent 的大脑**
  - 定义：$\pi(a|s)$，是一个概率密度函数。
  - 意义：在看到状态 $s$ 时，采取动作 $a$ 的概率。
  - 特征：可以是确定性的（总是出石头），也可以是随机性的（随机出拳）。**在 RL 中，通常保留随机性以应对博弈和探索**。
- **State Transition (状态转移, $P$) —— 环境的物理定律**
  - 定义：$p(s'|s, a)$，也是一个概率密度函数。
  - 意义：在状态 $s$ 做出动作 $a$ 后，环境变成新状态 $s'$ 的概率。
  - 特征：**我们（Agent）通常不知道这个函数**，只有环境知道。它是随机的（例如马里奥里小怪的移动）。

#### 3. 交互循环 (Interaction Loop)

1.  Agent 观测到状态 $s_t$。
2.  Agent 根据策略 $\pi$ **随机抽样** 出动作 $a_t$。
3.  Environment 根据转移函数 $P$ **随机抽样** 出新状态 $s_{t+1}$，并反馈奖励 $r_t$。
4.  循环继续...

**关键点：随机性的两个来源**

1.  **动作随机性**：来自 Agent 的策略 $\pi$（如果不确定）。
2.  **环境随机性**：来自状态转移 $P$。

---

### 第三部分：从奖励到价值 (从短视到远见)

这是本节课最难理解、也最重要的部分。概念层层递进：

#### 1. Reward (奖励, $R_t$)

- **定义**：某一步即时获得的反馈（如吃到金币+1，死掉-10000）。
- **局限**：只代表当下，不代表长远利益。

#### 2. Return (回报, $U_t$) —— 未来的累计奖励

- **定义**：从 $t$ 时刻开始，一直到游戏结束，所有未来奖励的总和。
- **折扣 (Discount Factor, $\gamma$)**：虽然都是奖励，但这节课强调了 **“现在的钱比未来的钱值钱”**。
  - 公式：$U_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \dots$
  - 意义：$\gamma \in [0, 1]$，$\gamma$ 越小，Agent 越短视；$\gamma$ 越接近 1，Agent 越有远见。
- **性质**：**它是随机变量**。在 $t$ 时刻，你还没玩完游戏，未来的奖励都没发生，所以 $U_t$ 是未知的。

#### 3. Action-Value Function (动作价值函数, $Q_\pi(s, a)$) —— 对 Return 的预估

- **痛点**：因为 Return $U_t$ 是随机的，我们做决策时不能直接用一个未知的随机变量。
- **解决**：**求期望**。把随机性（未来的动作、未来的状态）积分积掉。
- **定义**：$Q_\pi(s_t, a_t) = E[U_t | s_t, a_t]$。
- **直观意义**：如果我使用策略 $\pi$，在当前状态 $s$ 下，强制做一个动作 $a$，**平均**能拿多少分？
- **作用**：给“动作”打分。

#### 4. Optimal Action-Value Function (最优动作价值函数, $Q^*(s, a)$)

- **定义**：$Q^*(s, a) = \max_\pi Q_\pi(s, a)$。
- **意义**：不管策略怎么变，我想知道在状态 $s$ 做动作 $a$ **理论上能达到的最高分数**。
- **作用**：**这是 Value-based Learning 的核心目标**。如果我们知道了 $Q^*$，那我们在任何状态下，只要挑 $Q^*$ 值最大的那个动作做就可以了。

#### 5. State-Value Function (状态价值函数, $V_\pi(s)$)

- **定义**：$V_\pi(s) = E_a [Q_\pi(s, a)]$。
- **操作**：把动作 $a$ 也当成随机变量，再次求期望积分积掉。
- **直观意义**：单纯评价当前“局势”好不好（如围棋盘面胜率），不管下一步具体走哪。

---

### 第四部分：如何解决 RL 问题？

基于上述概念，王老师指出了两条技术路线（后续课程的伏笔）：

1.  **Policy-based Learning (策略学习)**

    - 直接学习 **策略函数 $\pi(a|s)$**。
    - 输入状态，输出动作概率。
    - 代表算法：Policy Gradient。

2.  **Value-based Learning (价值学习)**
    - 学习 **最优动作价值函数 $Q^*(s, a)$**。
    - 输入状态和动作，输出这个动作好不好（打分）。
    - 决策方式：$a = \arg\max_a Q^*(s, a)$（挑分最高的）。
    - 代表算法：DQN。

---

### 第五部分：工具与实践 (OpenAI Gym)

最后，介绍了强化学习的标准实验平台 **OpenAI Gym**。

- **环境类型**：
  - 经典控制 (CartPole, Pendulum)
  - Atari 游戏 (打砖块, 太空侵略者)
  - 连续控制 (MuJoCo, 机器人走路)
- **代码交互流程**：
  1.  `env = gym.make(...)`: 创建环境。
  2.  `env.reset()`: 重置环境，获取初始状态。
  3.  `loop`:
      - `agent` 选动作 `action`。
      - `env.step(action)`: 执行动作，返回 `state`, `reward`, `done`, `info`。
      - `env.render()`: 显示画面。
      - 如果 `done` 为 True，根据需要重置或退出。

---

### 总结要点

1.  **随机性无处不在**：RL 中充满了随机变量（状态、动作、回报），我们要用概率论（期望）来处理它们。
2.  **核心目标**：最大化 **期望回报 (Expected Return)**。
3.  **两个关键函数**：
    - **Policy $\pi$**：决定怎么做动作。
    - **Value Function ($Q, V$)**：评价动作或状态好不好。
4.  **学习方向**：要么学 $\pi$（让动作更聪明），要么学 $Q^*$（让评价更准）。

这节课搭建了 DRL 的骨架，下节课（DQN）将开始往骨架里填充具体的算法血肉。

# 强化学习基础

这份 PDF 是 Shusen Wang 教授关于 **深度强化学习（Deep Reinforcement Learning, DRL）** 系列课程的第 13 讲第一部分，主题为 **《Reinforcement Learning Basics》（强化学习基础）**。

这份讲义非常系统地构建了强化学习的底层逻辑，从最基础的概率论回顾，到 RL 的核心术语，再到价值函数的数学定义。以下是对这份材料的深入解构与解读：

### 1. 理论基石：概率论回顾 (Probability Theory Review)

在进入 RL 之前，讲义先铺垫了必要的数学基础，因为 **RL 本质上是对随机过程的建模与控制**。

- **随机变量 (Random Variable)**：强调了我们面对的“状态”和“动作”本质上都是随机变量。
  - **大写 $X$** 表示随机变量（未知的，待发生的）。
  - **小写 $x$** 表示观测值（已知的，已发生的）。
- **PDF vs PMF**：
  - **PDF (概率密度函数)**：用于连续变量（如高斯分布）。在 RL 中，对应的场景是**连续控制**（Continuous Control），例如控制机器人的关节角度或自动驾驶的方向盘角度。
  - **PMF (概率质量函数)**：用于离散变量。在 RL 中对应**离散控制**（Discrete Control），例如在超级马里奥中选择“左、右、跳”。
- **期望 (Expectation)**：这是 RL 所有的核心，因为我们无法预知确切的未来，只能追求“平均意义上”的最好结果（即期望最大化）。

---

### 2. 强化学习核心世界观 (Terminologies)

RL 的世界是由五个核心元素构成的元组。

#### (1) 核心组件

- **State ($s$)**：当前世界的样子（例如：马里奥游戏的一帧画面）。
- **Action ($a$)**：智能体能做的事情（例如：向左走、向右走、跳）。
- **Reward ($R$)**：环境给的反馈分数。
  - **正反馈**：吃到金币 (+1)，赢了游戏 (+10000)。
  - **负反馈**：碰到敌人 Goomba 挂了 (-10000)。
  - **零反馈**：什么都没发生 (0)。
  - _解读：Reward 是引导 Agent 行为的唯一信号。_

#### (2) 两个核心函数 (函数的角度看世界)

这部分是讲义的重点，明确了**谁控制什么**。

1.  **Policy Function (策略函数) $\pi(a|s)$**

    - **控制者**：Agent (智能体)。
    - **定义**：$\pi(a|s) = P(A=a | S=s)$
    - **含义**：在看到状态 $s$ 时，采取动作 $a$ 的概率。
    - **例子**：看到前方有坑（State），跳（Action）的概率是 0.9，直接走过去（Action）的概率是 0.1。这就是策略。

2.  **State Transition Function (状态转移函数) $p(s'|s, a)$**
    - **控制者**：Environment (环境)。
    - **定义**：$p(s'|s, a) = P(S'=s' | S=s, A=a)$
    - **含义**：在状态 $s$ 做出了动作 $a$，下一时刻变成状态 $s'$ 的概率。
    - _解读：这是物理法则或游戏规则。例如你按了“跳”，但地面湿滑，你可能跳起来了，也可能摔倒了。这是环境决定的，Agent 无法直接控制，只能适应。_

#### (3) 两个随机性来源 (Sources of Randomness)

这是一个非常深刻的见解，讲义特意将其区分开：

1.  **动作的随机性**：来自 **Actor (Agent)**。因为 Policy $\pi$ 通常是一个概率分布，即使面对同样的情况，Agent 也可能通过抽样做出不同的选择（Exploration, 探索）。
2.  **状态的随机性**：来自 **Environment**。即使你做出了同样的动作，环境的反馈（下一个状态）也可能不同。

---

### 3. 核心目标：回报与价值 (Rewards, Returns and Value)

RL 的目标不仅是拿当前的奖励，而是拿“未来的总奖励”。

#### (1) Return (回报 / 累积未来奖励) $U_t$

$$U_t = R_t + R_{t+1} + R_{t+2} + \dots$$
但讲义指出这里有个问题：现在的奖励比未来的奖励更值钱（类似于经济学中的通货膨胀或利息）。

#### (2) Discounted Return (折扣回报)

引入折扣因子 **$\gamma$ (Gamma)**，通常 $\gamma \in [0, 1]$。
$$U_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \dots$$

- **数学作用**：保证无穷级数收敛（如果游戏无限进行下去，总分不会变成无穷大）。
- **直观作用**：$\gamma$ 越小，Agent 越短视（只看眼前）；$\gamma$ 越大，Agent 越有远见。

#### (3) 随机性的传递

$$U_t \text{ 是随机变量}$$
因为 $U_t$ 依赖于未来的所有 $R$，而未来的 $R$ 依赖于未来的 $S$ 和 $A$。由于 $S$ 和 $A$ 都是随机的，所以 $U_t$ 也是随机的。

---

### 4. 价值函数 (Value Functions) —— RL 的灵魂

既然未来是随机的，我们怎么知道现在的状态好不好？答案是：**算期望 (Expectation)**。

#### (1) Action-Value Function (动作价值函数) $Q_\pi(s, a)$

- **定义**：$Q_\pi(s, a) = \mathbb{E}[U_t | S_t=s, A_t=a]$
- **解读**：假设我现在处于状态 $s$，并且**非要**做动作 $a$（哪怕这个动作很蠢），然后从此以后都按策略 $\pi$ 玩下去，我平均能拿多少分？
- **作用**：$Q$ 函数直接指导 Agent 选动作。哪个 $a$ 对应的 $Q$ 值大，就选哪个。

#### (2) State-Value Function (状态价值函数) $V_\pi(s)$

- **定义**：$V_\pi(s) = \mathbb{E}_{A \sim \pi}[Q_\pi(s, A)]$
- **数学关系**：$V_\pi(s) = \sum_{a} \pi(a|s) \cdot Q_\pi(s, a)$
- **解读**：$V$ 函数是对 $Q$ 函数关于动作分布的加权平均。它评价的是**当前局势本身好不好**。
- **例子**：
  - 围棋棋盘上现在的局势，$V(s)$ 很高，说明胜率很大。
  - 具体的某一步棋，$Q(s, a)$ 很高，说明这步棋是妙手。

---

### 5. 交互流程图解 (Agent-Environment Interaction Loop)

讲义最后用图示总结了 RL 的标准工作流：

1.  **观察**：Agent 看到 State $s_t$。
2.  **决策**：Agent 根据 Policy $\pi$ 采样出 Action $a_t$。
3.  **执行**：Agent 执行 $a_t$。
4.  **反馈**：Environment 根据状态转移 $p$，返回新的 State $s_{t+1}$ 和 Reward $r_t$。
5.  **循环**：时间推移到 $t+1$，重复上述过程。

### 总结

这份讲义清晰地定义了强化学习的数学框架：**马尔可夫决策过程 (MDP)**（虽然讲义中未显式使用此术语，但内容即是 MDP）。

- **世界观**：Agent 与 Environment 交互。
- **不确定性**：由 $\pi$（策略）和 $p$（环境）共同产生的随机性。
- **目标**：最大化 Discounted Return 的期望。
- **工具**：利用 $Q(s, a)$ 和 $V(s)$ 函数来评价策略的好坏，从而改进策略。

# 基于策略的学习

这节课是深度强化学习系列的第三课，王树森老师讲解了 **Policy-Based Learning（策略学习）**，核心内容是 **Policy Network（策略网络）** 以及训练它的 **Policy Gradient（策略梯度）算法**。

这节课的内容与上节课的 Deep Q-Network (DQN) 形成了鲜明的对比，是强化学习的另一大流派。

以下是对这节课的深度分析与逻辑重构：

---

### 第一部分：策略学习的目标与工具

#### 1. 回顾：Policy Function （策略函数）

- **定义**：$\pi(a|s)$，是一个概率密度函数。
- **输入**：状态 $s$（如游戏画面）。
- **输出**：动作 $a$ 的概率分布（如左:0.2, 右:0.1, 上:0.7）。
- **决策方式**：根据输出的概率分布进行**随机抽样**。

#### 2. 为什么要用神经网络？

- **查表法不可行**：状态空间太大（无穷多），没法把每个状态对应的策略记在表里。
- **Policy Network (策略网络)**：
  - 用一个带参数 $\theta$ 的神经网络 $\pi(a|s; \theta)$ 来近似真实的策略。
  - **结构**：输入层（状态） -> 卷积层/全连接层（特征提取） -> **Softmax 层**（输出概率分布，确保和为 1）。

---

### 第二部分：目标函数 —— 我们到底想优化什么？

在策略学习中，我们的目标很直接：**赢**。

1.  **State-Value Function ($V_\pi$) 的新视角**

    - 回顾：$V_\pi(s)$ 衡量了在当前状态 $s$ 下，使用策略 $\pi$ 的好坏（胜算）。
    - **目标函数 $J(\theta)$**：我们希望改进策略网络 $\theta$，使得总体的期望回报最大化。
      $$ J(\theta) = E_S [ V(s; \theta) ] $$
    - **直观意义**：让 $J(\theta)$ 越大越好，就是让 Agent 的平均胜算越大越好。

2.  **优化方法：梯度上升 (Gradient Ascent)**
    - 因为我们想求极大值，所以用梯度上升（区别于深度学习中常用的梯度下降求 Loss 极小值）。
    - **核心难题**：我们需要算出 $\nabla_\theta J(\theta)$，即 **Policy Gradient (策略梯度)**。

---

### 第三部分：Policy Gradient (策略梯度) 的推导

这部分是课程的数学核心，王老师通过简化的方式推导了梯度的计算公式。

#### 1. 原始形式

目标是求 $V$ 对 $\theta$ 的导数。经过（非严谨但直观的）推导：
$$ \nabla*\theta V \approx \sum_a \frac{\partial \pi(a|s; \theta)}{\partial \theta} \cdot Q*\pi(s, a) $$

- **含义**：改进策略的方向 = (策略本身的变化率) $\times$ (该动作的好坏 $Q$ 值)。

#### 2. Log-Likelihood Trick (对数技巧)

利用数学恒等式 $\frac{\partial \pi}{\partial \theta} = \pi \cdot \frac{\partial \log \pi}{\partial \theta}$，公式变形为：
$$ \nabla*\theta V = E*{A \sim \pi} [ \frac{\partial \log \pi(a|s; \theta)}{\partial \theta} \cdot Q_\pi(s, a) ] $$

- **形式二**：这是最常用的形式。它将连加/积分变为了**期望**。

#### 3. 蒙特卡洛近似 (Monte Carlo Approximation)

由于期望很难直接算（特别是动作是连续时积分积不出来），我们再次使用**随机抽样**。

- **操作**：
  1.  让 Agent 根据当前策略 $\pi$ 玩一步，随机抽样出一个动作 $\hat{a}$。
  2.  计算该样本的梯度：
      $$ g = \frac{\partial \log \pi(\hat{a}|s; \theta)}{\partial \theta} \cdot Q\_\pi(s, \hat{a}) $$
  3.  这个 $g$ 就是策略梯度的**无偏估计**。

---

### 第四部分：算法流程总结

一个标准的 Policy Gradient 迭代步骤如下：

1.  **观测**：获取当前状态 $s_t$。
2.  **抽样**：将 $s_t$ 输入策略网络，输出概率，随机抽样得到动作 $a_t$。
3.  **求导**：计算 $\nabla_\theta \log \pi(a_t|s_t; \theta)$。（PyTorch/TensorFlow 自动计算）。
4.  **估值**：计算 $q_t \approx Q_\pi(s_t, a_t)$。（这一步很关键，见下文）。
5.  **梯度**：计算近似策略梯度 $g = q_t \times \nabla_\theta \log \pi$。
6.  **更新**：$\theta_{new} = \theta_{old} + \beta \cdot g$ （梯度上升）。

#### 关键直觉：

- 如果动作 $a_t$ 的价值 $q_t$ 很**高**（比如是正数），梯度上升会让 $\log \pi(a_t)$ 变大 $\rightarrow$ **增加未来做该动作的概率**。
- 如果动作 $a_t$ 的价值 $q_t$ 很**低**（比如是负数），梯度上升会（因为乘了负数相当于梯度下降）让 $\log \pi(a_t)$ 变小 $\rightarrow$ **减少未来做该动作的概率**。
- **总结**：**好的动作多做，坏的动作少做**。

---

### 第五部分：那个缺失的拼图 —— Q 值怎么算？

公式里需要 $Q_\pi(s, a)$ 来评价动作好坏，但我们只知道策略网络，不知道价值函数。怎么办？

王老师提到了两种解决方案：

1.  **REINFORCE 算法 (Monte Carlo Policy Gradient)**

    - **做法**：硬着头皮把游戏玩到底，直到 Game Over。
    - **计算**：根据这一局实际拿到的总回报 $u_t$ 来代替 $Q_\pi$。
    - **缺点**：必须玩完一整局才能更新一次，效率低，方差大。

2.  **Actor-Critic 方法 (下节课预告)**
    - **做法**：再训练一个神经网络（Critic）来专门估计 $Q$ 值。
    - **结构**：
      - **Actor (演员)**：即策略网络 $\pi$，负责做动作。
      - **Critic (评论家)**：即价值网络 $Q$，负责给动作打分。

---

### 总结要点

1.  **Policy Network**：直接输出动作概率，适合解决连续动作空间或随机策略问题。
2.  **梯度上升**：目标是最大化期望回报，利用 Policy Gradient 更新参数。
3.  **核心逻辑**：$\text{Gradient} \propto \nabla \log \pi(a) \times Q(a)$。简单来说，就是根据动作获得的回报（$Q$），来调整该动作的概率权重（$\log \pi$）。
4.  **与 DQN 的区别**：
    - DQN 是 Value-based，通过选 max Q 来决定动作（间接）。
    - Policy Gradient 是 Policy-based，直接输出动作概率（直接）。

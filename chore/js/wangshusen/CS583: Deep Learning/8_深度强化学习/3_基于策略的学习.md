# 基于策略的学习

这份 PDF 是 Shusen Wang 教授关于 **深度强化学习 (Deep Reinforcement Learning)** 系列课程的第 13 讲第三部分，主题为 **《Policy-Based Reinforcement Learning》（基于策略的强化学习）**。

如果说上一讲（Value-Based / DQN）是教 AI “怎么评价当前局势”，那么这一讲则是教 AI **“直接行动”**。这是深度强化学习的另一大流派，也是现代 AI（如 ChatGPT 的 RLHF 阶段）中非常关键的技术基础。

以下是对这份讲义的深入解构与解读：

### 1. 核心范式转变：从“评委”到“演员”

#### (1) Value-Based (上一讲回顾)

- **思路**：学习一个 Q 函数（评委），给每个动作打分。
- **决策**：挑分最高的那个动作做（确定性决策，或者 $\epsilon$-greedy）。
- **局限**：难以处理连续动作空间（比如机器人精确控制力的大小），且动作缺乏随机性。

#### (2) Policy-Based (本讲重点)

- **思路**：不学打分了，直接学一个 **策略函数 (Policy Function)** $\pi(a|s)$。
- **决策**：这个函数直接输出每个动作的**概率**，然后 Agent 根据概率**抽样**决定做什么。
  - $\pi(\text{Left}|s) = 0.2$
  - $\pi(\text{Right}|s) = 0.1$
  - $\pi(\text{Up}|s) = 0.7$
- **拟合**：用神经网络（Policy Network）来近似这个概率分布 $\pi(a|s; \theta)$。通常最后一层是 **Softmax**，保证概率和为 1。

---

### 2. 数学核心：策略梯度 (Policy Gradient)

这是本讲最硬核的部分，解释了**“没老师教，只有最后的分数，怎么修网络参数”**的问题。

#### (1) 目标函数

我们的目标是最大化**状态价值函数 (State-Value Function)** 的期望：
$$J(\theta) = \mathbb{E}[V(S; \theta)]$$
通俗地说，就是调整参数 $\theta$，让 Agent 无论处于什么状态，通过策略网络算出的平均预期回报（也就是胜率/得分）最大化。

#### (2) 梯度推导 (The "Log-Derivative Trick")

我们要用到梯度上升 (Gradient Ascent) 来更新参数。但是 $V$ 函数里包含了未知的环境反馈，怎么求导？
讲义展示了一个简化版的推导（Policy Gradient Theorem 的核心直觉）：

1.  **定义**：$V(s) = \sum_a \pi(a|s) \cdot Q(s, a)$
2.  **求导**：对 $\theta$ 求导这一坨东西。
    $$\nabla_\theta V \approx \sum_a \nabla_\theta \pi(a|s) \cdot Q(s, a)$$
3.  **Log 技巧**：利用数学恒等式 $\nabla \pi = \pi \cdot \nabla \log \pi$。
    $$\nabla_\theta V = \sum_a \pi(a|s) \cdot [\nabla_\theta \log \pi(a|s) \cdot Q(s, a)]$$
4.  **期望形式**：$\sum_a \pi(a|s) \dots$ 正好就是期望 $\mathbb{E}_{A \sim \pi}[\dots]$。
    $$\nabla_\theta J(\theta) = \mathbb{E}_{A \sim \pi} [\nabla_\theta \log \pi(A|s; \theta) \cdot Q(s, A)]$$

#### (3) 物理意义 (直观解读)

这个公式非常优美，它告诉我们应该如何更新网络：

- **$\nabla_\theta \log \pi(a|s)$**：这是告诉参数 $\theta$，“请往**增加**采取动作 $a$ 的概率的方向走”。
- **$Q(s, a)$**：这是动作 $a$ 的“好坏程度”。
- **相乘**：
  - 如果 $Q(s, a)$ 是正数（好动作），我们就**增加**该动作的概率。
  - 如果 $Q(s, a)$ 是负数（坏动作），我们就**减少**该动作的概率。
  - $Q$ 值越大，调整的力度越大。

---

### 3. 算法实现：怎么算 $Q$？

在实际操作中，公式里的 $Q(s, a)$ 是未知的，我们怎么填这个坑？讲义给出了两种主流方案：

#### 方案一：REINFORCE 算法 (Monte Carlo)

- **方法**：不管三七二十一，先玩完这一整局游戏。
- **计算**：记录下整局的轨迹，算出现实的**折扣回报 (Discounted Return)** $u_t$。
- **核心假设**：用现实发生的 $u_t$ 来近似 $Q(s, a)$。
- **特点**：简单，但是方差大（玩一局运气成分太大，可能这次赢了是因为运气好，不代表策略真好）。

#### 方案二：Actor-Critic (演员-评论家)

- **方法**：这就是把 RL 的两大流派结合起来了！
- **Actor (演员)**：即 Policy Network $\pi(a|s; \theta)$，负责行动。
- **Critic (评论家)**：即 Value Network $Q(s, a; \mathbf{w})$，负责打分。
- **计算**：用一个神经网络（Critic）去估算 $Q$ 值，来指导另一个神经网络（Actor）更新。
- **特点**：更稳定，是目前更主流的方法（如 A2C, A3C, PPO）。

---

### 4. 完整的 Policy Gradient 训练循环

1.  **观察 (Observe)**：看到状态 $s$。
2.  **行动 (Sample)**：策略网络算出概率 $\pi$，随机抽样得动作 $a$。
3.  **估值 (Estimate)**：得到一个评价分数 $q$（通过 REINFORCE 的实际回报，或 Actor-Critic 的预测）。
4.  **求导 (Differentiate)**：计算神经网络对 $\log \pi$ 的梯度 $\mathbf{d}$。
5.  **更新 (Update)**：
    $$\theta \leftarrow \theta + \beta \cdot (q \cdot \mathbf{d})$$

### 总结

这份讲义标志着课程进入了 RL 的高级阶段。

- **Policy-Based** 方法让 AI 像人一样，拥有了“直觉”（看到画面直接反应动作），并且能处理连续动作（如自动驾驶的方向盘角度）。
- 它利用 **Policy Gradient** 将“动作的好坏”直接反向传播到神经网络的参数中，即使没有老师手把手教每一帧该按哪个键。
- 讲义最后引出的 **Actor-Critic** 架构，是现代深度强化学习（包括训练大语言模型）的基石架构。

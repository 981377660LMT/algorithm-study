# 基于价值的学习

这份 PDF 是 Shusen Wang 教授关于 **深度强化学习 (Deep Reinforcement Learning)** 系列课程的第 13 讲第二部分，主题为 **《Value-Based Reinforcement Learning》（基于价值的强化学习）**。

如果说第一部分是构建 RL 的世界观，那么这部分则是讲解**怎么让 AI 真正学会玩游戏**的核心算法：**DQN (Deep Q-Network)** 及其背后的数学原理 **TD Learning (时间差分学习)**。

以下是对这份讲义的深入解构与解读：

### 1. 核心逻辑：从回报到动作价值 (From Return to Action-Value)

#### (1) 为什么我们需要 Q 函数？

- **目标**：玩游戏的最终目的是赢，即最大化**折扣回报 (Discounted Return)** $U_t = R_t + \gamma R_{t+1} + \dots$。
- **困难**：$U_t$ 是未来的事，充满随机性（依赖于未来的状态变化和策略选择）。如果我们在 $t$ 时刻只看 $U_t$，那相当于要玩完整局游戏才能知道这一步走得好不好，这太慢了（Monte Carlo 方法）。
- **解决方案**：我们需要一个函数，能**立即**告诉我们在当前状态 $s$ 下，做一个动作 $a$，未来大概能拿多少分。这就是 **动作价值函数 (Action-Value Function)** $Q(s, a)$。

#### (2) 最优 Q 函数 ($Q^\star$)

- 定义 $Q^\star(s, a) = \max_\pi Q_\pi(s, a)$。
- **意义**：上帝视角的函数。它告诉我们在状态 $s$ 做动作 $a$ 后，如果之后**每一步都玩得最好**，能拿到的最高分数。
- **决策法则**：如果我们有了 $Q^\star$，玩游戏就变得很简单——在任何状态 $s$，只需要查一下哪个动作 $a$ 的分最高：
  $$a^\star = \text{argmax}_a Q^\star(s, a)$$

---

### 2. 深度 Q 网络 (Deep Q-Network, DQN)

#### (1) 为什么叫 "Deep"？

在简单的迷宫里，我们可以用表格记录每个状态-动作对的分数（Q-Table）。但在像 Atari 游戏这样的场景中，状态是**屏幕像素**。

- **输入**：游戏画面（例如 84x84 的像素矩阵）。
- **状态空间**：巨大无比，表格存不下。
- **解决方案**：用**神经网络 (Neural Network)** 来近似这个函数。
  $$Q(s, a; \mathbf{w}) \approx Q^\star(s, a)$$
  其中 $\mathbf{w}$ 是神经网络的参数。

#### (2) DQN 的架构

- **输入**：State $s$ (例如游戏截图)。
- **中间层**：卷积层 (Conv, 提取图像特征) + 全连接层 (Dense)。
- **输出**：每个可能动作 score（例如：Left: 2000, Right: 1000, Up: 3000）。
- **决策**：直接选输出值最大的那个动作（Up）。

---

### 3. 如何训练 DQN？—— 时间差分学习 (TD Learning)

这是讲义中最精彩的部分，使用了通俗易懂的**长途驾驶**例子来解释复杂的 TD 算法。

#### (1) 直觉解释：开车去亚特兰大

假设你要从纽约 (NYC) 开车去亚特兰大 (Atlanta)。

1.  **出发前预测**：你的模型（大脑）估计全程需要 **1000 分钟**。
2.  **现实经验**：你开到了中间站华盛顿 (DC)，花了 **300 分钟**。
3.  **再次预测**：在 DC，你根据当前路况重新估计，去亚特兰大还需要 **600 分钟**。
4.  **矛盾产生**：
    - 之前的估计总时间：1000 分钟。
    - 现在的更准确估计：300 (已发生) + 600 (新估计) = **900 分钟**。
5.  **模型更新**：**900** 比 **1000** 更可靠，因为其中 300 分钟是**真实发生**的事实。你应该调整你的模型，让它下次在 NYC 时预测得更接近 900，而不是 1000。

#### (2) 数学原理：TD Error

将上述直觉映射到强化学习：

- **NYC** = 状态 $s_t$
- **DC** = 下一状态 $s_{t+1}$
- **NYC 到 DC 的时间 (300)** = 奖励 $r_t$
- **在 NYC 的预测 (1000)** = $Q(s_t, a_t; \mathbf{w})$
- **在 DC 的预测 (600)** = $\gamma \cdot \max_a Q(s_{t+1}, a; \mathbf{w})$ (未来的折扣奖励)

**核心公式 (Bellman Equation 近似)**：
我们希望：
$$Q(s_t, a_t; \mathbf{w}) \approx r_t + \gamma \cdot \max_a Q(s_{t+1}, a; \mathbf{w})$$

- **TD Target (也就是那个更准的 900)**：$y_t = r_t + \gamma \cdot \max_a Q(s_{t+1}, a; \mathbf{w})$
- **Loss (损失函数)**：$L = (Q(s_t, a_t; \mathbf{w}) - y_t)^2$
  - _注：这就是回归问题 (Regression)，试图让网络的预测值靠近更靠谱的目标值。_

---

### 4. 完整的 DQN 训练算法流程

讲义最后总结了 Value-Based RL 的单步迭代逻辑：

1.  **观察与行动**：在状态 $s_t$ 下，根据当前网络预测或探索策略，执行动作 $a_t$。
2.  **体验真实**：环境反馈奖励 $r_t$ 和新状态 $s_{t+1}$。
3.  **计算目标 (Target)**：
    - 用网络预测一下在新状态 $s_{t+1}$ 能拿到的最好分数：$\max_a Q(s_{t+1}, a; \mathbf{w})$。
    - 加上刚拿到的真实奖励：$y_t = r_t + \gamma \cdot \max$。
4.  **计算梯度**：对比自己在 $s_t$ 的预测 $Q(s_t, a_t)$ 和目标 $y_t$ 之间的差距。
5.  **更新参数**：使用梯度下降法 (Gradient Descent) 修改网络参数 $\mathbf{w}$，减小这个差距。

### 总结

这份讲义的核心思想是 **Bootstrapping (自举)**。

- 我们没有真实的 label（像监督学习那样）。
- 我们用**"现在的奖励 + 对未来的估计"**，来更新**"对现在的估计"**。
- 虽然是在用一个估计值去更新另一个估计值，但因为引入了真实的 $r_t$（锚点），随着数据量的增加，网络最终会收敛到真实的最优价值函数 $Q^\star$。

这就是 DeepMind 用来让 AI 学会打砖块 (Breakout) 和吃豆人等这一系列 Atari 游戏的魔法背后的原理。

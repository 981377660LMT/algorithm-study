# 基于价值的学习

这节课是深度强化学习（DRL）系列的第二课，王树森老师重点讲解了 **Value-Based Learning（价值学习）**，核心算法是 **Deep Q-Network (DQN)**，以及用于训练它的 **Temporal Difference (TD) 算法**。

以下是对课程内容的深入分析和逻辑重构：

---

### 第一部分：核心逻辑起点 —— 寻找“先知” (Q-Star)

课程首先回顾了上节课的概念，并引出了价值学习的终极目标。

1.  **从 $Q_\pi$ 到 $Q^*$ (去伪存真)**

    - **$Q_\pi(s, a)$**：反映了在特定策略 $\pi$ 下，动作的好坏。但它还不够完美，因为策略 $\pi$ 可能很烂。
    - **$Q^*(s, a)$ (Optimal Action-Value Function)**：这是我们真正想要的。它是通过对策略 $\pi$ 求最大化得到的。
    - **物理意义**：$Q^*$ 是一个**“先知” (Oracle)**。它告诉我们：无论未来发生什么随机事件，只要你现在处于状态 $s$，选择动作 $a$，理论上你平均能拿到的最高最高的分数就是 $Q^*(s, a)$。

2.  **决策逻辑 (Oracle Policy)**
    - 假设我们拥有了 $Q^*$ 这个先知，决策就变得非常简单：**只选分数最高的那个动作**。
    - _股票比喻_：先知告诉你 A 股票平均涨 10 倍，B 涨 2 倍，C 跌一半。虽然未来有随机性（蝴蝶效应），但理性的决策一定是买 A。
    - **价值学习的本质**：既然我们实际上没有 $Q^*$，我们就想办法**近似（Approximate）** 出这样一个函数。

---

### 第二部分：Deep Q-Network (DQN) —— 制造“先知”

由于状态空间太大（例如图片像素组合无穷无尽），无法用表格记录 $Q^*$，所以我们用**神经网络**来拟合它。

1.  **结构设计**

    - **输入**：状态 $s$（例如超级玛丽的游戏画面）。
    - **网络**：卷积层提取特征 -> 全连接层映射。
    - **输出**：一个向量，维度等于动作数量（例如 3 维：左、右、上）。
    - **含义**：输出的每一个数值，就是神经网络对该动作的打分（估计的 Q 值）。

2.  **工作流程**
    - 输入画面 $s_t$ $\rightarrow$ DQN 计算所有动作分数 $\rightarrow$ 选分数最高的动作 $a_t$ 执行 $\rightarrow$ 环境反馈奖励 $r_t$ 和新画面 $s_{t+1}$。

---

### 第三部分：训练灵魂 —— Temporal Difference (TD) 算法

有了神经网络，怎么训练它？这就是 **TD 算法** 的用武之地。王老师用了一个非常直观的“开车”类比来解释这个晦涩的算法。

#### 1. 直觉类比：纽约到亚特兰大

- **场景**：你要开车从纽约去亚特兰大，途径华盛顿 DC。
- **模型预测 (Prediction)**：出发前，模型估算全程需要 **1000 分钟**。
- **真实观测 (Ground Truth)**：你开到了 DC，看表发现走了 **300 分钟**。
- **修正预测 (TD Target)**：在 DC，模型再次估算，说剩下路程还要 **600 分钟**。
- **核心冲突**：
  - 旧的估计：全程 1000 分钟。
  - 新的估计（基于部分事实）：300 (已发生) + 600 (新预估) = 900 分钟。
- **结论**：**900 分钟比 1000 分钟更靠谱**。因为它包含了一部分已发生的真实数据（Real Reality）。
- **TD Error**：$1000 - 900 = 100$ 分钟。我们要更新模型，让它下次预测得更接近 900。

#### 2. 数学原理：Bellman 等式

- 根据折扣回报的定义，$U_t$ 和 $U_{t+1}$ 存在递归关系：
  $$ U*t = R_t + \gamma \cdot U*{t+1} $$
  _(回报 = 现在的奖励 + 打折后的未来回报)_

#### 3. TD 算法在 DQN 中的应用

对应开车的例子：

- **Prediction (出发前的猜测)**：$Q(s_t, a_t; w)$ —— 模型在 $t$ 时刻对未来的总估值。
- **TD Target (开到 DC 后的修正)**：$y_t = r_t + \gamma \cdot Q(s_{t+1}, a_{t+1}; w)$。

  - $r_t$：刚刚拿到手的真实奖励（对应开车去 DC 花的真实 300 分钟）。
  - $Q(s_{t+1}, a_{t+1})$：站在下一刻看未来的估值（对应从 DC 到终点的预计 600 分钟）。
  - _注意：这里的 $a_{t+1}$ 通常是根据网络选出的最优动作（Max 操作）。_

- **TD Error (Loss)**：
  $$ L = (y_t - Q(s_t, a_t; w))^2 $$
- **更新**：对 Loss 求导，用梯度下降更新参数 $w$。目标是让 $T$ 时刻的预测值，去逼近 $T+1$ 时刻算出来的 Target。

---

### 第四部分：DQN 完整训练流程总结

王老师给出了一个标准的算法迭代步骤：

1.  **观测**：获取当前状态 $s_t$ 和动作 $a_t$。
2.  **预测**：DQN 计算 $Q(s_t, a_t; w)$。
3.  **求导**：反向传播计算梯度 $d_t$（此时还未更新）。
4.  **执行与反馈**：Agent 执行动作，环境给出奖励 $r_t$ 和新状态 $s_{t+1}$。
5.  **计算目标 (Target)**：
    - 用 DQN 预测新状态下的最佳分数 $Q(s_{t+1}, a_{t+1}; w)$。
    - 计算 $y_t = r_t + \gamma \cdot Q(s_{t+1}, a_{t+1}; w)$。
6.  **更新**：计算 Loss ($y_t - \text{预测值}$)，做梯度下降更新参数 $w$。

---

### 第五部分：实例展示 (Breakout 游戏)

最后，通过 DeepMind 的打砖块（Breakout）游戏展示了 DQN 的威力：

- **初期**：Agent 随机乱动，接不住球。
- **中期**：学会了接球，能把球弹回去。
- **后期 (高光时刻)**：Agent 学会了 **“打洞” (Tunneling)** —— 把球打到砖块上方，让球在上面疯狂反弹。
- **原理**：DQN 发现这种操作能带来极高的 Reward 累计（Q 值最高），所以它“有意”地选择了这种策略。

---

### 总结要点

1.  **目标**：DQN 的目的是近似最优价值函数 $Q^*$，从而找到最优策略。
2.  **手段**：使用神经网络（CNN 等）来处理复杂状态输入。
3.  **方法**：使用 TD 算法。**不需要等游戏结束**（不需要像蒙特卡洛那样跑完全程），每走一步，利用“即时奖励 + 下一步的估计”作为目标，来修正当前的估计。
4.  **哲学**：利用“部分真实 + 未来预测” 去修正 “纯粹的预测”。

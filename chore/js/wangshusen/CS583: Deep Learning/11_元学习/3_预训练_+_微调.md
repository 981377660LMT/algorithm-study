# 预训练*+*微调

这节课内容是用 pretraining (预训练) + Fine Tuning 解决小样本学习。虽然这类方法很简单，但是准确率与最好的方法相当。

这节课是王树森老师讲解 **Few-shot Learning（小样本学习）** 的第三课，也是该系列的最后一课。

本课聚焦于目前解决 Few-shot Learning 问题**最简单但效果极好**的方法论：**Pretraining + Fine-Tuning（预训练 + 微调）**。

以下是逻辑清晰、深入且不遗漏的课程分析：

---

### 第一部分：核心策略 ——大道至简 (The Power of Simplicity)

王老师一开始就明确指出：尽管近年来有很多花里胡哨的复杂模型（如元学习的 MAML 等），但很多时候，一个简单粗暴的策略就能打败它们。这个策略就是：

1.  **大规模数据预训练 (Pretraining on Large Data)**。
2.  **小样本微调 (Fine-Tuning on Support Set)**。

---

### 第二部分：基础知识铺垫

为了理解后续内容，王老师复习了三个数学工具：

1.  **余弦相似度 (Cosine Similarity)**

    - 衡量两个向量方向是否一致。
    - 公式：$\text{sim}(x, w) = \frac{x^T w}{||x|| \cdot ||w||}$。
    - 如果向量都归一化（长度为 1），则等于内积 $x^T w$。

2.  **Softmax 函数**

    - 将一组数（Logits）转化为概率分布（相加为 1，全为正）。
    - 特点：**会放大最大值的优势**（让强者更强）。

3.  **Softmax 分类器**
    - 结构：全连接层 ($Wx + b$) + Softmax 激活。
    - 输出：$P = \text{softmax}(Wx + b)$。

---

### 第三部分：预训练网络的直接使用 (Baseline)

在不做微调的情况下，如何直接利用预训练好的网络进行 Few-shot 预测？

1.  **预训练 (Pretraining)**：

    - 使用大数据集（如 ImageNet 或 Mini-ImageNet 的训练集）训练一个 CNN 特征提取器 $f$（aka Embedding)。
    - 训练方法可以是监督学习（Softmax Loss）或度量学习（Siamese/Triplet Loss）。

2.  **原型计算 (Prototype Calculation)**：

    - 假设 Support Set 是 3-way 2-shot（3 类，每类 2 图）。
    - 提取每类所有样本的特征，取**平均值**，得到该类的**均值向量（原型，Prototype）** $\mu_1, \mu_2, \mu_3$。
    - 通常会对 $\mu$ 进行归一化 ($||\mu||=1$)。

3.  **预测 (Prediction)**：
    - 提取 Query 图片的特征 $q$，并归一化。
    - 计算 $q$ 与 $\mu_1, \mu_2, \mu_3$ 的余弦相似度（或内积）。
    - 谁的相似度最高，就属于哪一类。

---

### 第四部分：进阶策略 —— Fine-Tuning (微调)

直接用预训练特征虽然不错，但加上 Fine-Tuning 可以让准确率大幅提升（实验证明可提升 2%~7%）。

#### 1. 如何做 Fine-Tuning？

- **目标**：在 Support Set 上训练一个新的分类器（甚至微调 CNN）。
- **网络初始化**：
  - 保持 CNN 参数（可以固定，也可以微调）。
  - **关键技巧**：Softmax 分类器的权重矩阵 $W$ **不要随机初始化**！
  - **初始化 Trick**：将 $W$ 的每一行初始化为对应类别的**均值向量 (Prototype) $\mu_k$**。如果不做训练，这等价于上面的 Baseline；以此为起点训练，收敛更快更好。

#### 2. Loss Function (损失函数)

- 使用 Support Set 中的样本计算 **Cross Entropy Loss**。
- 通过梯度下降更新分类器参数 $W, b$ （以及可选地更新 CNN 参数）。

#### 3. 正则化技巧 (Three Powerful Tricks)

由于 Support Set 样本极少，极易过拟合，这三个技巧至关重要：

1.  **初始化 (Initialization)**：如上所述，用 $\mu$ 初始化 $W$。
2.  **熵正则化 (Entropy Regularization)**：
    - **直觉**：对于 Query 图片，我们希望分类器的预测要“自信”（即属于某类的概率很大，其他很小），而不是由犹豫不决（概率均匀）。
    - **操作**：在 Loss 中加入一项，最小化预测分布 $P$ 的信息熵 (Entropy)。熵越小，分布越尖锐（越自信）。
3.  **Cosine Classifier (余弦分类器)**：
    - **传统 Softmax**：$Wx + b$ （内积）。
    - **改进**：$\frac{W_i \cdot x}{||W_i|| \cdot ||x||} \cdot \text{scale}$ （余弦相似度）。
    - **效果**：消除向量长度的影响，关注方向（语义），在 few-shot 场景下通常比内积更有效。

---

### 总结

1.  **大道至简**：解决 Few-shot Learning，最强也是最稳健的方法往往是 **Pretraining + Fine-Tuning**。
2.  **流程**：
    - 利用大数据预训练 CNN。
    - 计算 Support Set 各类别的均值向量 $\mu$。
    - 用 $\mu$ 初始化分类器 $W$。
    - 在 Support Set 上进行少量迭代的 Fine-Tuning。
3.  **核心 Tricks**：**Cosine 分类器** + **Prototype 初始化** + **熵正则化**。

王老师通过这三节课，从基础概念（Support Set/Query），到度量学习（Siamese/Triplet），最后回归到实用主义的 Pretraining + Fine-tuning，完整构建了 Few-shot Learning 的知识体系。

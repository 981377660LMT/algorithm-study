# 预训练*+*微调

这份 PDF 是 Shusen Wang 教授关于 **元学习 (Meta Learning)** 专题的第三部分，主题回归到了深度学习中最经典的范式：**预训练 (Pretraining) 与 微调 (Fine-Tuning)**。

这就好比说：之前的 Meta Learning（如孪生网络）是试图让模型学会“如何比较”，而这一讲的方法则是让模型先学会“看世界”（提取特征），然后在面对新任务时，利用极少的信息快速调整自己的认知。

以下是对该课件的深度分析与结构化解读：

### 1. 核心理念：从“学会学习”回归到“特征迁移”

这部分内容其实挑战了复杂的 Meta Learning 算法（如 MAML, Matching Networks 等）。它指出，通过简单的 **Baseline（基线）** 方法，即“在大数据上预训练特征提取器，在小数据上做简单分类器”，往往能达到甚至超越复杂的 SOTA（State-of-the-Art）效果。

### 2. 方法论一：基于余弦相似度的简单分类 (Baseline)

这种方法不需要复杂的元学习训练阶段，直接利用已有的特征提取能力。

#### 2.1 步骤一：预训练 (Pretraining)

- **动作**：在大规模数据集（Base Classes，比如 ImageNet 或整个 Omniglot 的训练集）上训练一个标准的 CNN。
- **目的**：获得一个强大的**特征提取器** $f(x)$。这个网络能把图片转换成有意义的特征向量。

#### 2.2 步骤二：构建原型 (Prototypes)

- 场景：推理阶段，给定一个 $N$-way $K$-shot 的 Support Set（例如 3 类，每类 2 张图）。
- **动作**：
  1.  将 Support Set 中的所有图片通过预训练的 CNN 提取特征。
  2.  对于每一类，计算其所属样本特征向量的**平均值 (Mean Vector)**，记为 $\mu_c$。
  3.  通常会对 $\mu_c$ 进行归一化 (Normalize) 处理，使其模长为 1。
- **意义**：这个平均向量 $\mu_c$ 就代表了该类别在这个特征空间中的**中心/原型**。

#### 2.3 步骤三：预测 (Prediction)

- **动作**：
  1.  给定一个 Query 图片，提取其特征 $q$（同样归一化）。
  2.  计算 $q$ 与各类中心 $\mu_c$ 的**余弦相似度** (Cosine Similarity)。
  3.  通过 Softmax 函数将相似度转化为概率分布。
  - 公式：$p = \text{Softmax}(M^T q)$，其中 $M$ 是由所有 $\mu_c$ 组成的矩阵。

---

### 3. 方法论二：微调 (Fine-Tuning)

如果觉得直接用“均值”做分类器还不够准，我们可以利用 Support Set 中那一点点数据，对分类器进行微调。

#### 3.1 什么是 Fine-Tuning？

在上述 Baseline 中，我们实际上构建了一个简单的线性分类器 $W \cdot x + b$，其中权重 $W$ 被硬性设定为类中心 $\mu$。
**Fine-Tuning** 的思想是：不要把 $W$ 锁死，而是把它作为**初始化值**，然后在 Support Set 上继续训练几步。

#### 3.2 优化目标

$$ \min*{W, b} \sum*{(x,y) \in Support} \text{CrossEntropy}(y, \hat{p}) + \text{Regularization} $$

- 利用 Support Set 中的已知标签来更新分类层权重 $W$ 和 $b$。
- 甚至可以微调 CNN 的一部分参数（Feature Extractor）。
- **效果**：课件指出，Fine-Tuning 可以带来 2% ~ 7% 的显著准确率提升。

---

### 4. 提升性能的三个关键技巧 (Tricks)

课件重点强调了在 Few-Shot 场景下，如何让 Fine-Tuning 效果更好：

#### Trick 1: 良好的初始化 (Good Initialization)

- 不要随机初始化分类头 (Classification Head) 的权重 $W$。
- **做法**：直接用 Support Set 的**类均值特征 $\mu$** 来初始化 $W$。
- **逻辑**：既然 $\mu$ 已经也是类中心了，那么从这里开始微调，比从乱猜开始微调收敛更快、更稳。

#### Trick 2: 熵正则化 (Entropy Regularization)

- **问题**：Support Set 样本太少，容易过拟合。
- **做法**：利用 Query 样本（虽然没有标签）的信息。在 Loss 中加入一项，要求模型对 Query 样本的预测分布具有**低熵 (Low Entropy)**。
- **直观解释**：如果你不知道 Query 是什么，但你必须选一个。低熵意味着“我很确信它是 A 类”，高熵意味着“它可能是 A，也可能是 B”。强迫模型“自信”通常有助于找到清晰的决策边界。

#### Trick 3: 余弦分类器 (Cosine Classifier)

- **传统分类器**：$W \cdot x + b$ （点积，受向量模长影响大）。
- **做法**：改用余弦相似度。
  $$ p = \text{Softmax}(\text{sim}(W, x) + b) $$
    $$ \text{sim}(W, x) = \frac{W \cdot x}{\|W\| \cdot \|x\|} $$
- **优势**：消除了向量模长（亮度和对比度等因素）的影响，专注于向量的方向（语义特征），在小样本场景下更鲁棒。

---

### 5. 总结

这份课件提供了一个关于 Few-Shot Learning 的**去魅 (Demystifying)** 视角：

- **不要迷信元学习**：有时候，你不需要复杂的 MAML 或定制的图神经网络。
- **大道至简**：
  1.  找个大数据集把特征提取器（CNN）练好。
  2.  拿到新任务时，算一下新类别的特征中心。
  3.  用这些中心初始化一个分类器。
  4.  用这几个新样本微调一下。
- 这套 **"Pretrain + Fine-tune"** 的组合拳，结合几个 Tricsk，往往是解决小样本问题最简单且强力的方案。

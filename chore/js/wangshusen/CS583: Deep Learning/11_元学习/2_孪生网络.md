# 孪生网络

这份 PDF 是 Shusen Wang 教授关于 **元学习 (Meta Learning)** 专题的第二部分，核心主题是 **孪生网络 (Siamese Network)** 及其在小样本学习 (Few-Shot Learning) 中的应用。

这份课件深入讲解了如何通过神经网络学习“相似度”或“距离度量”，从而解决小样本分类问题。以下是对该课件的深度分析与结构化解读：

### 1. 核心方法论：学习度量 (Metric Learning)

在上一份课件中提到，Few-Shot Learning 的核心思想是“学习一个相似度函数”。这份课件具体落实了**如何用神经网络来实现这个函数**。

主要介绍了两种训练范式：

1.  **Pairwise Similarity (成对相似度)**：输入两张图，输出它们是否属于同一类（0 或 1）。
2.  **Triplet Loss (三元组损失)**：输入三张图（锚点、正样本、负样本），优化它们在特征空间中的距离。

---

### 2. 范式一：基于成对相似度的孪生网络 (Siamese Network with Pairwise Loss)

这是最经典的 Siamese Network 用法（源自 Koch et al., 2015）。

#### 2.1 网络架构

- **双塔结构**：网络有两个输入端（Input 1 和 Input 2），分别接收两张图片。
- **共享参数**：两个分支使用**完全相同**的 CNN（参数共享）提取特征。
  - $h_1 = f(x_1)$
  - $h_2 = f(x_2)$
- **特征交互**：计算两个特征向量的差异。课件中展示的方法是计算绝对差值：$z = |h_1 - h_2|$。
- **相似度输出**：将差异向量 $z$ 输入全连接层 (Dense Layers)，最后通过 Sigmoid 激活函数输出一个标量 score（范围 0~1）。

#### 2.2 训练过程

- **数据构造**：
  - **正样本对 (Positive Samples)**：两张图属于同一类（例如都是老虎），Target = 1。
  - **负样本对 (Negative Samples)**：两张图属于不同类（例如老虎和汽车），Target = 0。
- **目标**：如果是同类，输出分数趋近 1；如果是异类，输出分数趋近 0。这本质上被建模为一个**二分类问题** (Binary Classification)。

#### 2.3 小样本预测 (One-Shot Prediction)

- 训练完成后，给定一张 Query 图片和 Support Set（比如 6 张不同类别的图片）：
  1.  将 Query 分别与 Support Set 中的 6 张图片组成“对”。
  2.  送入网络计算相似度。
  3.  取相似度最高的那个类别作为预测结果。

---

### 3. 范式二：基于三元组损失的孪生网络 (Siamese Network with Triplet Loss)

为了更好地进行特征嵌入（Feature Embedding），FaceNet (Google, 2015) 提出了 Triplet Loss，课件详细讲解了这一机制。

#### 3.1 三元组 (Triplet) 的构成

训练数据不再是成对的，而是由三个样本组成 $(x^a, x^p, x^n)$：

- **Anchor ($x^a$)**：锚点样本（例如一张“老虎 A”）。
- **Positive ($x^p$)**：与 Anchor 同类的正样本（例如另一张“老虎 B”）。
- **Negative ($x^n$)**：与 Anchor 异类的负样本（例如一张“汽车”）。

#### 3.2 训练目标与损失函数

- **特征提取**：三个样本通过同一个 CNN 得到特征向量 $f(x^a), f(x^p), f(x^n)$。
- **距离计算**：通常使用欧氏距离。
  - $d^+ = \|f(x^a) - f(x^p)\|^2$ （Anchor 与同类的距离）
  - $d^- = \|f(x^a) - f(x^n)\|^2$ （Anchor 与异类的距离）
- **优化目标**：我们需要 $d^+$ 尽可能小，且 $d^-$ 尽可能大。
- **Triplet Loss 公式**：
  $$ Loss(x^a, x^p, x^n) = \max(0, \ d^+ + \alpha - d^-) $$
  - **Margin ($\alpha$)**：这是一个超参数（正数）。它的作用是强制 $d^-$ 不仅要大于 $d^+$，而且要**大出一个 $\alpha$ 的距离**。这能防止网络“偷懒”（即不再优化那些已经分得开但分得不够开的样本）。

#### 3.3 几何解释

在特征空间中，Triplet Loss 的作用是将 Anchor “拉向” Positive，同时将 Negative “推开”。

---

### 4. 总结与对比

这份课件清晰地展示了从“相似度判断”到“距离度量学习”的演进：

| 特性         | Pairwise Siamese Network          | Triplet Network                        |
| :----------- | :-------------------------------- | :------------------------------------- |
| **输入**     | 两张图片 (Pair)                   | 三张图片 (Triplet)                     |
| **核心逻辑** | **分类逻辑**：判断是同类还是异类  | **度量逻辑**：直接优化特征空间距离     |
| **Loss**     | Binary Cross Entropy (二元交叉熵) | Triplet Loss (Ranking Loss)            |
| **预测方式** | 看 Similarity Score (越高越好)    | 看 Distance (越小越好)                 |
| **优势**     | 结构简单直观，容易实现            | 对特征空间的结构约束更强，效果通常更好 |

**在 Few-Shot Learning 中的意义**：
这一讲解决了“模型并未见过测试类别”的问题。通过在大规模数据集上训练 Siamese Network，模型学会了**“如何区分两张图片是否属于同一物体”**（学习到了通用的特征比较能力）。这种能力可以迁移到全新的类别上，从而实现 One-Shot 或 Few-Shot 分类。

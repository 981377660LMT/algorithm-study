# 孪生网络

这节课是王树森老师讲解 **Few-shot Learning（小样本学习）** 的第二课，深入讲解了 **“孪生网络”（Siamese Network）** 及其两种核心训练方法：基于二元组（Pairwise）的训练和基于三元组（Triplet Loss）的训练。

以下是逻辑清晰、不遗漏的深度讲解：

---

### 第一部分：孪生网络 (Siamese Network) 的基本概念

#### 1. 名字的由来

- **Siamese**：原意指连体婴儿（Siamese Twins）。
- **形象比喻**：网络有两个（或三个）“头”（输入端），但共用一个“身体”（特征提取网络）。
- **本质**：两个完全相同的神经网络，**共享同一套参数**。

#### 2. 工作原理

- **特征提取器 (Feature Extractor)**：一个卷积神经网络（CNN，如 ResNet），记作函数 $f$。
- **输入**：两张图片 $x_1, x_2$。
- **特征向量**：分别通过同一个 CNN，得到向量 $h_1 = f(x_1)$ 和 $h_2 = f(x_2)$。
- **核心思想**：我们不直接让 CNN 分类（输出它是老虎还是猫），而是让 CNN **提取特征**。提取出的特征应该具备某种性质：同类物体的特征挨得近，不同类物体的特征离得远。

---

### 第二部分：训练方法一 —— Pairwise Method (二元组)

这是最直观的训练方法：给一对图，判断它们是不是同类。

#### 1. 数据构造

- **正样本 (Positive Pair)**：
  - 从训练集随机抽一张图（如“老虎 A”）。
  - 同类中再抽一张（如“老虎 B”）。
  - **Label = 1**（相似度满分）。
- **负样本 (Negative Pair)**：
  - 从训练集随机抽一张图（如“汽车”）。
  - 从其他类别抽一张（如“大象”）。
  - **Label = 0**（相似度为零）。
- **平衡**：训练时正负样本比例通常设为 1:1。

#### 2. 网络结构

![alt text](image-1.png)

1.  两张图 $x_1, x_2$ 分别通过同一个 CNN 得到特征 $h_1, h_2$。
2.  **计算差异向量**：$z = |h_1 - h_2|$ （逐元素取绝对值）。
3.  **全连接层 (Dense Layer)**：将 $z$ 输入几个全连接层。
4.  **输出层**：使用 `Sigmoid` 激活函数，输出一个 0~1 之间的标量。
    - 输出 $\approx 1$：是同类。
    - 输出 $\approx 0$：是异类。

#### 3. 损失函数 (Loss)

- 这是一个典型的二分类问题（是同类 vs. 不是同类）。
- 使用 **Cross Entropy Loss (交叉熵损失)**：衡量预测值（0~1）和真实标签（0 或 1）之间的差距。

#### 4. 预测 (Inference)

- **场景**：One-shot Classification (Support Set 没见过，比如全是松鼠、狐狸)。
- **操作**：拿到一张 Query 图片，将其与 Support Set 里的每一张图分别组队，输入网络计算相似度。
- **决策**：选相似度最高的那个类别。

---

### 第三部分：训练方法二 —— Triplet Loss (三元组损失)

这是更加常用且更有几何直觉的方法，它直接在特征空间（Feature Space）里拉扯距离。

#### 1. 数据构造 (Triplet)

每次训练不是取两张图，而是取三张图 $(x_a, x_+, x_-)$：

- **Anchor ($x_a$)**：锚点图片（如“老虎 A”）。
- **Positive ($x_+$)**：正样本，同类图片（如“老虎 B”）。
- **Negative ($x_-$)**：负样本，异类图片（如“大象”）。

#### 2. 距离计算

![alt text](image-2.png)

- 将三张图分别输入同一个 CNN，得到特征 $f(x_a), f(x_+), f(x_-)$。
- **正样本距离 ($d_+$)**：$||f(x_a) - f(x_+)||^2$ （欧氏距离）。希望它越小越好。
- **负样本距离 ($d_-$)**：$||f(x_a) - f(x_-)||^2$。希望它越大越好。

#### 3. 核心逻辑：Margin (间隔)

- 我们不仅希望 $d_- > d_+$，还希望 $d_-$ 即使减去一个安全距离（Margin $\alpha$）依然大于 $d_+$。
- 即目标是：$d_+ + \alpha < d_-$。
- **直观理解**：不仅要分对，还要分得足够开，留有余地。

#### 4. 损失函数 (Triplet Loss)

$$ Loss = \max(0, d*+ + \alpha - d*-) $$

- **可以理解为**：
  - 如果 $d_+ + \alpha < d_-$（分得很好了），Loss = 0，不更新参数。
  - 如果 $d_+ + \alpha > d_-$（分得不好，离得太近甚至倒挂了），Loss > 0，网络需要调整参数。
- **梯度更新**：拉近 Anchor 和 Positive，推开 Anchor 和 Negative。

#### 5. 预测 (Inference)

- 与 Pairwise 略有不同，不用 Sigmoid 算相似度了。
- 直接算**特征向量的欧氏距离**。
- Query 和谁的距离最近，就归为哪一类。

---

### 第四部分：课程总结

1.  **解决问题**：Few-shot Learning（训练集没见过 Query 的类别）。
2.  **核心策略**：虽然不认识新类别，但可以通过大量已知类别训练一个“判断异同”的神经网络。
    - 让网络学会：即使不认识它们，也能算出它们的特征是“像”还是“不像”。
3.  **两种流派**：
    - **Pairwise (Sigmoid)**：输入一对，输出相似度概率 (0~1)。
    - **Triplet (Distance)**：输入三元组，在特征空间优化距离（同类聚拢，异类排斥）。
4.  **实际效果**：Embedding (提取特征向量) 的方法是解决 Few-shot 最简单且极其有效的主流思路。即便后续有很多花哨的模型，Siamese Network / Triplet Loss 依然是基石。

---

我对新方法是比较质疑的。各种奇奇怪怪的算法，可能最后还比不上简单的模型+有效的 tricks。

https://www.bilibili.com/video/BV14q4y177gQ

# NAS\_基础

这是一份关于王树森老师“神经网络结构搜索 (1/3): 基本概念和随机搜索”课程的深度分析与逻辑讲解。

这节课是 NAS（Neural Architecture Search）系列的入门篇，主要明确了问题定义（什么是参数 vs. 超参数）、搜索目标（寻找最优架构）、难点（搜索空间巨大 vs. 评估代价昂贵）以及最基准的解法（随机搜索）。

---

### 第一部分：核心概念辨析 (Definitions)

在进入 NAS 之前，王老师首先理清了深度学习中两个极易混淆的概念，这是理解整个问题的基础。

#### 1. 参数 (Parameters) vs. 超参数 (Hyperparameters)

| 概念                         | 定义                                             | 例子                                                                                                      | 来源                                                 |
| :--------------------------- | :----------------------------------------------- | :-------------------------------------------------------------------------------------------------------- | :--------------------------------------------------- |
| **参数 (Parameters)**        | 神经网络内部需要学习的变量，也叫权重 (Weights)。 | 卷积核的数值、全连接层的权重矩阵 $W$ 和偏置 $b$。                                                         | 通过**训练数据**，利用梯度下降等算法自动学习得到。   |
| **超参数 (Hyperparameters)** | 在开始训练前，必须人为手动设定的数值配置。       | **结构类**：层数、卷积核大小、通道数。<br>**优化类**：学习率、Batch Size、Epochs、优化器选择 (SGD/Adam)。 | 传统上依赖人工经验 (Manual Tuning)，现在试图自动化。 |

#### 2. NAS 的定位

- **目标**：NAS 旨在自动化寻找最优的**结构类超参数**。
- **输入**：无需人工过多干预。
- **输出**：一组能使 Validation Accuracy（验证集准确率）最大化的网络结构配置。
- **其他指标**：除了准确率，现代 NAS 还会考虑**计算量 (FLOPs)** 和 **内存开销 (Memory)**，特别是在移动端部署场景（如 MobileNet vs. ResNet）。

---

### 第二部分：问题建模 (Problem Formulation)

如何用数学或逻辑语言描述 NAS 问题？王老师通过构建一个具体的**搜索空间**来解释。

#### 1. 卷积神经网路 (CNN) 的超参数举例

对于一个卷积层，至少包含三个维度的超参数：

1.  **卷积核数量 (Number of Filters)**：决定输出通道数（宽度）。
2.  **卷积核大小 (Filter Size)**：如 $1\times1, 3\times3, 5\times5$。
3.  **步长 (Stride)**：如 1 或 2。

#### 2. 搜索空间 (Search Space)

由于连续的数值空间（如卷积核数量取 10 到 100 任意整数）太大，通常我们会将搜索空间离散化。

- **设定候选集 (Candidates)**：
  - 核数量：$\{24, 36, 48, 64\}$ (4 种)
  - 核大小：$\{3\times3, 5\times5, 7\times7\}$ (3 种)
  - 步长：$\{1, 2\}$ (2 种)
- **网络深度**：假设我们要搭建 **20 层** 卷积。
- **空间大小计算**：
  - 单层可能性：$4 \times 3 \times 2 = 24$ 种组合。
  - 全网可能性：$24^{20} \approx 4 \times 10^{27}$。
- **结论**：这是一个**天文数字**。搜索空间的大小呈指数级爆炸。

---

### 第三部分：基准解法 —— 随机搜索 (Random Search)

最简单、最初级的 NAS 方法就是“撞大运”。

#### 1. 算法流程

1.  **采样 (Sampling)**：从巨大的搜索空间中**均匀随机**抽取一组超参数（即确定每一层的配置）。
2.  **构建 (Build)**：根据这组超参数搭建神经网络。
3.  **训练 (Train)**：初始化权重，在训练集上从头训练（Training from Scratch），直到收敛。
4.  **评估 (Evaluate)**：在验证集上测试，得到 Validation Accuracy。
5.  **循环 (Loop)**：重复上述步骤 $N$ 次。
6.  **择优 (Select)**：从这 $N$ 个结果中，选择准确率最高的那组架构。

#### 2. 也是交叉验证 (Cross Validation)

- 这种流程本质上就是标准的**交叉验证**调参过程。

---

### 第四部分：NAS 的两大难点 (Challenges)

为什么 NAS 很难做？为什么随机搜索不够好？王老师总结了两个核心痛点：

#### 1. 评估代价极其昂贵 (High Evaluation Cost)

- **痛点**：要“知道”一组架构好不好，必须先把它**训练出来**。
- **耗时**：训练一个大型 CNN（如 ResNet 级别）在 GPU 上可能需要几小时甚至几天。
- **限制**：由于算力限制，你可能只能尝试几百到几千次。

#### 2. 搜索空间过于巨大 (Huge Search Space)

- **痛点**：搜索空间是 $10^{27}$ 级别，而你只能尝试 $10^3$ 次。
- **比喻**：如果搜索空间是汪洋大海，随机搜索尝试的几次就像是几滴水。
- **结论**：随机搜索（Random Search）在如此稀疏的采样下，很难找到真正的全局最优解。

---

### 第五部分：总结与展望

1.  **随机搜索的地位**：它是 Baseline（基准线）。任何新的 NAS 算法（如强化学习、进化算法、可微搜索等）被提出来，都必须证明比随机搜索效果更好，才算有价值。
2.  **后续课程**：由于随机搜索效率太低，接下来的两节课将介绍更高效的策略：
    - **基于强化学习 (Reinforcement Learning)** 的方法。
    - **基于可微搜索 (Differentiable Search)** 的方法（如 DARTS）。

这节课清晰地界定了 NAS 的战场：**在有限的计算资源（主要约束）下，如何从指数级爆炸的搜索空间中，高效地找到性能最好的网络架构。**

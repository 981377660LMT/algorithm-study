https://www.bilibili.com/video/BV14q4y177gQ

# NAS\_基础

这份 PDF 是 Shusen Wang 教授关于 **神经架构搜索 (Neural Architecture Search, NAS)** 系列课程的第一部分：**基础 (Basics)**。

这份课件主要完成了对 NAS 这一领域的**定性**和**问题定义**，解释了“我们在做什么”以及“这件事为什么难”。以下是对该课件的深度分析与与结构化解读：

### 1. 宏观定位：模型设计的三个层次

课件首先通过层层递进的方式，厘清了深度学习模型中三个维度的差异。这是理解 NAS 必须具备的视角：

- **参数 (Parameters / Weights)**：
  - **定义**：卷积核的具体数值、全连接层的权重等。
  - **获取方式**：通过**训练 (Training)** 获得（如 SGD, Adam）。
  - **层级**：最底层。
- **超参数 (Hyper-parameters)**：
  - **定义**：学习率、Batch size、正则化系数等。
  - **获取方式**：通常通过**人工调优 (Tuning)** 或简单的网格/随机搜索获得。
  - **层级**：中间层，控制训练过程。
- **架构 (Architecture)**：
  - **定义**：网络的层数、卷积层的类型、过滤器的数量 (Width)、卷积核的大小 (Kernel Size)、步长 (Stride)、层之间的连接方式（如 ResNet 的跳跃连接）。
  - **获取方式**：传统上由专家**手工设计** (Manually Designed)，如 VGG, ResNet, MobileNet。
  - **层级**：最高层，决定了模型能力的上限。

**NAS 的核心使命**：将“架构设计”这一步从“人工专家设计”转变为“算法自动搜索”。

---

### 2. 问题定义 (Problem Statement)

NAS 被定义为一个**优化问题**：

- **目标 (Objective)**：寻找一个网络架构，使其在验证集上的**准确率 (Accuracy)** 最高，或者在满足准确率的前提下**效率 (Efficiency)** 最高（如参数量最少、推断延迟最低）。
- **变量**：架构本身（层数、卷积核大小、通道数等）。

### 3. 核心难点：组合爆炸 (Combinatorial Explosion)

课件通过一个具体的量化例子，直观地展示了 NAS 面临的最大挑战——**巨大的搜索空间 (Search Space)**。

假设我们要设计一个简单的 20 层 CNN，每一层我们要决定三个参数：

1.  **过滤器数量 (# of filters)**：可选 {24, 36, 48, 64}（4 种选择）
2.  **卷积核大小 (Size of filters)**：可选 {3x3, 5x5, 7x7}（3 种选择）
3.  **步长 (Stride)**：可选 {1, 2}（2 种选择）

那么，每一层的可能性是 $4 \times 3 \times 2 = 24$ 种。
对于 20 层网络，总的架构可能性（搜索空间大小）为：
$$ 24^{20} \approx 4 \times 10^{27} $$

**解读**：

- $4 \times 10^{27}$ 是一个天文数字。这意味着我们绝对不可能通过“遍历”来找到最优解。
- 这也解释了为什么需要 NAS 算法——我们需要一种比“盲目尝试”更聪明的方法在这么大的空间里导航。

### 4. 基线方法：随机搜索 (Random Search)

课件介绍了一种最朴素的 NAS 方法作为 Baseline：

- **流程**：
  1.  从搜索空间中随机采样一组架构参数。
  2.  从头开始训练这个模型。
  3.  在验证集上评估准确率。
  4.  重复上述步骤，最后保留最好的那个。
- **缺点**：效率极低。在 $10^{27}$ 的空间里随机乱撞，很难找到最优解，且极度浪费计算资源。

### 5. NAS 的两大挑战 (Challenges)

最后，课件总结了 NAS 领域不仅要解决“怎么搜”的问题，还要解决“搜得起”的问题：

1.  **昂贵的评估成本 (Each trial is expensive)**：
    - 要评估一个架构好不好，通常需要把它**完全训练**出来。而在 GPU 上训练一个现代 CNN 可能需要数小时甚至数天。
    - 不仅搜得慢，而且烧钱（显卡时间）。
2.  **巨大的搜索空间 (The search space is too big)**：
    - 如前所述，离散的组合空间极其庞大，且非凸、非平滑，传统梯度下降法无法直接应用（至少在最原始的设定下）。

### 总结

这份 Slides 是 NAS 的**开篇**，它没有涉及强化学习 (RL)、进化算法 (Evolutionary Algorithms) 或可微搜索 (Differentiable NAS / DARTS) 等具体解法，而是扎实地构建了**问题背景**：

> **NAS 就是试图在一个 $10^{27}$ 规模的离散空间中，寻找一个训练代价极其昂贵的函数的全局最优解。**

这为后续介绍高效的 NAS 策略（如权重共享、性能预测器、可微搜索）埋下了伏笔：为了解决上述两个挑战，后续的研究方向必然是**缩小搜索空间**或者**降低评估成本**。

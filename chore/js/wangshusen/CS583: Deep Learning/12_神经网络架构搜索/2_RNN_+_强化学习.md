# RNN*+*强化学习

这份 Slides 是 Shusen Wang 教授关于 **神经架构搜索 (NAS)** 系列课程的第二部分，专门讲解了 NAS 领域最著名的开山之作——由 Zoph 和 Le 在 ICLR 2017 发表的 **"Neural Architecture Search with Reinforcement Learning"**。

这份课件深入解构了 **"RNN 控制器 + 强化学习 (RL)"** 这一经典 NAS 范式。以下是对该课件的深度分析与解读：

### 1. 核心范式：控制器-子网络 (Controller-Child) 架构

NAS 的自动搜索过程被建模为一个“生成”过程：

- **控制器 (Controller)**：

  - **角色**：这是“大脑”，负责设计网络。
  - **实现**：使用一个 **RNN (循环神经网络)**。
  - **工作流**：RNN 像写文章生成单词一样，一个接一个地生成子网络的架构参数。

- **子网络 (Child Network)**：
  - **角色**：这是被设计出来的产品（比如一个用于图像分类的 CNN）。
  - **生成过程**：被描述为一个序列预测问题。

### 2. 生成过程解构 (The Generation Process)

课件详细展示了 RNN 如何逐步确定一个 CNN 的架构。假设我们要生成一个 $N$ 层的 CNN：

1.  **分步预测**：对于每一层（Layer $i$），RNN 需要连续做出多个离散决策，例如：
    - **Filters 数量**：RNN 输出一个 Softmax 分布，从 {24, 36, 48, 64} 中选择。
    - **Filters 尺寸**：RNN 继续输出，从 {3x3, 5x5, 7x7} 中选择。
    - **步长 (Stride)**：RNN 继续输出，从 {1, 2} 中选择。
2.  **自回归 (Autoregressive)**：
    - 上一步的选择（Action）会经过 **Embedding 层** 变成向量，作为下一步 RNN 的输入。这意味着 RNN 的后续决策会考虑到之前的决策（例如，如果上一层用了 stride=2，下一层可能倾向于增加 filter 数量）。
3.  **循环生成**：这个过程不断重复，直到定义完所有层的参数，形成一个完整的网络架构描述。

### 3. 最核心的难点：不可微性 (Non-Differentiability)

为什么不能直接用梯度下降训练控制器？

- **目标函数 ($r$)**：我们要最大化生成的子网络在验证集上的**准确率 (Validation Accuracy)**。
- **断裂的梯度**：从“控制器参数 $\theta$”到“子网络架构”再到“验证集准确率”，中间隔着两个巨大的黑盒：
  1.  **离散采样**：从 RNN 的概率分布中采样具体的架构参数（argmax 或 sampling）是不可微的操作。
  2.  **黑盒评估**：训练子网络并评估准确率，这是一个复杂的系统过程，没有明确的解析式可以求导。

因此，**无法计算 $\frac{\partial \text{Accuracy}}{\partial \theta}$**，反向传播 (Backprop) 在这里失效了。

### 4. 解决方案：强化学习 (Reinforcement Learning)

既然不能求导，就把它当作一个 RL 问题来解。课件引入了 **REINFORCE 算法 (Policy Gradient)**。

- **映射关系**：

  - **Agent**：RNN 控制器。
  - **Environment**：训练和评估子网络的过程。
  - **Action ($a$)**：RNN 选取的每一个架构参数（如 "3x3"）。
  - **State**：RNN 当前的隐藏层状态。
  - **Policy Function ($\pi$)**：RNN 本身，由参数 $\theta$ 决定。
  - **Reward ($R$)**：子网络训练好后，在验证集上跑出来的**准确率**。

- **优化逻辑 (REINFORCE)**：
  - RNN 生成一个架构 $\rightarrow$ 训练子网络 $\rightarrow$ 得到准确率 $R$。
  - **如果 $R$ 很高**：通过梯度上升更新 $\theta$，**提高**生成该架构动作序列的概率（鼓励）。
  - **如果 $R$ 很低**：更新 $\theta$，**降低**该动作序列的概率（惩罚）。
  - **公式**：$\theta \leftarrow \theta + \alpha \sum \nabla_\theta \log \pi(a_t | \dots) \cdot R$。

### 5. 致命瓶颈：极其昂贵 (Extremely Expensive)

课件在最后点出了这种方法的阿喀琉斯之踵。

- **昂贵的反馈循环**：RL 算法需要成千上万次尝试才能收敛。
- **单次尝试的成本**：在 NAS 中，“执行一次动作并获得奖励”意味着**从头训练一个完整的 CNN**。这在现代 GPU 上可能需要几小时甚至几天。
- **计算量爆炸**：
  - 训练好 RNN 需要 10,000+ 次更新。
  - 意味着要从头训练 10,000+ 个 CNN。
  - **现实数据**：Zoph & Le 的原始论文使用了 **800 张 GPU** 并行运行了 **28 天**才搜出一个好的架构。

### 总结

这份课件讲解了 **第一代 NAS** 的原理。它的历史地位在于证明了“自动化设计优于人工设计”的可能性，但其实用性被极高的计算成本所限制。

**核心逻辑链**：

1.  用 **RNN** 描述网络结构（变长序列生成）。
2.  用 **验证集准确率** 作为好坏标准。
3.  因过程不可导，用 **强化学习 (REINFORCE)** 来根据准确率调整 RNN，让它倾向于生成高分架构。
4.  虽然有效，但因每一次尝试都要从头训练子网络，导致**计算成本极高**。

这为后续的课程（如 Parameter Sharing / ENAS, Differentiable NAS / DARTS）确立了改进目标：**如何不训练子网络就能预估其性能？**

# 可微架构搜索

这份 PDF 是 Shusen Wang 教授关于 **神经架构搜索 (NAS)** 系列课程的第三部分，核心主题是 **可微神经架构搜索 (Differentiable NAS)**，重点解读了 **DARTS** (ICLR 2019) 和 **FBNet** (CVPR 2019) 这两篇里程碑式的工作。

这份课件解决了 NAS 领域最大的痛点：**将原本离散、不可导的搜索问题，转化为连续、可导的优化问题**，从而极大地提升了搜索效率。

以下是对该课件的深度分析与结构化解读：

### 1. 核心思想：从离散到连续 (Relaxation)

#### 1.1 传统 NAS 的局限

在上一讲（NAS_2）中，强化学习 (RL) 方法需要把每一个可能的架构都当作一个“黑盒”去训练和评估。因为架构选择是离散的（比如选 3x3 Conv 还是 5x5 Conv，非此即彼），所以无法直接计算梯度。

#### 1.2 Differentiable NAS 的突破

为了利用高效的梯度下降 (Gradient Descent)，DARTS 提出了 **Super-net (超网)** 的概念，把离散的选择变成了连续的权重。

- **基本设定**：
  - 预设 $N$ 个候选操作（Candidate Blocks），例如：3x3 Conv, 1x1 Conv, Max Pooling 等。
  - 预设网络总层数为 $L$（例如 20 层）。
  - **离散空间大小**：$N^L$（天文数字）。

---

### 2. 构建超网 (Super-net)

这是 Differentiable NAS 的灵魂所在。

#### 2.1 层的结构

不再是“每一层选一个操作”，而是**每一层包含所有操作**。

- 假设第 $l$ 层有 9 个候选 Block。
- 输入 $x$ 会**同时**进入这 9 个 Block。
- 每个 Block 都有自己的权重参数 $W$。

#### 2.2 软选择 (Soft Selection/Softmax)

引入一组新的**架构参数 (Architecture Parameters)** $\Theta$。

- 对于第 $l$ 层的第 $j$ 个 Block，分配一个参数 $\theta_j$。
- 使用 Softmax 计算选择权重 $\alpha$：
  $$ \alpha*j = \frac{e^{\theta_j}}{\sum*{k} e^{\theta_k}} $$
- **层的输出**：不再是某一个 Block 的输出，而是所有 Block 输出的**加权和**。
  $$ z = \sum\_{j=1}^{9} \alpha_j \cdot f_j(x; W_j) $$

**解读**：此时，网络架构不再是“确定的”，而是一个“混合体”。因为加权求和是可微的，所以 $\text{Loss}$ 对 $\theta$ 的梯度 $\frac{\partial \text{Loss}}{\partial \theta}$ 可以通过链式法则算出。

---

### 3. 训练与离散化 (Training & Discretization)

#### 3.1 联合训练

训练过程变成了同时优化两组参数：

1.  **网络权重 $\mathcal{W}$**：卷积核、全连接层权重等。
2.  **架构参数 $\Theta$**：决定每个 Block 重要性的参数。

目标函数：
$$ \min\_{\mathcal{W}, \Theta} \sum \text{CrossEntropy}(y, \text{Predict}(x; \mathcal{W}, \Theta)) $$
(注：DARTS 原文中是双层优化 Bilevel Optimization，课件为了简化理解描述为联合优化，本质都是利用梯度更新)。

#### 3.2 最终架构生成 (Pruning)

训练结束后，我们得到了一组训练好的 $\alpha$ 值（例如 $\alpha_1=0.7, \alpha_2=0.2, \dots$）。

- **动作**：对于每一层，直接选择 $\alpha$ 最大的那个 Block（argmax）。
- **结果**：抛弃其他 Block，剩下的就是最终搜索出来的离散网络架构。

---

### 4. 硬件感知与效率优化 (Hardware-Awareness / FBNet)

仅仅搜索准确率高的网络是不够的，工业界更关注**延迟 (Latency)**。FBNet (Facebook Berkeley Net) 在 Differentiable NAS 的框架下引入了效率约束。

#### 4.1 延迟的可微化

- **查表法**：预先在目标设备（如 iPhone 12）上测量每一个候选 Block 的实际运行时间，记为 $t_j$。
- **期望延迟**：由于超网的输出是加权和，该层的“期望延迟”也可以表示为加权和：
  $$ \text{Lat}(\Theta) = \sum*{l=1}^{20} \sum*{j=1}^{9} t*{l,j} \cdot \alpha*{l,j} $$
- **特性**：因为 $\alpha$ 是由 $\theta$ 通过 Softmax 计算的，所以 $\text{Lat}(\Theta)$ 关于 $\Theta$ 也是可微的！

#### 4.2 多目标优化

将延迟项加入损失函数：

- **加法形式**：$\text{Loss} + \lambda \cdot \text{Lat}(\Theta)$
- **乘法形式**：$\text{Loss} \cdot \log(\text{Lat}(\Theta))^\gamma$ (FBNet 采用)

这样，梯度下降会引导 $\Theta$ 向着“准确率高”且“延迟低”的方向更新。

---

### 5. 总结

这份课件清晰地阐述了 NAS 技术的演进逻辑：

- **Phase 1 (Random Search)**: 瞎蒙。成本高，效果差。
- **Phase 2 (RL-based)**: 让 AI 学会蒙。思路先进，但因为 Sampling 不可导，导致 Sample Efficiency 极低，计算成本极其昂贵（几百个 GPU days）。
- **Phase 3 (Differentiable NAS)**: **DARTS/FBNet**。
  - **核心技巧**：把“选哪个”变成“算概率分布的加权和”。
  - **收益**：把搜索问题变成了训练问题，可以直接用 SGD 优化。搜索时间从几百个 GPU days 缩短到几个 GPU hours。
  - **工业价值**：通过引入 Lookup Table Latency，可以直接搜索出适配特定芯片（如 DSP, NPU, Mobile CPU）的高效模型。

# RNN\_基础

根据从 PDF 中提取的内容，这份课件是王树森教授 CS583 深度学习课程关于 **循环神经网络 (RNN)** 的第二部分（`9_RNN_2.pdf`）。

这是一份非常典型的、循序渐进的教学课件。它从“为什么要用 RNN”讲起，介绍了 Simple RNN 的数学原理、代码实战（Keras），然后通过实验暴露了 Simple RNN 的缺陷（梯度消失/长依赖问题），最后引出了 LSTM。

以下是对这份课件的**深入解构与解读**：

### 1. 为什么需要 RNN？ (Motivation)

课件开篇直击痛点，对比了全连接网络 (FC Nets) 和卷积网络 (ConvNets) 在处理序列数据时的局限性：

- **局限性**：传统网络通常处理固定大小的输入（如一张图片）并产生固定大小的输出。它们往往将输入（如一段话）视为一个整体，忽略了内部的时序结构。
- **RNN 的优势**：RNN 天生适合建模序列数据（文本、语音、时间序列），因为它能按顺序处理输入，并维护一个“记忆”（State）。

### 2. Simple RNN 的核心原理

课件极其精简地概括了 Simple RNN 的数学模型。

- **结构**：这是一个循环结构，核心在于**Hidden State (隐藏状态)** $h$。
- **公式**：
  $$h_{t} = \tanh(A \cdot [h_{t-1}, x_{t}] + b)$$
  - **$h_t$**：当前的记忆。
  - **$h_{t-1}$**：上一时刻的记忆。
  - **$x_t$**：当前时刻的输入（例如当前单词的 Embedding）。
  - **$A$**：**共享参数矩阵**。这是 RNN 的关键，无论序列多长，参数矩阵 $A$ 始终只有这一个，这也是 RNN 参数量少的原因。
- **为什么用 tanh？**
  - 课件提出了一个深刻的问题：为什么激活函数选 `tanh` 而不是其他？
  - **解释**：为了数值稳定性。如果在循环中通过矩阵乘法 $h_t = A h_{t-1}$ 反复迭代：
    - 如果 $A$ 的特征值大于 1，状态值会爆炸（Exploding）。
    - 如果 $A$ 的特征值小于 1，状态值会消失趋近于 0（Vanishing）。
  - `tanh` 把值压缩在 $[-1, 1]$ 之间，有助于防止这种数值震荡和发散。

### 3. 实战：IMDB 电影评论情感分析 (Code Walkthrough)

课件使用了 Keras 框架演示了一个完整的 NLP 流程。

- **预处理**：
  1.  **Tokenization**: 建立 10,000 词的词典。
  2.  **Padding**: 截断或补全序列长度为 500 (`word_num=500`)。
- **模型构建 (Model 1)**：
  - Embedding 层：把单词 ID 变向量。
  - **SimpleRNN 层**：`state_dim=32`，`return_sequences=False`。这表示只取**最后一个时间步**的状态 $h_{last}$ 作为整句话的特征向量。
  - Dense 层：输出二分类结果。
- **结果**：
  - 验证集准确率约 **84.3%**。
  - 对比上一节课（`9_RNN_1`）中只用最后 20 个词的 Logistic Regression（约 75%），性能有显著提升。

### 4. 变体与调优尝试

课件展示了多种模型结构的尝试，这非常有教学意义，展示了深度学习中的“试错”过程：

- **尝试 1：堆叠 RNN (Stacked RNN)**
  - 使用了两层 RNN。第一层 `return_sequences=True`（输出所有时间步的状态给下一层），第二层 `return_sequences=False`。
  - **结果**：准确率 **84.7%**。并没有显著优于单层 RNN。这暗示了 Simple RNN 的瓶颈可能不在深度，而在其他地方。
- **尝试 2：使用所有时间步 (Visualize Global Features)**
  - 设置 `return_sequences=True`，得到所有 $h_1, h_2, ..., h_{500}$，然后把它们 Flatten 后喂给 Dense 层。
  - **结果**：准确率依然是 **84.7%**。这说明最后那个状态 $h_{last}$ 已经大概率包含了前面大部分有效信息，或者模型根本没法有效利用长序列的所有信息。

### 5. Simple RNN 的致命缺陷：短视 (Short-term Memory)

这是本课件最重要的理论转折点。

- **现象**：虽然 RNN 理论上能捕捉无限长的依赖，但实际上它**只能记住最近的信息**。
  - 例子 1（短期依赖）："clouds are in the **sky**"。RNN 能很好地预测，因为 "sky" 和 "clouds" 挨得很近。
  - 例子 2（长期依赖）："I grew up in **China**... (很长一段话) ... I speak fluent **Chinese**"。Simple RNN 很难预测 "Chinese"，因为它早把开头的 "China" 忘光了。
- **原因**：
  - 数学上的**梯度消失/爆炸**。
  - 随着时间步 $t$ 变大，早期的输入 $x_1$ 对当前状态 $h_t$ 的影响微乎其微（权重连乘导致的衰减）。
  - 引用了 Christopher Olah 的经典博客图解来说明这一点。

### 6. 总结与引出 LSTM

- **Simple RNN 总结**：
  - 参数量少（参数共享）。
  - 适合序列数据。
  - **最大痛点**：遗忘早期输入，无法处理长距离依赖 (Long-term Dependence)。
- **Next Step**：
  - 为了解决这个问题，我们需要引入更复杂的单元结构 —— **LSTM (Long Short-Term Memory)**，这将是下一节课的主题。

### 核心结论 (Takeaway)

这份 PPT 是从“朴素 RNN”到“现代 RNN (LSTM/GRU)”的过渡桥梁。它通过代码证明了 RNN 比全连接层有效，又通过理论分析证明了 Simple RNN 还是不够好（记不住长句子）。

**对于学习者的启示**：
在 NLP 任务中，几乎不会直接使用 `SimpleRNN`（因为它真的记不住）。如果任务很简单且序列短，用它也许行；但在绝大多数情况下，应当直接使用 **LSTM** 或 **GRU**，或者现在的 **Transformer**。

# 机器翻译

这篇 PDF 课件详细讲解了 **神经机器翻译（Neural Machine Translation, NMT）** 的基础架构——**Sequence-to-Sequence (Seq2Seq)** 模型。它是深度学习处理自然语言处理（NLP）任务的经典范式。

以下是对该课件内容的**深入分析与解构**：

### 1. 核心任务与数据流 (Task & Data Pipeline)

任务是将一种语言的序列翻译成另一种语言的序列（例如：英语 $\to$ 德语）。为了让神经网络处理文本，必须经过严谨的预处理：

- **双独立系统 (Dual Tokenizers & Dictionaries)**：
  - **原因**：源语言和目标语言的字符集（Alphabet）或词汇表（Vocabulary）完全不同（例如德语有特殊元音 Ä, Ö, Ü，英语没有）。
  - **操作**：分别建立 Source Dictionary 和 Target Dictionary。
- **Tokenization 级别**：
  - **Char-level (字符级)**：词汇表小（几十个字符），不需要 Embedding 层，用 One-Hot 即可；但序列会非常长，不仅训练慢，还容易导致 LSTM 遗忘（Vanishing Gradient）。
  - **Word-level (词汇级)**：序列短（平均缩短约 4.5 倍），保留语义更好；但词汇表巨大（几万+），必须使用 Embedding 层以避免参数爆炸。
- **特殊符号**：
  - `[start]` (通常用 `\t`)：告诉 Decoder “开始翻译”。
  - `[stop]` (通常用 `\n`)：告诉 Decoder “翻译结束”。

### 2. 模型架构：Seq2Seq (The Anatomy)

Seq2Seq 的核心在于**Encoder** 和 **Decoder** 的配合，中间通过**状态向量 (Context Vector)** 传递信息。

#### **A. Encoder (编码器)**

- **角色**：阅读理解。
- **行为**：读取源句子（如 "go away"）。
- **输出**：它**不输出**预测结果，而是输出它最终的 **内部状态 (Hidden States $\mathbf{h}$ & Cell States $\mathbf{c}$)**。
- **意义**：这个最终状态被认为是对整个源句子的“语义压缩”或“思维向量”。

#### **B. Decoder (解码器)**

- **角色**：生成翻译。
- **行为**：将 Encoder 的最终状态 $\mathbf{(h, c)}$ 作为自己的**初始状态**（就像接过了接力棒）。
- **特殊的输入**：Decoder 的第一个输入永远是 `[start]` 符号。

### 3. 训练与推理的本质区别 (Training vs. Inference)

文档非常清晰地展示了 Seq2Seq 在这两个阶段有着完全不同的运作模式，这是初学者最容易混淆的地方：

#### **训练阶段 (Training) —— Teacher Forcing**

- **输入**：源句子 + 目标句子（带有 `[start]`）。
- **标签**：目标句子（移位后的，即下一个字符）。
- **关键点**：在训练时，不管上一时刻模型预测得对不对，**下一时刻输入给 Decoder 的都是正确的真实字符**。这被称为 "Teacher Forcing"。
  - _例子_：即使模型在第一步把 "m" 预测成了 "x"，第二步输入给模型的依然是正确的 "a"（来自 "mach"），强行矫正。

#### **推理阶段 (Inference) —— Autoregressive (自回归)**

- **输入**：源句子。
- **行为**：
  1.  Encoder 跑完，状态传给 Decoder。
  2.  喂给 Decoder `[start]`。
  3.  Decoder 吐出第一个字符（例如 "m"）。
  4.  **关键点**：将**上一步生成的 "m"** 喂回给 Decoder 作为下一步的输入。
  5.  循环往复，直到模型生成 `[stop]` 符号。
- **风险**：一步错，步步错（Error Propagation）。

### 4. 改进策略 (How to improve?)

针对基础 LSTM Seq2Seq 的缺陷，课件提出了三个改进方向：

1.  **Bi-LSTM (双向 LSTM)**：
    - **只用于 Encoder**。普通的 LSTM 读到句子末尾时可能忘了开头（尤其当句子很长时）。Bi-LSTM 同时从左向右和从右向左读，最后的状态汇聚了整个句子的全景信息。
2.  **Word-Level Tokenization**：
    - 用词代替字符，缩短序列长度，减轻长距离依赖问题。
3.  **Multi-Task Learning (多任务学习)**：
    - 让同一个 Encoder 为多个不同的 Decoder（翻译德语、翻译法语、甚至翻译回英语）提供状态。这能强迫 Encoder 学出更通用的语言表示，起到数据增强的作用。

### 总结

这份课件主要讲述了**不带 Attention 机制**的最原始 seq2seq 模型。它的核心逻辑是：**将源语言编码成一个固定大小的向量，再解码成目标语言**。这种“固定大小向量”不仅是连接两端的桥梁，也是该模型的瓶颈（Information Bottleneck），这就是后续引入 Attention 机制所要解决的问题（课件最后也预告了这一点）。

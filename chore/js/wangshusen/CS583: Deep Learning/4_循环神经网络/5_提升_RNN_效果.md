# 提升*RNN*效果

这节课由王树森老师讲解，主题是 **提升 RNN 性能的三种高级技巧**：**Stacked RNN（多层堆叠 RNN）**、**Bidirectional RNN（双向 RNN）** 以及 **Pretraining（预训练）**。这些技巧旨在解决单一 LSTM 层的容量限制和信息提取不足的问题。

以下是逻辑清晰、深入且不遗漏的分析与讲解：

### 第一部分：Stacked RNN (多层堆叠 RNN)

不仅全连接层和卷积层可以从浅变深，RNN 也可以从单层变为多层，从而构建“深度循环神经网络”。
![alt text](image-3.png)

#### 1. 结构原理

- **层级结构**：
  - **Layer 1**：接收原始输入（词向量序列），计算出状态序列 $h^{(1)}_1, h^{(1)}_2, ..., h^{(1)}_T$。
  - **Layer 2**：将 Layer 1 输出的状态序列作为自己的输入，计算出更高层的状态序列。
  - **Layer 3**：依此类推...
- **特征提取的层级性**：
  - 底层 RNN 提取低级特征（如词性、短语结构）。
  - 高层 RNN 基于底层特征，提取更抽象的语义特征（如情感极性、意图）。
- **最终输出**：通常取**最后一层**的最后一个状态 $h^{(L)}_T$ 作为整个句子的特征向量，喂给全连接层分类。

#### 2. Keras 实现细节（关键点：`return_sequences`）

在堆叠 RNN 时，有一个很容易出错的参数设置：

- **中间层 (Layer 1, Layer 2)**：必须设置 `return_sequences=True`。
  - 原因：下一层 RNN 需要接收一个**时间序列**作为输入，而不是单个向量。
- **最后一层 (Layer 3)**：通常设置 `return_sequences=False`。
  - 原因：我们只需要该层输出的最终状态向量来进行分类（Many-to-One 模式）。

#### 3. 实验观察

- 在老师的演示中，Stacked LSTM 并未带来显著提升。
- **原因分析**：瓶颈可能不在于 RNN 层的深度，而在于**数据量不足**以及 **Embedding 层的过拟合**。仅仅增加网络深度在小数据集上反而可能加剧过拟合。

---

### 第二部分：Bidirectional RNN (双向 RNN)

Simple RNN 和标准 LSTM 都是从左到右阅读文本，这存在“偏见”。

#### 1. 单向 RNN 的局限

- **阅读习惯**：从左向右读取。
- **问题**：
  - 如果在句尾，状态 $h_T$ 包含了很多句尾的信息，但可能已经忘记了句首的信息（Despite LSTM's improvement）。
  - 有些语言结构需要后文信息才能理解前文（例如指代消解）。

#### 2. 双向结构原理

- **两条独立的链**：
  - **Forward RNN**：从左到右处理 ($x_1 \rightarrow x_T$)，输出状态 $h_T$。
  - **Backward RNN**：从右到左处理 ($x_T \rightarrow x_1$)，输出状态 $h'_T$。
- **参数独立**：前向和后向 RNN **不共享** 任何参数。
- **特征融合**：将 $h_T$ (前向最后状态) 和 $h'_T$ (后向最后状态) **拼接 (Concatenate)** 起来，形成一个 $2 \times d_h$ 维的向量。
- **优势**：捕捉了完整的上下文信息（Both Past and Future context），消除了“遗忘句首”的问题。

#### 3. Keras 实现

- 使用 `Bidirectional` 包装器：
  ```python
  model.add(Bidirectional(LSTM(32)))
  ```
- **参数量翻倍**：因为有两套独立的 LSTM 参数。
- **输出维度翻倍**：如果是 32 维 LSTM，双向输出就是 64 维。

---

### 第三部分：Pretraining (预训练) —— 解决过拟合的大杀器

这是这节课提到的最实用的工程技巧，直接针对 Embedding 层过拟合的问题。

#### 1. 问题根源

- **参数分布不均**：
  - RNN 层参数很少（几千到几万）。
  - Embedding 层参数巨大（词表大小 $\times$ 维度，例：$10000 \times 32 = 320,000$）。
- **过拟合风险**：在只有 2 万条训练数据的情况下，要训练好 32 万个参数是非常困难的。Embedding 层会死记硬背训练集中的特定词汇，导致泛化能力差。

#### 2. 预训练解决方案

- **Step 1：找大数据**
  - 寻找一个类似任务（如其他大规模情感分析数据集，或维基百科等无监督语料）。
- **Step 2：预训练 Embedding**
  - 在大数据集上训练一个神经网络（结构不限，只要有 Embedding 层）。
  - 目标是学到通用的词向量（Word Vectors），让语义相近的词在空间中距离相近。
- **Step 3：迁移学习 (Transfer Learning)**
  - 将训练好的 Embedding 矩阵“移植”到现在的任务中。
  - **冻结 (Freeze)**：通常在微调初期固定 Embedding 层参数，**不让它更新**，只训练上层的 RNN 和分类器。

#### 3. 意义

这是现代 NLP（如 BERT, GPT）的前身思想。通过预训练，我们将通用的语言知识注入到模型中，解决了小样本下的大参数训练难题。

---

### 第四部分：课程内容总结 (Takeaways)

王老师最后对 RNN 系列课程做了一个整体回顾：

1.  **模型演进**：Simple RNN (易梯度消失) $\rightarrow$ LSTM (引入传输带，首选方案) / GRU。
2.  **通用增强技巧**：
    - **Stacked RNN**：增加深度，提取抽象特征（需大数据支持）。
    - **Bi-directional RNN**：双向阅读，融合左右上下文（推荐默认使用）。
    - **Pretraining**：`在大数据上预训练 Embedding 层，解决小数据过拟合问题（最有效的工程手段）。`

**最终建议**：在实际工程中，**LSTM/GRU + Bidirectional + Pretrained Embedding** 是处理文本序列任务的标准基线组合（在 Transformer 统领天下之前）。

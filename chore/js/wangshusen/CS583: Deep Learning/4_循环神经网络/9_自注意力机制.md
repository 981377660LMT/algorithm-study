# 自注意力机制

这篇 PDF 课件深入浅出地讲解了 **自注意力机制 (Self-Attention)** 在循环神经网络 (RNN) 中的应用。可以在后续的 Transformer 架构中看到这一思想的终极形态，但在本课件中，它被设计为 RNN 的一个增强补丁。

以下是对该课件的**深入分析与解构**：

### 1. 核心背景：为什么需要 Self-Attention？

- **原版 RNN 的痛点**：标准的 RNN（包括 SimpleRNN 和 LSTM）是线性的。第 $t$ 步的状态 $h_t$ 仅依赖于上一步的状态 $h_{t-1}$。随着序列变长，早期的信息在不断传递的过程中容易丢失（遗忘问题）。
- **Self-Attention 的解法**：不再完全依赖“上一步传下来的记忆”，而是让当前时刻的模型拥有“**回溯整个历史**”的能力。哪怕序列再长，它也可以直接去查看和引用第 1 步的信息。

### 2. 模型架构解构：RNN + Self-Attention

课件作者为了简化教学，构造了一个 **SimpleRNN + Self-Attention** 的变体模型。它的运作方式与标准 RNN 有显著不同。

#### **A. 状态定义的改变**

- **标准 RNN**：$h_t = \tanh(A \cdot [x_t, h_{t-1}] + b)$。
  - 依靠 $h_{t-1}$ 把历史信息“背”下来传给现在。
- **Self-Attention RNN**：$h_t = \tanh(A \cdot [x_t, \mathbf{c}_{t-1}] + b)$。
  - 这里引入了一个新的变量 **$\mathbf{c}$ (Context Vector)** 来替代 $h$ 传递信息。

#### **B. 运作流程 (Step-by-Step)**

假设我们处理序列 $x_1, x_2, x_3, \dots$

1.  **计算当前的以 $h_t$ (Query)**

    - 输入：当前的词 $x_t$ 和上一步的上下文 $\mathbf{c}_{t-1}$。
    - 输出：当前时刻的隐状态 $h_t$。
    - _此时，$h_t$ 仅仅融合了当前词和上一时刻的上下文，还没经过 Attention 的增强。_

2.  **计算 Self-Attention (The "Self-Scan")**

    - **对象**：拿当前的 $h_t$ 去跟**自己以及之前所有时刻**的隐状态 $(h_1, h_2, \dots, h_t)$ 进行比对。
    - **对齐 (Alignment)**：计算相关性分数 $\alpha_i = \text{align}(h_t, h_i)$。
    - **归一化**：通过 Softmax 得到权重，和为 1。
    - _直观理解_：模型问自己：“为了理解当前的 $x_t$，通过之前的历史记录 $(h_1 \dots h_{t-1})$，我应该重点关注哪一个？”

3.  **生成新的上下文 $\mathbf{c}_t$ (Context Update)**

    - 公式：$\mathbf{c}_t = \sum_{j=1}^{t} \alpha_j h_j$。
    - **关键点**：$\mathbf{c}_t$ 是历史记忆的加权总和。如果 $x_t$ 与 $x_1$ 关系密切（比如代词指代），那么 $\alpha_1$ 会很大，信息流就直接从 $t=1$ 跳跃到了 $t$。

4.  **传递**
    - 将 $\mathbf{c}_t$ 传给下一步，用于计算 $h_{t+1}$。

### 3. 可视化理解 (Slide 33)

课件通过一个 NLP 例子 _"The FBI is chasing a criminal on the run"_ 解释了其实际效果：

- 当模型读到 **"chasing"** 这个词时，Self-Attention 机制不仅关注了当前的动作，还回头给了 **"FBI"** 极高的权重（红色高亮）。
- 这意味着模型捕捉到了 _Agent (施动者)_ 和 _Action (动作)_ 之间的长距离依赖关系，即使它们中间隔了单词。

### 4. 总结与权衡

- **优势**：
  - **更强的记忆力**：理论上解决了长距离依赖问题，信息传递路径长度从 $O(t)$ 变成了 $O(1)$（可以直接访问历史）。
  - **上下文感知**：动态地根据当前内容捕捉相关的历史背景。
- **代价 (Complexity)**：
  - 标准 RNN 处理长为 $T$ 的序列，计算量是 $O(T)$。
  - Self-Attention RNN 每一步都要回头看所有历史，第 $t$ 步看 $t$ 次，总计算量变成了 $1+2+\dots+T = O(T^2)$。
  - 这大大增加了计算负担，但换来了更好的性能。

这个课件描述的模型其实是 **Transformer** 诞生前夕的过渡形态（引用了 2016 年 EMNLP 的论文）。它在 RNN 的骨架上嫁接了 Attention，为后来彻底抛弃 RNN、纯粹使用 Self-Attention 的 Transformer 架构铺平了道路。

# 长短期记忆网络

这份课件 **`9_RNN_3.pdf`** 是王树森教授 CS583 深度学习课程关于 **LSTM (Long Short-Term Memory)** 的核心章节。

它是对上一节课 Simple RNN 提出问题的“解答篇”。Simple RNN 因为梯度消失无法处理长序列，而 LSTM 通过精巧的门控机制解决了这个问题。

以下是对这份课件的**深度解构与解读**：

### 1. 核心隐喻：传送带 (The Conveyor Belt)

课件借用了 Christopher Olah 的经典比喻，将 LSTM 的核心组件 —— **Cell State ($C$)** 比作一条**传送带**。

- **Simple RNN 的痛点**：记忆 ($h$) 在传递过程中会被不断地乘上权重矩阵 $A$。如果 $A<1$，记忆像沙子一样流失；如果 $A>1$，记忆像滚雪球一样爆炸。
- **LSTM 的创新**：
  - **Cell State ($C$)** 是一条贯穿所有时间步的“高速公路”。
  - 过去的信息 $C_{t-1}$ 可以**直接**流向未来，只有少量的线性交互（加法和乘法）。
  - 这种设计保证了梯度可以长时间不衰减地反向传播，从而捕捉**长距离依赖**。

### 2. 内部解剖：四个关键组件

Simple RNN 只有一个 $tanh$ 层，而 LSTM 内部有四个交互的层（通常称为“门”），课件通过数学公式和图解详细拆解了它们：

#### A. 遗忘门 (Forget Gate $f$)

- **作用**：决定要从传送带上**丢弃**什么信息。
- **公式**：$f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)$
- **解读**：输出 0 到 1 之间的数值。**0** 代表“完全遗忘”（关门），**1** 代表“完全保留”（开门）。比如在文本生成中，如果在 $x_t$ 看到了句号，可能就需要遗忘上一句话的主语。

#### B. 输入门 (Input Gate $i$) 与 新值 ($\tilde{C}$)

- **作用**：决定要向传送带上**添加**什么新信息。
- **两步走**：
  1.  **Input Gate ($i_t$)**：决定**更新哪些值**（Sigmoid，$0 \sim 1$）。
  2.  **New Candidate ($\tilde{C}_t$)**：计算出**待更新的新信息**（Tanh，$-1 \sim 1$）。

#### C. 更新传送带 (Update Cell State)

- **核心公式**：
  $$C_t = f_t \circ C_{t-1} + i_t \circ \tilde{C}_t$$
- **解读**：
  - **旧的**乘以遗忘门（$f_t \circ C_{t-1}$）：把没用的忘掉。
  - **加上**新的乘以输入门（$i_t \circ \tilde{C}_t$）：把有用的填进去。
  - 注意这里是**加法**运算，这是梯度能长距离传播的关键（加法运算的导数是常数，不会由连乘导致梯度消失）。

#### D. 输出门 (Output Gate $o$)

- **作用**：基于当前的 Cell State，决定最终输出什么给 Hidden State ($h_t$)。
- **公式**：
  $$o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)$$
  $$h_t = o_t \circ \tanh(C_t)$$
- **解读**：Cell State 可能包含了太多信息，Hidden State 只需要输出当前时刻下游任务关心的那部分。

### 3. 参数量分析 (Parameter Count)

这是一个很重要的考点。

- Simple RNN 只有 1 个权重矩阵。
- LSTM 有 **4** 个权重矩阵（对应 $f, i, \tilde{C}, o$）。
- **参数量公式**：
  $$\text{Params} = 4 \times [ \text{shape}(h) \times (\text{shape}(h) + \text{shape}(x)) + \text{shape}(h) ]$$
  _(主要看第一项，偏置项通常忽略不计或占比较小)_
- **结论**：同等隐层维度下，LSTM 的参数量是 Simple RNN 的 **4 倍**，计算开销也更大。

### 4. 实战对比：IMDB 情感分析

课件最后展示了在同样的任务上将 `SimpleRNN` 替换为 `LSTM` 的效果。

- **代码变更**：
  ```python
  # from
  model.add(SimpleRNN(state_dim))
  # to
  model.add(LSTM(state_dim))
  ```
- **实验结果**：
  - **Simple RNN**: 测试集准确率约 **84%**。
  - **LSTM**: 测试集准确率提升至 **88.6%**。
  - **结论**：显著优于 Simple RNN，证明了其处理长文本（IMDB 评论通常较长）的能力。
- **防过拟合**：展示了在 LSTM 中使用 `dropout`（如 `dropout=0.2`）可以进一步稳定训练，防止在小数据集上过拟合。

### 5. 总结

这份 PPT 完美诠释了**“增加复杂度以换取性能”**的工程哲学。
LSTM 牺牲了计算效率（参数量 x4，计算逻辑复杂），换取了强大的**记忆保持能力**。虽然现在的 Transformer 已经在很多领域取代了 LSTM，但在需要处理流式数据、资源受限或小样本的序列任务中，LSTM 的**“传送带 + 门控”**思想依然是极其经典和有效的。

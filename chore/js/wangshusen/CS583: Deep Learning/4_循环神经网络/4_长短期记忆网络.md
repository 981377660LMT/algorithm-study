# 长短期记忆网络

这节课由王树森老师讲解，重点介绍 **LSTM (Long Short-Term Memory，长短期记忆网络)** 及其在 Keras 中的实现。LSTM 是为了解决 Simple RNN “健忘”（梯度消失）问题而设计的经典模型。

以下是对课件内容的深入逻辑分析与讲解：

### 第一部分：LSTM 的核心设计理念

Simple RNN 的最大痛点是**梯度消失（Vanishing Gradient）**，导致它记不住长距离的依赖关系（记忆像金鱼一样短）。LSTM 在 1997 年提出，通过引入一套复杂的“门控机制”来精准控制信息的遗忘与保留。

#### 1. 核心创新：传输带（Cell State, $C$）

- **概念**：这是 LSTM 的“灵魂”。可以把它想象成一条贯穿所有时间步的**高速公路**。
- **作用**：过去的信息 $C_{t-1}$ 可以通过这条传输带，几乎无损地传递到下一个时刻 $C_t$。
- **数学意义**：正是因为这条传输带的存在，梯度在反向传播时可以顺畅地流回很久以前的节点，从而**避免了梯度消失**，实现了长期记忆。

---

### 第二部分：LSTM 的内部结构（四个关键组件）

LSTM 的每一个时间步内部包含四个关键运算模块（三个门 + 一个候选记忆），这比 Simple RNN 只有一个简单的 $tanh$ 层要复杂得多。

#### 1. 遗忘门 (Forget Gate, $f_t$) —— “决定丢弃什么”

- **功能**：观察当前输入 $x_t$ 和上一时刻的状态 $h_{t-1}$，决定从旧的传输带 $C_{t-1}$ 中删掉哪些信息。
- **计算逻辑**：
  - $f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)$
  - **Sigmoid 函数 ($\sigma$)**：输出值在 0 到 1 之间。
  - **语义**：
    - $0$ 代表“完全遗忘/阻断”。
    - $1$ 代表“完全保留/通过”。
- **操作**：计算出的 $f_t$ 与旧传输带 $C_{t-1}$ 进行 **Element-wise Multiplication（按元素乘法）**。如果 $f_t$ 某位置是 0，则该位置的历史记忆被抹除。

#### 2. 输入门 (Input Gate, $i_t$) —— “决定更新什么”

- **功能**：决定哪些新的信息（来自当前输入 $x_t$）值得存储到传输带中。
- **计算逻辑**：
  - $i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)$
  - 同样使用 Sigmoid，输出 0~1，作为“写入强度”的系数。

#### 3. 新候选值 (New Candidate Value, $\tilde{C}_t$) —— “准备写入的内容”

- **功能**：根据当前输入生成待写入的信息向量。
- **计算逻辑**：
  - $\tilde{C}_t = \tanh(W_c \cdot [h_{t-1}, x_t] + b_c)$
  - **Tanh 函数**：将值压缩在 $[-1, 1]$ 之间，用于规范化数值。

> **传输带更新公式 (核心)**：
> $C_t = f_t \times C_{t-1} + i_t \times \tilde{C}_t$
>
> - **逻辑解释**：新状态 = (旧状态 $\times$ 遗忘比例) + (新内容 $\times$ 写入比例)

#### 4. 输出门 (Output Gate, $o_t$) —— “决定通过什么”

- **功能**：基于更新后的传输带 $C_t$，决定当前时刻的隐状态 $h_t$ 输出什么。
- **计算逻辑**：
  - $o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)$ (控制输出比例)
  - $h_t = o_t \times \tanh(C_t)$
  - **双重输出**：$h_t$ 一份传给下一个时刻，一份作为当前层的输出（喂给分类器或深层网络）。

---

### 第三部分：参数量计算（必考点）

Simple RNN 只有 1 个参数矩阵，而 LSTM 有 **4 个**（对应 $f, i, \tilde{C}, o$ 这四个组件）。

假设：

- $d_h$：Hidden state / Output 维度 (如 32)
- $d_x$：Input 词向量维度 (如 32，注意这只是巧合，通常不同)

计算公式推导：

1.  每个门的输入都是拼接收量 $[h_{t-1}, x_t]$，维度为 $d_h + d_x$。
2.  每个门的输出维度都是 $d_h$。
3.  所以**一个门**的参数矩阵大小为：$d_h \times (d_h + d_x)$。
4.  再加上 Bias（偏置项），一个门有 $d_h$ 个偏置。
5.  **LSTM 总参数量** = $4 \times [d_h \times (d_h + d_x) + d_h]$

**课件中的算例**：

- 输入维度 $d_x = 32$，状态维度 $d_h = 32$。
- 单门参数：$32 \times (32+32) + 32 = 2080$。
- 总参数：$2080 \times 4 = 8320$。

---

### 第四部分：Keras 实战

课件演示了将上一节的 SimpleRNN 替换为 LSTM，代码改动极小，但效果提升明显。

#### 1. 代码实现

```python
# 伪代码示例
model = Sequential()
model.add(Embedding(input_dim=10000, output_dim=32))
# 唯一的改动：将 SimpleRNN 换成 LSTM
model.add(LSTM(units=32, return_sequences=False))
model.add(Dense(1, activation='sigmoid'))
```

- 这里 `units=32` 指的就是 $h$ 和 $C$ 的维度。
- `return_sequences=False` 表示只输出序列因后一个时间步的 $h_t$（作为整句话的特征向量）。

#### 2. 实验结果与过拟合分析

![alt text](image-2.png)

- **准确率提升**：Simple RNN (84%) $\rightarrow$ LSTM (88%左右)。证明 LSTM 能捕捉更长距离的依赖。
- **Dropout 无效的原因**：
  - 实验发现加了 Dropout 也没用。
  - **根本原因**：过拟合的源头不是 LSTM 层（仅 8000 多参数），而是 **Embedding 层**。
  - Embedding 层参数：$10,000 \text{ (词表)} \times 32 \text{ (维度)} = 320,000$。
  - **结论**：参数量的大头在词向量上，**要解决过拟合应该主要针对 Embedding 层做正则化**，而不是 LSTM 层。

### 总结

这节课完成了从“理论缺陷”到“工程解决方案”的闭环：

1.  **痛点**：RNN 的 $tanh$ 连乘导致梯度消失，无法记忆长句（如“I grew up in China ... I speak fluent **Chinese**”）。
2.  **方案**：LSTM 引入 **Cell State (传输带)** 作为信息高速公路，并用 **3 个门** (遗忘、输入、输出) 精细化管理信息的流动。
3.  **代价**：计算复杂度增加，参数量是同规模 Simple RNN 的 4 倍。
4.  **实战结论**：在处理文本序列时，**永远优先选择 LSTM (或 GRU)**，不要用 Simple RNN。

# 注意力机制

这篇 PDF 课件深入剖析了 **Seq2Seq 模型中的 Attention（注意力）机制**。它是自然语言处理（NLP）发展史上最重要的里程碑之一，打破了长序列处理的瓶颈，并为后来的 Transformer 奠定了基础。

以下是对该课件的**深入分析与解构**：

### 1. 核心痛点：长序列遗忘 (The Bottleneck)

在引入 Attention 之前，标准的 Seq2Seq 模型（Encoder-Decoder 架构）存在一个致命缺陷：

- **信息压缩瓶颈**：Encoder 必须将整个输入序列（无论多长）的所有信息压缩到一个**固定长度的向量**（Encoder 的最后一个隐藏状态 $\mathbf{h}_m$）中。
- **后果**：Decoder 只能通过这个“这一口”状态来生成整个翻译结果。如果句子很长（如 50 个单词），这个向量无法承载所有细节，导致前面输入的信息被“遗忘”，翻译质量大幅下降（BLEU 分数暴跌）。

### 2. 解决方案：Attention 机制 (The Fix)

Attention 的核心思想是**“不再只靠记忆，而是边做边看”**。

- **改进**：Decoder 不再仅仅依赖初始状态，而是在生成的**每一步**，都允许它“回头看”一眼 Encoder 的所有隐藏状态。
- **聚焦**：Decoder 会根据当前需要翻译的内容，自动计算出应该“重点关注”Encoder 中的哪些单词（状态）。

### 3. 技术解构：Attention 的计算三部曲

课件非常详细地拆解了在第 $t$ 步生成时，Attention 是如何运作的。这通常被称为“加性注意力”或“点积注意力”（课件中提到了两种变体）。

#### **Step 1: 计算对齐/分数 (Alignment/Attention Weights)**

Decoder 需要知道当前时刻（状态 $\mathbf{s}_{t-1}$）与 Encoder 的每一个历史状态（$\mathbf{h}_1, \dots, \mathbf{h}_m$）有多相关。

- **输入**：Decoder 当前状态 $\mathbf{s}$，Encoder 所有状态 $\mathbf{h}$。
- **计算方式**：
  - **Option 1 (原始论文)**：用一个小型的神经网络（tanh 激活）来评分。
  - **Option 2 (更流行，Transformer 基础)**：计算点积。
    - Query ($\mathbf{q}$) $\leftarrow$ Decoder 状态
    - Key ($\mathbf{k}$) $\leftarrow$ Encoder 状态
    - $Score = \mathbf{q} \cdot \mathbf{k}$
- **归一化**：使用 **Softmax** 将分数转换为概率分布 $\alpha_1, \dots, \alpha_m$，这里 $\sum \alpha_i = 1$。
  - _解读_：$\alpha_i$ 代表“在这个时刻，我应该分配 **$\alpha_i$** 的注意力给第 $i$ 个输入单词”。

#### **Step 2: 计算上下文向量 (Context Vector)**

根据计算出的权重，对 Encoder 的所有状态进行加权求和。
$$ \mathbf{c}_t = \sum_{j=1}^{m} \alpha\_{j} \mathbf{h}\_j $$

- _解读_：$\mathbf{c}_t$ 是一个动态变化的向量。在翻译 "cat" 时，它可能包含了 90% 的 "猫" 的特征；在翻译 "sat" 时，它又变成了包含 90% "坐" 的特征。

#### **Step 3: 更新 Decoder 状态**

将上下文向量 $\mathbf{c}_t$ 与 Decoder 的输入（或上一时刻状态）结合，送入 RNN 单元，生成新的状态 $\mathbf{s}_t$ 和输出。

### 4. 复杂度分析 (The Price)

Attention 并不是免费的午餐，它用**计算换取了精度**。

- **无 Attention**：计算量是线性的 $O(m + t)$。Encoder 扫一遍，Decoder 扫一遍。
- **有 Attention**：计算量是乘积关系 **$O(m \times t)$**。
  - Decoder 每走一步（共 $t$ 步），都要去扫描一遍 Encoder 的所有状态（共 $m$ 个）。
  - 如果不做优化，Attention 是计算密集型的。

### 5. 可视化 (Visualization)

课件最后展示了经典的 **Attention Heatmap (热力图)**：

- 矩阵的 $(i, j)$ 元素代表 Target 的第 $i$ 个词关注 Source 第 $j$ 个词的程度。
- **对角线现象**：通常翻译是顺序对应的。
- **非对角线高亮**：处理语序不同（如英语倒装句翻译成法语）时，模型能自动跳跃关注到正确的位置，证明它理解了语法结构。

### 总结

这份课件揭示了 Seq2Seq 从“死记硬背”（固定向量）进化到“活学活用”（Attention 动态聚焦）的过程。**Context Vector $\mathbf{c}$** 不再是静态的，而是随着生成过程每一毫秒都在动态重组，这使得模型拥有了处理超长文本的能力。

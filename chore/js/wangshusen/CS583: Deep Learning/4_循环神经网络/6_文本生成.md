# 文本生成

这篇 PDF 课件的主要内容是关于 **使用循环神经网络（特别是 LSTM）进行字符级文本生成（Character-level Text Generation）**。它是深度学习课程中关于 RNN 应用的一个章节。

以下是对该课件的**深入分析与解构**：

### 1. 核心任务：预测下一个字符 (Predict the Next Char)

整个任务被定义为一个**分类问题**。

- **输入 (Input)**：一个固定长度的文本片段（Segment），长度记为 $l$。
- **输出 (Output)**：该片段紧接着的**下一个字符**（Next Char）。
- **词汇表 (Vocabulary)**：所有可能出现的唯一字符集合，大小记为 $v$（例如包含大小写字母、标点、空格等）。

### 2. 数据预处理 (Data Preparation)

机器学习模型无法直接理解文本，需要进行数值化转换：

1.  **切分数据 (Partitioning)**：将长文本切分为无数个 `(segment, next_char)` 样本对。
    - _例子_：如果文本是 "the cat sat on the"，且 $l=4$。
    - 样本 1: Input="the ", Output="c"
    - 样本 2: Input="he c", Output="a"
    - ...以此类推，窗口滑动。
2.  **独热编码 (One-hot Encoding)**：
    - 每个**字符**被表示为一个长度为 $v$ 的向量（只有一个位置是 1，其余为 0）。
    - 每个**输入片段**（Segment）被表示为一个 $l \times v$ 的矩阵。

### 3. 模型架构 (Model Architecture)

这是一个典型的 "Many-to-One" 结构用于训练，但在生成时变成循环调用：

- **输入层**：接收 $l \times v$ 的矩阵。
- **LSTM 层**：核心层，负责捕捉序列中的上下文信息和长期依赖关系。
- **Dense 层 (全连接层)**：将 LSTM 的输出映射到 $v$ 维空间。
- **Softmax 激活函数**：输出一个概率分布 (Probability Distribution)，表示下一个字符也是词汇表中每个字符的概率。
- **损失函数**：Categorical Cross Entropy（多分类交叉熵）。

### 4. 生成策略 (Text Generation Strategy)

训练好的模型会输出下一个字符的概率分布，如何根据这个分布选择字符是生成质量的关键：

- **策略 1：贪婪选择 (Greedy Selection)**
  - 直接选择概率最大的字符。
  - _缺点_：容易陷入重复循环，缺乏创造性，生成的文本非常枯燥。
- **策略 2：多项式采样 (Sampling from Multinomial Distribution)**
  - 严格按照预测出的概率分布进行随机采样。
  - _缺点_：可能太随机，容易生成拼写错误的单词或乱码。
- **策略 3：温度控制采样 (Temperature Sampling)** —— **推荐方案**
  - 引入一个超参数 **Temperature ($T$)** 来调整概率分布：
    $$ P*{new} \propto P*{old}^{\frac{1}{T}} $$
  - **$T \to 0$ (低温度)**：分布变得尖锐，高概率的字符概率更高，接近贪婪算法（生成内容保守、准确用于拼写，但重复度高）。
  - **$T \to \infty$ (高温度)**：分布变得平坦，字符选择更加随机（生成内容更具“创造性”，但也更多错误）。
  - _课件展示_：$T=0.2$ 时非常保守，$T=0.5$ 时平衡较好。

### 5. 生成过程演示 (The Process)

文档展示了一个循环生成的伪代码逻辑：

1.  提供一个 **种子文本 (Seed)**，例如 "the cat sat on the"。
2.  将 Seed 编码后输入模型。
3.  模型输出概率，根据 Temperature 采样得到下一个字符（例如 'm'）。
4.  将 'm' 追加到 Seed 末尾，并丢弃 Seed 最前面的字符（保持长度不变）。
5.  重复上述步骤。

### 6. 实战效果 (Examples)

课件展示了模型训练不同 Epoch 后的生成效果，这体现了神经网络的学习过程：

- **1 Epoch**：生成的文本几乎没有意义，单词拼写错误多（如 "believest constive"），仅仅学到了字符组合的一些表面统计规律。
- **5 Epochs**：单词拼写开始正确，出现常见词汇（"power", "standard"），但句子语法和语义仍不通顺。
- **20 Epochs**：生成的文本在结构上非常接近英语，单词拼写准确，甚至能模仿出某种文风（哲学或宗教文本的风格），尽管深层逻辑可能仍然是无意义的。

### 总结

这份课件是一个标准的 **LSTM 字符级文本生成** Tutorial。它通过解构“训练”和“生成”两个阶段，重点解释了**数据如何向量化**以及**如何使用 Temperature 平衡生成的准确性与多样性**。

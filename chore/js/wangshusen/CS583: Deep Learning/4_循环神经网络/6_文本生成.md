# 文本生成

这节课由王树森老师讲解，深入探讨了 **RNN 的一项经典应用——文本生成（Text Generation）**。课程从技术原理、数据预处理、模型构建到最终的生成策略，进行了非常系统且实操性强的讲解。

以下是逻辑清晰、深入且不遗漏的分析：

---

### 第一部分：文本生成的应用场景与基本原理

#### 1. 什么是文本生成？

- **核心思想**：利用 RNN 模型学习文本数据的概率分布，从而自动生成符合该分布的文本。
- **风格迁移**：模型的“文风”取决于训练数据。
  - 喂《莎士比亚》 $\rightarrow$ 生成古英语风格戏剧。
  - 喂 Linux 内核源码 $\rightarrow$ 生成“看起来像 C 语言”的代码（虽然编译不过，但语法结构相似）。

#### 2. 工作原理：逐字预测 (Character-level Prediction)

![alt text](image-4.png)
这节课使用的是 **基于字符（Character-level）** 而非单词的生成模型。

- **输入**：一段文本片段（Sequence），如 `"the cat sat on the"`.
- **任务**：预测**下一个字符**是什么？
- **过程**：
  1.  输入 Sequence 进 RNN。
  2.  RNN 最后一个状态 $h_t$ 包含上文信息。
  3.  通过 Softmax 分类器输出概率分布。
  4.  概率最大的可能是字符 `'m'` (对应单词 mat)。
- **自回归生成 (Auto-regressive)**：一旦生成了 `'m'`，将其加入输入序列末尾，去掉最前面的字符，形成新的输入 `"he cat sat on the m"`，再预测下一个。如此循环往复。

---

### 第二部分：数据预处理流水线（Pre-processing Pipeline）

这是非常关键的工程步骤，决定了模型能否跑通。

#### 1. 原始文本切片

假设有一本 60 万字符的书。我们需要构建监督学习的 `(X, y)` 数据对。

- **超参数**：
  - `maxlen` (片段长度)：例如 60。作为 RNN 的时间步长。
  - `stride` (步长)：例如 3。每次窗口向右滑动的由跨度。
- **切分逻辑**：
  - **Sample 1**: `X` = text[0:60], `y` = text[60]
  - **Sample 2**: `X` = text[3:63], `y` = text[63] (因为 stride=3)
  - ...
  - 最终得到约 20 万条 `(Sequence, Next_Char)` 样本对。

#### 2. 向量化 (Vectorization)

- **字典构建**：统计所有出现的字符（字母+标点+换行），建立字符索引（如 'a'->1, 'b'->2）。
- **One-Hot Encoding**：
  - 不同于单词级模型需要 Embedding，**字符级模型直接用 One-Hot**。
  - **原因**：常用字符（Vocabulary）很少，通常只有 50-100 个（相比单词的 10,000+）。维度已经很低，不需要 `Embedding 降维`。
  - **输入张量形状**：`(样本数, 60, 57)`。
    - 60 是序列长度。
    - 57 是字符集大小（Vocabulary Size）。
  - **标签张量形状**：`(样本数, 57)`。

---

### 第三部分：模型构建与训练

#### 1. 模型架构

这是一个非常标准的“Many-to-One”`多分类`模型。

- **Input Layer**：形状 `(60, 57)`。
- **LSTM Layer**：
  - 单元数：例如 128。
  - **关键点**：必须使用 **单向 LSTM**。
  - **原因**：生成任务是有时序因果的（根据过去预测未来），不能偷看“未来”的信息，所以 Bidirectional RNN 在这里不适用。
- **Dense Layer (Output)**：
  - 单元数：57（字符集大小）。
  - 激活函数：**Softmax**（输出概率分布）。
- **Loss Function**：Categorical Crossentropy（多分类交叉熵）。

#### 2. 训练过程

- 模型本质上是在做 57 分类的任务。
- 随着 Epoch 增加，模型从最初输出乱码，逐渐学会拼写单词，最后学会语法结构。

---

### 第四部分：文本生成策略（Sampling Strategy）

当模型输出了下一个字符的概率分布后，如何选择字符？这是决定生成文本质量的关键。

#### 1. 贪心策略 (Greedy Selection)

- **做法**：直接选概率最大的那个（`argmax`）。
- **缺点**：生成的文本完全确定（Deterministic），缺乏多样性，很容易陷入不断重复的死循环，非常无聊。

#### 2. 多项式采样 (Multinomial Sampling)

- **做法**：根据概率分布进行随机抽样。如果 'a' 概率 0.1，就有 10% 几率抽到它。
- **缺点**：过于随机。可能会抽到概率很低、完全不通顺的字符，导致生成内容包含大量拼写错误。

#### 3. 温度采样 (Temperature Sampling) —— 最佳实践

- **引入参数 $T$ (Temperature)**：用于调整概率分布的“尖锐程度”。

  - 公式：$p_i' = \frac{\exp(\log(p_i)/T)}{\sum \exp(\log(p_j)/T)}$

- **逻辑**：
  - **高温 ($T \to \infty$)**：分布趋于均匀（Uniform），生成内容非常随机、疯狂，错误多。
  - **低温 ($T \to 0$)**：分布趋于极端（One-Hot），接近贪心策略，生成内容保守、准确但重复。
  - **适中 ($T \approx 0.5 \sim 0.8$)**：在创造性和准确性之间取得平衡。

---

### 第五部分：实战结论与展望

1.  **实操步骤总结**：
    - 准备 Seed Text（种子文本，作为开头）。
    - 预处理 Seed $\rightarrow$ 输入模型 $\rightarrow$ 得到概率 $\rightarrow$ **温度采样**得到 Char。
    - 将 Char 拼接到末尾，滑动窗口，重复上述步骤。
2.  **局限性**：
    - 字符级 LSTM 虽然能学会单词拼写和基本语法，但很难保持**长距离的逻辑一致性**（Semantic Consistency）。
    - 例如：它可能生成一篇格式完美的论文，引用格式都对，但内容全是胡说八道。
3.  **未来方向**：
    - 这就引出了下一节课的主题——**Seq2Seq (Sequence to Sequence)** 模型，它是机器翻译和更高级生成任务的基础。

总结来说，这节课展示了 RNN 最迷人的特性——“创造力”。尽管字符级 LSTM 能力有限，但它揭示了像 GPT 这样的大型语言模型底层的基本生成逻辑（自回归预测）。

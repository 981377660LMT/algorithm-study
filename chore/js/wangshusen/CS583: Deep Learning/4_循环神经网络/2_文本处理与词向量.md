# 文本处理与词向量

这份课件紧接着上一节的“基础处理”，深入讲解了 **文本处理进阶（Tokenization 与对齐）、词嵌入（Word Embedding）的原理与实现**，并演示了如何使用 Keras 搭建一个简单的 **电影评论二分类模型（Sentiment Analysis）**。

以下是逻辑清晰、涵盖所有知识点的深入讲解：

### 第一部分：文本处理进阶

在上一节介绍了基础的 Tokenization 之后，本节课通过 IMDB 电影评论数据集（5 万条评论，2.5 万训练+2.5 万测试）引入了更多实战中的文本处理细节。

#### 1. Tokenization 的细节挑战

虽然核心是将文本分割成单词列表，但课件指出了几个关键的工程考量：

- **大小写转换（Case Folding）**：
  - 通常将所有字母转为小写。
  - _权衡_：虽然这能统一 `Apple` 和 `apple`（指水果），但可能会混淆专有名词（如 `Apple` 公司 vs `apple` 水果）。
- **停用词（Stop Words）**：
  - 像 `the`, `a`, `of` 这样的词极其高频但对情感分类（Sentiment Classification）毫无贡献。
  - _策略_：在预处理阶段直接移除。
- **拼写纠错（Spelling Correction）**：
  - 用户生成内容（UGC）如推特或评论常有拼写错误，修正这些错误有助于提高模型准确率。

#### 2. 序列构建（Sequence Construction）

经过 Tokenization 和字典映射后，每条评论变成了一个**正整数列表（Sequence）**。

- **问题**：评论长度不一（有的几句话，有的洋洋洒洒几千字），导致生成的 Sequence 长度参差不齐（如长度 52 vs 90）。
- **机器学习的限制**：大多数模型（包括神经网络）要求输入数据具有固定的形状（Tensor/Matrix），以便进行批量矩阵运算。

#### 3. 序列对齐（Sequence Alignment）- 核心操作

为了让所有 Sequence 长度一致，必须进行 padding 或 truncation。

- **设定固定长度 $W$**：例如设定 $W=20$。
- **截断（Truncation）**：如果序列长度 > $W$，砍掉多余部分。
  - _策略_：通常保留**最后** $W$ 个词（因为结尾往往包含总结性评价）。
- **填充（Padding）**：如果序列长度 < $W$，用 **0** 补齐。
  - _注意_：这就是为什么字典索引要从 1 开始，因为 0 被保留用于 Padding。
- **结果**：所有输入数据被整理成一个 $N \times W$ 的整数矩阵，可以喂给模型了。

---

### 第二部分：词嵌入（Word Embedding）

这是本节课的核心理论部分。它解决了 One-Hot Encoding 维度灾难的问题。

#### 1. 为什么要使用 Embedding？

- **One-Hot 的局限**：
  - 如果字典大小 $V = 10,000$，每个单词都是 10,000 维的稀疏向量。
  - 这会导致模型参数量爆炸（尤其是接入 RNN 时）且计算极其低效。
  - 这种向量非常稀疏（只有一个 1，其余全 0）。
- **Embedding 的优势**：
  - 将高维稀疏向量（One-Hot）映射为**低维稠密向量**（Dense Vector）。
  - 例如：10,000 维 $\rightarrow$ 8 维。

#### 2. Embedding 的数学原理

- **本质**：Embedding 层本质上是一个**参数矩阵 $P$** 的查找（Look-up）操作。
  - 矩阵大小：$V \times D$
    - $V$ (Vocabulary Size)：词表大小（如 10,000）。
    - $D$ (Embedding Dimension)：词向量维度（如 8，这是超参数）。
- **操作**：
  - One-Hot 向量 $e_i$ 乘以参数矩阵 $P$：$e_i \times P$。
  - 因为 $e_i$ 只有第 $i$ 位是 1，这相当于直接取出矩阵 $P$ 的第 $i$ 行。
  - **矩阵 $P$ 的每一行就是一个单词的词向量。**

#### 3. 语义空间的直观理解

- Embedding 矩阵是**可学习参数**，在训练过程中自动更新。
- **结果**：意思相近的词，在向量空间中距离会变近。
  - `Excellent`, `Good`, `Fun` $\rightarrow$ 聚集在一起（正面情感）。
  - `Poor`, `Boring` $\rightarrow$ 聚集在一起（负面情感）。
  - 两组词群会互相远离。

---

### 第三部分：实战——从零搭建情感分类模型

课件使用 Keras 演示了一个简单的 Logistic Regression 模型（虽然带了 Embedding 层，本质上是个浅层神经网络）。

#### 1. 模型架构（Sequential）

1.  **Input**：形状为 $(N, 20)$ 的整数矩阵（取了最后 20 个词）。
2.  **Layer 1: Embedding 层**
    - 参数：`input_dim=10000` (词表), `output_dim=8` (词向量), `input_length=20`。
    - 输出形状：$(N, 20, 8)$。变成了一个三维张量。
    - 参数量：$10,000 \times 8 = 80,000$。
3.  **Layer 2: Flatten 层**
    - 作用：将 $(20, 8)$ 的矩阵“压扁”成一维向量。
    - 输出形状：$(N, 160)$。$20 \times 8 = 160$。
4.  **Layer 3: Dense 层（全连接层/Logistic Regression）**
    - 单元数：1。
    - 激活函数：**Sigmoid**（输出 0~1 的概率值）。
    - 参数量：$160 \times 1 + 1 (\text{bias}) = 161$。

#### 2. 训练与评估流程

- **编译（Compile）**：
  - 优化器：RMSprop。
  - 损失函数：Binary Crossentropy（二分类交叉熵）。
  - 指标：Accuracy。
- **训练（Fit）**：
  - **Epoch**：50 次（遍历全量数据 50 次）。
  - **Validation Split**：从训练集中分出 5000 条做验证。
- **结果分析**：
  - **过拟合现象**：训练集准确率一路飙升至 85%，但验证集在 74% 左右就停滞了。这说明模型开始死记硬背训练数据，并未真正学到更多泛化规律。
  - **测试集表现**：最终 Accuracy 约 75%。
  - **评价**：虽然只用了最后 20 个词且模型很简单，但 75% 的准确率远高于瞎猜（50%），证明 Embedding 有效地提取了语义特征。

### 总结

这节课构建了一个完整的 NLP 入门链路：

1.  **预处理**：Raw Text $\rightarrow$ Tokenization $\rightarrow$ Integer Sequence $\rightarrow$ Padding/Truncation。
2.  **特征提取**：Integer ID $\rightarrow$ **Embedding Matrix** $\rightarrow$ Dense Vector。
3.  **模型与评估**：利用 Keras 搭建浅层网络，验证了词向量在情感分析任务中的有效性，同时也暴露了浅层网络处理序列数据的局限性（只用了 20 个词，丢失了语序信息），为下一节课引入 **RNN（循环神经网络）** 埋下了伏笔。

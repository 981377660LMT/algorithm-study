# 文本处理与词向量

根据提取到的 PDF 内容（尽管前部分存在乱码，但关键的后半部分示例和总结非常清晰），这份课件 **`9_RNN_1.pdf`** 是王树森教授 **CS583 Deep Learning** 课程中关于 **循环神经网络 (RNN)** 章节的入门部分。

它主要通过一个具体的 **情感分析 (Sentiment Analysis)** 案例，介绍了 NLP 任务的标准预处理流程，并展示了一个**不使用 RNN 的基准模型 (Baseline Model)**。

以下是详细解读：

### 1. 核心主题：文本数据到数值序列 (Texts to Sequences)

课件首先讲解了如何将人类可读的文本转换为机器可读的数字序列，这是所有 NLP 模型的第一步。

- **原始数据 (Raw Text)**:
  - 使用电影评论作为例子（如 IMDB 数据集）。
  - 例如：_"For a movie that gets no respect there sure are a lot of memorable quotes..."_
- **分词 (Tokenization)**:
  - 将句子拆解为单词列表。
  - `['for', 'a', 'movie', 'that', ...]`
- **编码 (Encoding)**:
  - 构建词汇表，将每个单词映射为一个唯一的整数 ID。
  - 例如：`'for'` -> 1, `'a'` -> 2, `'movie'` -> 3。
  - 转换结果：`[1, 2, 3, 4, ...]`
- **对齐/截断 (Alignment)**:
  - 由于模型通常需要固定长度的输入，课件中采取了只保留**最后 20 个单词** (`word_num=20`) 的策略。

### 2. 基准模型：Embedding + Logistic Regression

为了展示效果，课件构建了一个简单的神经网络模型，这个模型还没有用到 RNN，而是用作对比的 Baseline。

- **输入 (Input)**:
  - 一个长度为 20 的整数序列（代表 20 个单词）。
- **层级 1：嵌入层 (Embedding Layer)**:
  - 这是 NLP 的核心组件。它将每个整数 ID 转换为一个低维稠密向量（例如 8 维）。
  - 参数量：假设词表大小为 10,000，嵌入维度为 8，则有 $10,000 \times 8$ 个参数。
  - 输出形状：$20 \times 8$ 的矩阵。
- **层级 2：展平层 (Flatten Layer)**:
  - 将 $20 \times 8$ 的矩阵拉直成一个 $160$ 维的向量。
- **层级 3：逻辑回归 (Logistic Regression / Dense output)**:
  - 一个全连接层，输出 1 个值（用于二分类）。
  - 使用 Sigmoid 激活函数判断是正面评价还是负面评价。
  - 参数量：$160 \times 1 + 1 (\text{bias}) = 161$ 个参数。

### 3. 实验结果与总结

- **性能 (Performance)**:
  - 课件展示了 Loss 和 Accuracy 的训练曲线。
  - 最终在测试集上达到了约 **75% 的准确率**。
- **分析**:
  - 虽然只使用了评论的最后 20 个词，且模型结构非常简单（没有考虑词与词之间的复杂时序关系，仅仅是特征拼接），但依然能取得不错的效果。
  - 这为后续引入 RNN（能够处理变长序列、捕捉上下文依赖）提供了很好的对比基准。

### 总结

这份 PPT 的作用是 **"承上启下"**：

1.  **扫盲 NLP 预处理**：教你如何把 `string` 变成 `tensor`。
2.  **建立 Baseline**：用最简单的全连接网络处理文本，证明即使是简单的统计特征也有预测能力，引出后续 "为什么要用 RNN"（为了处理更长序列、捕捉语序和长期依赖）。

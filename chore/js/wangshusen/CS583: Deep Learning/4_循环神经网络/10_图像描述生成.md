# 图像描述生成

# 注意力机制

这篇 PDF 课件详细讲解了如何构建一个 **看图说话（Image Captioning）** 系统。这是一个经典的深度学习多模态任务，结合了计算机视觉（CV）和自然语言处理（NLP）。

以下是对该课件内容的**深入分析与解构**：

### 1. 核心任务

- **输入**：一张静态图片。
- **输出**：一段描述该图片内容的文字（Caption）。
- **数据集**：使用的是 **Flickr8K** 数据集，其中每张图片包含 5 个不同的人工标注句子。

### 2. 系统架构解构：Merge Model (合并模型)

该课件展示的是一种被称为 "Merge Model" 或 "Inject Model" 的经典架构。它的核心思想是分别处理图像和文本，然后在一个共享的语义空间中将它们融合。

#### **第一步：视觉特征提取 (The "Eye" - Encoder)**

模型并不是直接在原始像素上训练，而是利用迁移学习。

- **模型**：**VGG16** (预训练在 ImageNet 上的卷积神经网络)。
- **操作**：
  1.  加载 VGG16，保留卷积层和全连接层。
  2.  **去掉最后一层**（`vgg16.layers.pop()`）：去掉原本用于分类 1000 种物体的 Softmax 层。
  3.  **提取特征**：取倒数第二层全连接层 (`fc2`) 的输出。
- **结果**：每张图片被压缩成一个 **4096 维的特征向量**。这是一个高度抽象的语义向量，不再包含空间信息（Flatten 过了），只包含图片中有“什么”的信息。
- **优化**：为了节省训练资源，通常会先运行一次 VGG16，把所有图片的 4096 维向量存成 `.pkl` 或 `.npz` 文件，训练 Caption 模型时直接读取向量，不再重复跑 CNN。

#### **第二步：文本预处理 (The "Language")**

- **清洗**：分词、去标点、转小写。
- **特殊标记**：
  - `startseq`：作为生成的“扳机”，告诉模型开始说话。
  - `endseq`：模型生成这个词时停止。
- **序列化**：将文本转为整数索引序列 (`[2, 1, 43, ...]`)。

#### **第三步：构建训练数据 (关键逻辑)**

这是理解该任务最关键的一步。模型是**逐词预测**的。对于一张图和一个句子，会被拆解成多个 `(输入, 输出)` 样本对。
假设图片特征是 `IMG`，句子是 `[startseq, A, B, endseq]`。
训练样本如下：

1.  输入: `(IMG, [startseq])` $\to$ 目标: `A`
2.  输入: `(IMG, [startseq, A])` $\to$ 目标: `B`
3.  输入: `(IMG, [startseq, A, B])` $\to$ 目标: `endseq`

课件中提到，Flickr8K 有 6000 张训练图，30000 个句子，最终拆解出了 **354,000** 个这样的微小训练样本，极大地扩充了数据量。

#### **第四步：神经网络模型设计 (The "Brain")**

这是一个双分支网络，使用 Keras 实现：

1.  **图像分支 (Image Branch)**：
    - 输入：4096 维向量。
    - 处理：`Dropout(0.5)` $\to$ `Dense(256, relu)`。
    - 作用：将图像特征压缩到 256 维，使其与文本特征维度一致。
2.  **文本分支 (Sequence Branch)**：
    - 输入：变长文本序列。
    - 处理：`Embedding(vocab_size, 256)` $\to$ `LSTM(256)`。
    - 作用：将当前的单词序列历史编码为 256 维的状态向量。
3.  **融合 (Fusion)**：
    - 操作：**Add (相加)** 或 Concatenate (拼接)。课件中使用的是 `Add` 层，通过元素级相加将图像语义注入到 LSTM 的上下文流中。
4.  **预测头 (Prediction Head)**：
    - 处理：`Dense(256, relu)` $\to$ `Dense(vocab_size, softmax)`。
    - 输出：词汇表上每个词的概率分布。

### 3. 生成过程 (Inference)

训练好的模型在生成描述时采用**自回归 (Auto-regressive)** 方式，类似咱们之前看到的 seq2seq，但 Encoder 换成了 CNN：

1.  输入固定的图片向量 + 初始文本 `[startseq]`。
2.  模型预测下一个词（例如 `a`）。
3.  将 `a` 加入序列，变成 `[startseq, a]`，再次输入模型。
4.  模型预测 `cat`。
5.  将 `cat` 加入序列...
6.  循环直到生成 `endseq`。

### 总结

这份课件展示的是 **"Show and Tell" (NIC)** 风格的早期经典 Image Captioning 模型。它的特点是**将图像视为序列生成的第一个“单词”或者一种全局的上下文背景**。

- **优点**：结构清晰简单，易于用 Keras 实现（课件中给出了完整代码），利用了强大的 VGGNet。
- **局限**：它是基于全局图像特征的（4096 维向量），丢失了空间位置信息。这意味着模型虽然知道图里有“猫”，但可能不知道猫在左边还是右边，也无法像 Attention 机制那样在生成“猫”这个词时专门去“看”图中的猫的区域。

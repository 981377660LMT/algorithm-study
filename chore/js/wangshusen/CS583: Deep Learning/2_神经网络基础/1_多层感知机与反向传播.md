神经网络基础。本部分涵盖多层感知器、反向传播和深度学习库，重点是 Keras。

# 多层感知机与反向传播

这份 PDF 文档 (`6_NeuralNet_1.pdf`) 的后半部分（第 49 页至第 91 页）主要围绕**神经网络的训练机制**展开，特别是**反向传播（Backpropagation）**算法的原理与推导，以及**优化策略（Mini-Batch SGD）**和**全连接网络（FC Network）的构建指南**。

以下是对这份文档内容的深入解构和分析：

### 1. 核心算法：反向传播 (Backpropagation)

这是这部分 Slide 的重中之重。它解决了“如何高效计算神经网络梯度”的问题。

- **基本原理**：

  - 利用**链式法则（Chain Rule）**。
  - 定义函数 $f(X)$，输入是向量 $x$，参数是 $W$。
  - 目标是计算损失函数 $L$ 关于权重 $W$ 的梯度 $\frac{\partial L}{\partial W}$。

- **流程解构**：

  1.  **前向传播 (Forward Pass)**：
      - 从输入层开始，逐层计算中间结果（$z$ 和激活后的 $x$）。
      - **关键点**：前向传播计算的值需要**缓存在内存中**，因为反向传播计算梯度时需要用到这些值（例如 ReLU 的导数依赖于 $z$ 是否大于 0）。
  2.  **反向传播 (Backward Pass)**：
      - 从输出层的 Loss 开始，逆向计算每一层的误差项（通常记为 $\delta$ 或 $\frac{\partial L}{\partial z}$）。
      - 利用 $\frac{\partial L}{\partial z}$ 和当前层的输入 $x_{in}$ 来计算当前层权重的梯度：$\frac{\partial L}{\partial W} = \frac{\partial L}{\partial z} \cdot \frac{\partial z}{\partial W}$。
  3.  **参数更新**：
      - 一旦拿到梯度，根据梯度下降公式更新 $W$。
      - 更新完后，可以释放内存。

- **1D 示例推导 (Pages 52-72)**：
  - 文档用一个极简的标量网络演示了全过程：
    - 模型：$z^{(1)} = w^{(0)}x \to x^{(1)} = \text{ReLU}(z^{(1)}) \to z^{(2)} = w^{(1)}x^{(1)} \dots$
    - 推导展示了如何反向一步步求出 $\frac{\partial L}{\partial z^{(2)}}$，然后利用 ReLU 的导数（$z>0$ 时为 1，否则为 0）传递到 $\frac{\partial L}{\partial z^{(1)}}$。
    - **Take-Home Message**：必须先做前向，再做反向；反向的每一步都在复用前向的值。

### 2. 训练策略：Mini-Batch 与 SGD

文档对比了三种梯度计算方式，强调了 **Mini-Batch** 的重要性。

- **SGD (Stochastic Gradient Descent)**:
  - Batch Size = 1。
  - 优点：单次迭代极快。
  - 缺点：噪声大，震荡剧烈，无法利用 GPU 的矩阵并行计算优势。
- **Full Gradient (Batch Gradient Descent)**:
  - Batch Size = $n$ (全部样本)。
  - 优点：方向准，适合凸优化。
  - 缺点：单次迭代极慢，且对于非凸的神经网络容易陷入局部最优或鞍点，通常**不适用于深度学习**。
- **Mini-Batch SGD**:
  - Batch Size > 1 (e.g., 16, 32, 64, 128)。
  - **结论**：这是工业界标准做法。它平衡了计算效率（利用矩阵乘法并行）和随机性（甚至有助于跳出局部极小值）。

### 3. 构建全连接神经网络 (Build a FC Network)

文档最后总结了构建一个标准的 FC 网络所需的五大要素：

1.  **网络结构 (Network Structure)**：
    - 层数 (Number of layers)。
    - 每层的宽度 (Width，即神经元个数)。
    - 激活函数 (Activation Function，如 ReLU, SoftMax)。
2.  **参数量 (Parameters)**：
    - 明确输入维度 $d_{in}$ 和输出维度 $d_{out}$，则权重矩阵 $W$ 大小为 $d_{in} \times d_{out}$（如果算上 Bias 则是 $(d_{in}+1) \times d_{out}$）。
    - 文档举例计算了参数总量：$\sum (\text{上一层宽} \times \text{下一层宽})$。
3.  **损失函数 (Loss Functions)**：
    - 分类任务：交叉熵 (Cross-entropy)。
    - 回归任务：L1 Loss 或 L2 Loss (Squared L2)。
4.  **梯度计算 (Compute Gradient)**：
    - 通过前向+反向传播自动完成（现代框架如 PyTorch/TensorFlow 自动处理）。
5.  **优化算法 (Optimizer)**：
    - 一阶优化算法：SGD, SGD with Momentum, AdaGrad, RMSprop。

### 总结关键结论 (Take-Home Messages)

1.  **反向传播是核心**：它通过链式法则将 Loss 的梯度传回每一层参数。
2.  **先前向，后反向**：必须保存前向的中间结果才能计算反向梯度（这也是为什么训练显存占用比推理大的原因）。
3.  **Mini-Batch 是标配**：永远不要使用 Full Batch 训练神经网络，也不建议只用 Batch Size=1。
4.  **结构化思维**：设计网络时，不仅要考虑层数，还要明确每层的输入输出维度，这直接决定了参数量和计算量。

---

这份文档（`bp.pdf`）是由 Stevens Institute of Technology 的 Shusen Wang 教授编写的课程笔记，主题是**全连接神经网络（FC NN）与卷积神经网络（CNN）的反向传播（BackPropagation, BP）推导**。

文档的核心价值在于它不仅推导了标准的全连接层梯度，还通过**将卷积操作转化为矩阵乘法（im2col/unfold）**的方式，优雅地推导了卷积层的反向传播算法。

以下是对文档内容的深度解构：

### 第一部分：全连接层 (Fully-Connected Layer) 的反向传播

这一部分详细展示了如何利用链式法则（Chain Rule）对单层 FC 网络进行求导。

1.  **定义与符号**：

    - 输入 $X \in \mathbb{R}^{b \times d_{in}}$（Batch Size 为 $b$）。
    - 权重 $W \in \mathbb{R}^{d_{out} \times d_{in}}$。
    - 线性变换 $Z = X W^T$。
    - 输出 $X' = \sigma(Z)$（激活函数 $\sigma$ 通常是 element-wise 的，如 ReLU）。

2.  **三大梯度推导**：
    目标是根据最终 Loss $Q$ 对输出 $X'$ 的梯度 $\frac{\partial Q}{\partial X'}$，反推 $Z, X, W$ 的梯度。

    - **从 输出 $X'$ 反推到 线性激活前 $Z$**：

      - 公式：$\frac{\partial Q}{\partial Z} = \frac{\partial Q}{\partial X'} \circ \sigma'(Z)$
      - 解读：这里用到了哈达玛积（Hadamard product, $\circ$），即对应元素相乘。因为激活函数是逐元素的，某位置的梯度只取决于该位置的激活值。

    - **从 $Z$ 反推到 输入 $X$**：

      - 公式：$\frac{\partial Q}{\partial X} = \frac{\partial Q}{\partial Z} \cdot W$
      - 解读：这是梯度的“回传”。$Z$ 对于 $X$ 的导数实际上就是权重矩阵 $W$。从维度上看：$(b \times d_{out}) \cdot (d_{out} \times d_{in}) = (b \times d_{in})$，维度完美匹配。

    - **从 $Z$ 计算 参数 $W$ 的梯度**（用于更新参数）：
      - 公式：$\frac{\partial Q}{\partial W} = (\frac{\partial Q}{\partial Z})^T \cdot X$
      - 解读：这是梯度的“累积”。一个权重 $w_{ij}$ 会影响该 Batch 中所有样本的计算，因此需要对 Batch 维度进行聚合。维度检查：$(d_{out} \times b) \cdot (b \times d_{in}) = d_{out} \times d_{in}$，与 $W$ 形状一致。

### 第二部分：全连接网络与反向传播机制

- **结构**：由多个 FC 层堆叠而成，$Input \to X^{(1)} \to Z^{(1)} \to X^{(2)} \dots \to Loss$。
- **流程**：
  1.  **前向 (Forward)**：从底层向上层计算，保存每一层的输入 $X^{(l)}$ 和中间结果 $Z^{(l)}$。
  2.  **反向 (Backward)**：从 Loss 开始，利用第一部分推导的公式，逐层向下传递 $\frac{\partial Q}{\partial X}$ (即上一层的 $\frac{\partial Q}{\partial X'}$)，并在途中计算出每一层的 $\frac{\partial Q}{\partial W}$ 进行参数更新。

### 第三部分：卷积层 (Convolutional Layer) 的反向传播（核心亮点）

这部分是文档的精华。通常卷积的求导非常繁琐，涉及到大量的索引变换。文档采用了一种**“降维打击”**的方法：将卷积转化为矩阵乘法。

1.  **前向传播的矩阵化视角**：

    - **Tensor 卷积**：输入 Tensor $T$ 与 Kernel $K$ 进行卷积得到 $C$。
    - **Unfolding (展开)**：
      - 将输入 Tensor $T$ 中每一个参与卷积的“小块”（Patch）拉直。
      - 形成一个巨大的矩阵 $X$，其中每一行对应输入的一个 Patch。
      - 同时将 Kernel $K$ 向量化为向量 $w$。
    - **转化**：于是，卷积操作 $T * K$ 等价于矩阵-向量乘法 $z = X \cdot w$。
    - **Reshape**：最后将结果 $z$ 还原（Reshape）为输出矩阵 $C$。

    **流程图解**：
    $$T \xrightarrow{\text{unfold}} X_{tens} \xrightarrow{\text{reshape}} X_{mat} \xrightarrow{\text{multiply } w} z \xrightarrow{\text{reshape}} C$$

2.  **卷积的反向传播**：
    利用上述的矩阵视角，卷积的 Gradient 推导变得极其简单，复用了 FC 层的结论。

    - **对 Kernel 的梯度 ($\frac{\partial Q}{\partial K}$)**：

      - 既然前向是 $z = X \cdot w$，那么对 $w$ 的梯度就是 $\frac{\partial Q}{\partial w} = X^T \cdot \frac{\partial Q}{\partial z}$。
      - 计算出 $\frac{\partial Q}{\partial w}$ 后，Reshape 一下就得到了卷积核的梯度 $\frac{\partial Q}{\partial K}$。

    - **对 Input 的梯度 ($\frac{\partial Q}{\partial T}$)**：
      - 同理，对展开矩阵 $X$ 的梯度是 $\frac{\partial Q}{\partial X} = \frac{\partial Q}{\partial z} \cdot w^T$。
      - **难点**：如何从“展开矩阵的梯度”还原回“原始 Tensor 的梯度”？
      - **解决方案 (Fold 操作)**：前向传播中的 Unfold 是将一个元素复制到矩阵的多个位置（因为它参与了多次卷积运算）；因此，反向传播时的 **Fold** 操作就是将这些位置的梯度**累加（Sum）**回原始位置。
      - 文档形象地解释了：$\frac{\partial x_{ij}}{\partial t_{xyz}} = 1$，如果 $t_{xyz}$ 参与了对应 patch 的构建。因此 $\frac{\partial Q}{\partial T} = \text{fold}(\frac{\partial Q}{\partial X})$。

### 总结 (Take-Home Message)

这份文档提供了一个非常清晰的视角来理解深度学习框架（如 PyTorch/TensorFlow）的底层实现：

1.  **所有层都是矩阵运算**：无论是 FC 还是 Conv，本质上都可以写成 $Y = XW$ 的形式（即便 Conv 需要先 Unfold）。
2.  **Fold 与 Unfold 是共轭的**：
    - Forward: Input $\xrightarrow{Copy}$ Matrix
    - Backward: Matrix Gradient $\xrightarrow{Sum}$ Input Gradient
3.  **内存开销**：这种实现方式揭示了为什么卷积层训练时显存占用大——因为 `Unfold` 操作会生成一个比原始输入大得多的中间矩阵 $X$（数据量膨胀倍数约为 $k_1 \times k_2$）。

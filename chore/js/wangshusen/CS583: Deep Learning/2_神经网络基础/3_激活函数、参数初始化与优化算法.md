# 激活函数、参数初始化与优化算法

这是一个关于神经网络核心组件的深入讲解，涵盖了激活函数、参数初始化和优化算法。

### 1. 激活函数 (Activation Functions)

激活函数通过引入非线性特性，使神经网络能够逼近任意复杂的函数。如果没有激活函数，多层网络仅仅是线性变换的叠加，等价于单层网络。

#### 常用激活函数

- **Sigmoid**:

  - **公式**: $f(x) = \frac{1}{1 + e^{-x}}$
  - **特点**: 将输出压缩到 (0, 1)。常用于二分类输出层。
  - **缺点**: 容易出现梯度消失（当 $|x|$ 很大时，梯度趋近于 0）；输出不是以 0 为中心的（导致收敛变慢）；指数运算计算量大。

- **Tanh (双曲正切)**:

  - **公式**: $f(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$
  - **特点**: 将输出压缩到 (-1, 1)，以 0 为中心。
  - **缺点**: 仍然存在梯度消失问题。

- **ReLU (Rectified Linear Unit)**:

  - **公式**: $f(x) = \max(0, x)$
  - **特点**: 计算非常简单；在 $x > 0$ 区域梯度恒为 1，解决了正区间的梯度消失问题；收敛速度通常比 Sigmoid/Tanh 快。
  - **缺点**: "Dead ReLU" 问题（负区间梯度为 0，若神经元陷入负区间，可能永远无法更新）。

- **Leaky ReLU / PReLU**:

  - **公式**: $f(x) = \max(\alpha x, x)$，其中 $\alpha$ 是一个小常数（Leaky）或可学习参数（PReLU）。
  - **特点**: 解决了 Dead ReLU 问题，负区间也有微小的梯度。

- **Softmax**:
  - **公式**: $\frac{e^{x_i}}{\sum_{j} e^{x_j}}$
  - **特点**: 将向量映射为概率分布（和为 1）。通常用于多分类问题的输出层。

```python
# ReLU 实现示例
import numpy as np

def relu(x):
    return np.maximum(0, x)

def softmax(x):
    e_x = np.exp(x - np.max(x)) # 减去 max 防止溢出
    return e_x / e_x.sum(axis=0)
```

---

### 2. 参数初始化 (Parameter Initialization)

初始化决定了模型训练的起点。糟糕的初始化会导致梯度消失或爆炸，使模型无法收敛。

- **零初始化 (Zero Initialization)**:

  - 将所有权重设为 0。
  - **后果**: 同一层的所有神经元将在前向传播计算出相同的值，后向传播得到相同的梯度，导致它们学到相同的特征（对称性破坏失败）。**绝对不可用**。

- **随机小数值初始化**:

  - 例如 `np.random.randn(D_in, D_out) * 0.01`。
  - 适用于浅层网络。深层网络中可能导致激活值在传递过程中逐层变小（梯度消失）或变大（梯度爆炸）。

- **Xavier (Glorot) 初始化**:

  - **目标**: 保持每一层输出的方差与输入的方差一致。
  - **公式**: $W \sim U[-\sqrt{\frac{6}{n_{in} + n_{out}}}, \sqrt{\frac{6}{n_{in} + n_{out}}}]$ 或正态分布 $N(0, \frac{2}{n_{in} + n_{out}})$。
  - **适用**: 适用于 **Sigmoid** 或 **Tanh** 激活函数。

- **He (Kaiming) 初始化**:
  - **目标**: 针对 ReLU 及其变体，因为 ReLU 会将一半的激活置零，所以方差需要加倍保持。
  - **公式**: 正态分布 $N(0, \frac{2}{n_{in}})$。
  - **适用**: 适用于 **ReLU** 激活函数。

---

### 3. 优化算法 (Optimization Algorithms)

优化算法用于更新网络参数以最小化损失函数。

- **SGD (随机梯度下降)**:

  - 每次随机抽取一个样本或一个小批量（Mini-batch）来计算梯度并更新。
  - **缺点**: 此路径由于噪声容易震荡；可能陷入局部极小值或鞍点；所有参数共享同一个学习率。

- **Momentum (动量)**:

  - 模拟物理动量，通过累积过去的梯度方向来加速收敛并抑制震荡。
  - **公式**: $v_t = \gamma v_{t-1} + \eta \nabla_\theta J(\theta)$，$\theta = \theta - v_t$。

- **AdaGrad**:

  - 自适应学习率。对于频繁出现的特征（大梯度），降低学习率；对于稀疏特征（小梯度），增大学习率。
  - **缺点**: 累积的平方梯度会导致学习率单调递减至 0，导致训练提前停止。

- **RMSProp**:

  - 改进 AdaGrad，引入衰减系数（Decay Rate），只累积最近的梯度平方信息，解决了学习率急剧下降的问题。

- **Adam (Adaptive Moment Estimation)**:
  - 结合了 Momentum（一阶动量估计）和 RMSProp（二阶动量估计/自适应学习率）。
  - 通常是深度学习中的**首选**优化器。
  - 它维护梯度的均值 $m_t$ 和未中心化的方差 $v_t$，并进行偏差修正。

```python
# 每个算法的更新逻辑伪代码
# w: 参数, dx: 梯度, config: 超参数配置

# SGD
# w -= config['learning_rate'] * dx

# Momentum
# v = config['momentum'] * v - config['learning_rate'] * dx
# w += v

# Adam
# m = beta1 * m + (1 - beta1) * dx
# v = beta2 * v + (1 - beta2) * (dx**2)
# m_hat = m / (1 - beta1**t)
# v_hat = v / (1 - beta2**t)
# w -= learning_rate * m_hat / (sqrt(v_hat) + epsilon)
```

**总结建议**:

1.  **激活函数**: 隐藏层优先使用 **ReLU**，如果是极深网络可尝试 Leaky ReLU。
2.  **初始化**: 若使用 ReLU，必须配合 **He 初始化**；若使用 Tanh/Sigmoid，配合 **Xavier 初始化**。
3.  **优化器**: 起手使用 **Adam** (lr=3e-4 是个常见的初始值)，如果是单纯的计算机视觉任务，SGD + Momentum 有时能达到更好的泛化性能但调参更难。

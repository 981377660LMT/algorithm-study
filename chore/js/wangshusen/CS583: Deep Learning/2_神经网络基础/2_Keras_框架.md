# Keras\_框架

这份文件 **`6_NeuralNet_2.pdf`** 是深度学习课程（CS583）关于**神经网络（Neural Networks）**的第二部分课件。它不像理论课件那样推导公式，而是一个极具实践性的**“动手实验手册”**。

这份课件的核心内容是**使用 Keras 框架构建、训练并评估一个全连接神经网络（MLP）来解决 MNIST 手写数字识别问题**。它演示了从数据处理到模型评估的完整深度学习工作流。

以下是对该课件的深度分析和解构：

### 1. 核心任务与工具

- **任务**：MNIST 手写数字识别（10 分类问题）。
- **工具**：Python, Keras (TensorFlow backend), Matplotlib, Numpy。
- **目标**：通过引入隐藏层（非线性变换），超越线性 Softmax 分类器的性能（此前提到线性分类器约为 91.3% 的准确率）。

### 2. 标准深度学习工作流（The Five-Step Recipe）

课件主要围绕以下五个步骤展开，这也是深度学习项目的标准范式：

#### 第一步：数据加载与预处理 (Data Processing)

- **加载数据**：使用 `keras.datasets.mnist` 加载数据。
  - 训练集：60,000 张。
  - 测试集：10,000 张。
  - 图片尺寸：28x28 像素。
- **向量化 (Vectorization)**：将 28x28 的二维图像矩阵“展平”为 784 维的向量 (`reshape`)。这是全连接网络的输入要求。
- **独热编码 (One-hot Encoding)**：处理标签（Label）。
  - 将标量标签（如数字 `3`）转换为 10 维向量（`[0, 0, 0, 1, 0, ...]`）。
- **数据集划分 (Partitioning)**：
  - 这一点非常重要且专业。并没有直接使用原始的 60k 作为训练集，而是从中**划分出 10k 作为验证集 (Validation Set)**，剩余 50k 作为训练集。
  - _目的_：用于在训练过程中监控模型性能，调整超参数，防止过拟合，尽量少碰最终的测试集。

#### 第二步：搭建网络模型 (Build the Network)

课件构建了一个**多层感知机 (MLP)**。

- **API**：使用 Keras 的 `Sequential` 模型，像搭积木一样堆叠层。
- **架构设计**：
  1.  **输入层**：784 维。
  2.  **隐藏层 1**：`Dense(500, activation='relu')`。500 个神经元，使用 ReLU 激活函数处理非线性。
  3.  **隐藏层 2**：`Dense(500, activation='relu')`。再叠一层 500 个神经元。
  4.  **输出层**：`Dense(10, activation='softmax')`。输出 10 个类别的概率分布。
- **参数量统计**：
  - 课件展示了 `model.summary()` 的输出。
  - 总参数量约为 **648,010** 个。计算逻辑是：$(784+1)\times500 + (500+1)\times500 + (500+1)\times10$（+1 是偏置项 bias）。

#### 第三步：编译模型 (Compile)

配置训练过程中的关键组件：

- **优化器 (Optimizer)**：`RMSprop` (学习率设为 0.0001)。_注：这是一个自适应学习率算法，比标准的 SGD 收敛通常更快。_
- **损失函数 (Loss Function)**：`categorical_crossentropy`（多分类交叉熵，分类问题的标准选择）。
- **评估指标 (Metrics)**：`accuracy`。

#### 第四步：模型训练 (Fit Data)

执行 `model.fit()`：

- **Epochs**：50 轮（遍历全量数据 50 次）。
- **Batch Size**：128（每次梯度下降使用 128 个样本）。
- **Validation Data**：传入之前划分好的验证集，实时监控性能。
- **训练日志分析**：
  - 课件展示了漫长的训练 Log。
  - 可以看到 Loss 逐渐下降，Accuracy 逐渐上升。
  - 初期 Loss 较高（~6.5），准确率低；随着训练进行，训练集准确率接近 99.3%。

#### 第五步：评估与结果分析 (Examine Results)

- **可视化**：
  - 使用 Matplotlib 绘制了 **Epochs vs Accuracy** 的曲线图。
  - **蓝点**（训练集准确率）持续上升至接近 100%。
  - **红线**（验证集准确率）稳定在 **97.4%** 左右，且没有出现明显的下降趋势（说明过拟合在可控范围内）。
- **最终测试**：
  - 使用从未见过的 Test Set (10k) 进行最终评估。
  - **最终准确率：97.24%**。
- **对比结论**：
  - **MLP (97.2%) >> Linear Softmax (91.3%)**。
  - 这证明了引入隐藏层和非线性激活函数（ReLU）极大地提升了模型的表达能力。

### 3. "Under the Hood"（底层细节）

课件不仅展示了高层 API 调用，还展示了如何查看底层细节，帮助学生祛魅：

- **查看权重**：演示了如何使用 `K.get_session().run(model.trainable_weights)` 获取具体的权重矩阵 $W$ 和偏置向量 $b$。
- **手动预测**：演示了提取出权重后，使用 Numpy 进行矩阵乘法 $h = W^T x + b$ 来手动验证模型的预测过程，证明神经网络本质上就是一系列矩阵运算。

### 4. 总结与建议 (Summary)

- **核心流程**：数据处理 -> 搭建积木 -> 编译 -> 训练 -> 预测。
- **最后建议**：**Practice! Practice! Practice!** （多练习，跑代码）。如果不亲手写代码，理论知识很快就会遗忘。

### 结论

这份课件是一个非常典型的**工程导向**教程。它跳过了反向传播的数学推导，直接聚焦于**“如何用现代框架解决实际问题”**。它清晰地展示了深度学习比传统机器学习（线性模型）更强大的原因——通过堆叠非线性层来拟合更复杂的函数。

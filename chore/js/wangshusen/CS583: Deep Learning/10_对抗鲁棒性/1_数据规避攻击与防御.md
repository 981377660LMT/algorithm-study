# 数据规避攻击与防御

这份 PDF 文档是关于 **深度学习中的逃逸攻击 (Evasion Attacks)** 的课程课件（第 11 讲），作者为 Wang Shusen。它详细讲解了如何通过对抗样本（Adversarial Examples）攻击深度神经网络，以及如何通过对抗训练进行防御。文档以 MNIST 手写数字数据集和卷积神经网络（CNN）为案例进行了代码级别的演示。

以下是该课件的深度分析与与结构化解读：

### 1. 主题概览

- **核心议题**：**数据逃逸攻击 (Evasion Attacks)**，即在测试阶段（Test Time）通过对输入数据添加微小的、人眼难以察觉的扰动，欺骗已经训练好的深度学习模型，使其做出错误的分类。
- **演示环境**：基于 Keras 框架，使用 MNIST 数据集，构建了一个标准的 CNN 模型进行攻防演练。

---

### 2. 内容解构

#### 第一部分：基础环境构建

课件首先展示了一个“受害者”模型的建立过程：

- **模型结构**：一个包含两层卷积层（Conv2D）、两层池化层（MaxPooling2D）和全连接层的标准 CNN。
- **训练结果**：该模型在 MNIST 测试集上达到了约 **99.48%** 的准确率。这是一个高精度的模型，作为后续攻击的靶子。

#### 第二部分：非目标攻击 (Untargeted Attack)

- **定义**：攻击者的目标仅仅是**让模型预测错误**，至于预测成哪一类并不关心（例如，只要不是“熊猫”就行）。
- **核心原理**：
  - **梯度上升 (Gradient Ascent)**：通常训练模型是修改参数 $W$ 来最小化 Loss；而攻击是固定参数 $W$，修改输入图像 $x$ 来**最大化**真实标签 $y$ 的 Loss。
  - **数学表达**：
    $$ x\_{new} = x + \alpha \cdot \nabla_x Loss(y, f(x; W)) $$
  - **过程**：
    1. 获取一张真实图片及其标签。
    2. 计算 Loss 相对于**输入图像**的梯度。
    3. 沿着梯度方向给图像添加少量噪音（Update input_img）。
    4. 迭代几次后，模型对该图像的置信度会剧烈改变，导致错误分类。
- **演示结果**：展示了经过极少次数的迭代（如 20 次），一张原本是 "7" 的图片，人眼看起来毫无变化，但模型预测它是 "7" 的概率从 99.9% 降到了几乎为 0。

#### 第三部分：目标攻击 (Targeted Attack)

- **定义**：攻击者不仅想让模型犯错，还指定了模型必须**错误地分类为某一个特定类别**（例如，必须把“7”认成“9”）。
- **核心原理**：
  - **优化问题**：这被建模为一个有约束的优化问题。
  - **损失函数**：
    $$ Loss = \text{CrossEntropy}(\tilde{y}, f(\tilde{x}; W)) + \lambda ||\tilde{x} - x|| $$
    - 第一项：让预测结果尽可能接近攻击者设定的伪造目标 $\tilde{y}$（比如“9”）。
    - 第二项：让伪造图像 $\tilde{x}$ 尽可能接近原始图像 $x$（保持隐蔽性）。
  - **求解方法**：文档中展示了使用 `scipy.optimize.fmin_l_bfgs_b`（L-BFGS 算法）或简单的梯度下降来求解这个伪造图像 $\tilde{x}$。
- **演示结果**：展示了将数字 0-9 互相伪造的例子。例如，一张人眼看来明明是“7”的图，模型却以高置信度认为是“9”。

#### 第四部分：防御机制 (Defense) - 对抗训练

既然模型如此容易受攻击，课件介绍了如何防御。

**4.1 Min-Max 模型 (Min-Max Model)**

- **思想**：这是一种博弈论视角的训练方法。不仅要最小化训练误差，还要保证在最坏的扰动情况下（Max）误差也是最小的（Min）。
- **公式**：
  $$ \min*W \sum \max*{\delta} Loss(y, f(x + \delta; W)) $$
  - 内层 $\max_{\delta}$：寻找最能让模型犯错的扰动（即攻击者）。
  - 外层 $\min_W$：调整模型参数以抵抗这种扰动（即防御者）。
- **实现流程**：
  1. 在每一步训练中，随机抽取样本。
  2. 生成该样本的对抗样本（Adversarial Example）。
  3. 用这些对抗样本（虽然看起来有噪音，但强制使用原始正确标签）来更新模型参数，教模型“不要被噪音欺骗”。

**4.2 梯度正则化 (Gradient Regularization)**

- **思想**：如果 Loss 函数相对于输入 $x$ 的变化非常剧烈（梯度大），那么微小的输入变化就会导致 Loss 剧变，导致预测错误。反之，如果 Loss 表面很平坦，模型就更鲁棒。
- **方法**：在 Loss 函数中加入梯度惩罚项。
  $$ \text{TotalLoss} = Loss(y, f(x; W)) + \lambda ||\nabla_x Loss|| $$
- **效果**：强迫模型学习到的决策边界更加平滑，使得对抗样本难以生成（需要更大的扰动才能跨越边界）。

---

### 3. 核心结论与启示

1.  **深度学习模型的脆弱性**：即使是准确率极高（99%+）的模型，也非常容易被精心设计的微小噪音欺骗。这在自动驾驶（如识别停车标志）或身份认证等安全敏感领域是巨大的隐患。
2.  **攻击成本低**：不需要复杂的数学工具，简单的梯度计算（反向传播到输入而非参数）即可生成攻击样本。
3.  **鲁棒性与准确性的权衡**：对抗训练虽然能提高鲁棒性，但计算成本更高（每一步都要生成对抗样本），且通常难以完全消除对抗风险。

这篇课件是深度学习安全领域的入门级经典材料，清晰地阐述了“攻击生成”与“模型加固”的矛与盾关系。

---

这份文件是 Stevens Institute of Technology 的 **Shusen Wang** 教授关于 **对抗机器学习 (Adversarial Machine Learning)** 的讲义，题为 **“Data Evasion and Poisoning Attacks against Machine Learning”**。

该文档系统性地分析了针对机器学习模型的两类核心数据攻击手段及其防御机制。以下是对该文档的深度分析与结构化解读：

### 1. 核心主题与总体架构

文档围绕机器学习模型的**安全性**展开，重点区分了两种攻击发生的时间点和机制：

- **数据逃逸攻击 (Data Evasion Attacks)**：发生在**测试阶段 (Test Time)**。攻击者通过微调输入数据，欺骗已训练好的模型。
- **数据中毒攻击 (Data Poisoning Attacks)**：发生在**训练阶段 (Training Time)**。攻击者通过污染训练数据集，导致模型“学坏”，植入后门或特定缺陷。

---

### 2. 数据逃逸攻击 (Evasion Attacks)

这是目前最常见的对抗攻击形式，通过添加人眼不可见的扰动，导致模型分类置信度剧烈变化。

#### 2.1 攻击分类

- **非目标攻击 (Untargeted Attack)**

  - **目的**：只要让模型预测*错误*即可，不关心具体预测成哪一类。
  - **数学原理**：**梯度上升 (Gradient Ascent)**。
    - 正常训练是调整参数 $w$ 以*最小化*损失 $L$。
    - 攻击是固定 $w$，调整输入 $x$ 以*最大化*真实标签 $y$ 的损失 $L(w, x+\delta, y)$。
  - **生成方法**：计算 Loss 对输入 $x$ 的梯度 $\nabla_x L$，然后沿着梯度方向修改图片像素（如 FGSM 算法）。

- **目标攻击 (Targeted Attack)**
  - **目的**：让模型将输入误分类为攻击者指定的**特定目标类** $y_{target}$（例如，把所有“熊猫”都识别为“猪”）。
  - **数学原理**：**最小化距离**或**定向损失优化**。
    - 寻找扰动 $\delta$，使得模型预测 $f(x+\delta)$ 与伪造标签 $y_{target}$ 的距离最小。
    - 或者，寻找扰动 $\delta$ 以最小化目标类别的损失 $L(w, x+\delta, y_{target})$。

#### 2.2 攻击场景

- **白盒 (White-box)**：攻击者完全掌握模型结构和参数。上述基于梯度的攻击均属此类。
- **黑盒 (Black-box)**：攻击者不知道模型细节。
  - **迁移性 (Transferability)**：文档指出，在一个模型（如 ResNet）上有效的对抗样本，往往也能欺骗另一个结构不同但任务相同的模型（如 InceptionNet）。攻击者可以本地训练替身模型生成样本，进而攻击商业 API。

---

### 3. 数据逃逸的防御机制 (Valid Defenses)

文档介绍了两种主要的增强模型鲁棒性的方法。

#### 3.1 Min-Max 模型 (对抗训练)

这是一个博弈论视角的防御策略。

- **核心公式**：
  $$ \min*w \sum \max*{\|\delta\| \le \eta} L(w, x+\delta, y) $$
- **解读**：
  - **内层 Max**：模拟攻击者，寻找在限制范围内最能让模型犯错的扰动 $\delta$。
  - **外层 Min**：模拟防御者，调整参数 $w$ 使得即使在最坏的扰动下，损失依然最小。
- **实现**：训练时交替进行“生成对抗样本”和“使用对抗样本更新参数”。

#### 3.2 梯度正则化 (Gradient Regularization)

- **洞察**：如果损失函数 $L$ 对于输入 $x$ 的变化非常敏感（即梯度很大），那么微小的噪音就能导致 Loss 剧变。
- **方法**：在损失函数中增加惩罚项，惩罚输入梯度的范数。
  $$ J = L(w, x, y) + \lambda \|\nabla_x L(w, x, y)\|^2 $$
- **效果**：强制模型学习更平滑的决策边界 (Decision Boundary)，使得微小扰动不足以跨越分类边界。

---

### 4. 数据中毒攻击 (Poisoning Attacks)

这种攻击更隐蔽，危害更深远，尤其是针对依赖网络爬取数据的模型。

#### 4.1 核心机制 (Clean-label Attack)

文档通过 **Clean-label Attack**（标签“干净”的攻击）解释了其原理：

- **手段**：攻击者不修改标签 $y$，而是修改图片 $x$ 的**特征表达**。
- **过程**：
  1.  选取一张目标图片 $x_{target}$（例如某人的脸），这是攻击者希望模型将来误认的对象。
  2.  选取一张伪装图片 $x$（例如猩猩），这是攻击者准备上传到网络的“毒药”。
  3.  通过优化算法添加扰动 $\delta$，使得神经网络提取的特征 $h(x+\delta)$ 与 $h(x_{target})$极其相似。
- **结果**：当模型在包含有毒样本 $(x+\delta, \text{"Gorilla"})$ 的数据上训练时，由于 $x_{target}$ 的特征与毒药样本极度相似，模型会将 $x_{target}$ 的特征与标签 "Gorilla" 关联起来。
- **隐蔽性**：$x_{target}$（受害者图片）从未出现在训练集中，但模型却已经在特征层面为其留了“后门”。

#### 4.2 潜在危害

- **案例**：文档提到了 Google Photos 将黑人误识别为大猩猩的事件。
- **场景**：恶意攻击者可以在网络上散布精心设计的“毒图”。当自动化爬虫抓取这些数据训练模型后，模型就会内嵌歧视性或危险的错误偏见。

---

### 5. 总结

这份讲义清晰地揭示了深度学习安全领域的攻防逻辑：

1.  **脆弱性**：基于梯度的优化不仅可以用来训练模型，反过来也可以用来高效地生成攻击样本。
2.  **防御代价**：通过 Min-Max 对抗训练或梯度正则化可以提升鲁棒性，但这通常增加了训练的计算成本或复杂度。
3.  **信任危机**：数据中毒攻击表明，即使数据集的标签看起来是正确的，数据本身可能在特征空间被“污染”，这对数据来源的审查提出了更高要求。

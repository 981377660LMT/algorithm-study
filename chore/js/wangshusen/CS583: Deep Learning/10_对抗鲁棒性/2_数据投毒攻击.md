# 数据投毒攻击

Data Poisoning Attack 是针对深度学习的一种攻击，通过对训练样本做修改，使得神经网络会犯特定的错误。

这份 PDF 文档是关于 **数据中毒攻击 (Data Poisoning Attacks)** 的课程课件（第 11 讲），作者为 Wang Shusen。它主要介绍了一种隐蔽性极高、针对深度学习训练阶段的攻击方式——**“标签干净”的中毒攻击 (Clean-label Poisoning Attack)**。

以下是对该课件的深度分析与结构化解读：

### 1. 核心概念：数据逃逸 vs. 数据中毒

课件首先通过对比明确了两种攻击的本质区别：

- **数据逃逸 (Data Evasion)**：发生在**测试阶段 (Test Time)**。攻击者不改变模型，只修改**测试样本**（例如给图片添加噪音），让已训练好的模型犯错。
- **数据中毒 (Data Poisoning)**：发生在**训练阶段 (Training Time)**。攻击者向**训练集**注入恶意样本。这些样本看起来很正常，但会像“特洛伊木马”一样潜伏在模型中，导致模型即使在面对未被修改的正常测试样本时，也会按攻击者的意图犯错。

---

### 2. 攻击机制解构：Clean-label Attack

这份课件重点介绍的是一种被称为 **"Feature Collision" (特征碰撞)** 的 Clean-label 攻击方法。

#### 2.1 攻击目标

- **受害者样本 ($x_{target}$)**：攻击者希望模型出错的特定对象。例如，攻击者希望模型把“某个特定的人脸”识别为“蛤蟆 (Toad)”。
- **注意**：受害者样本 $x_{target}$ **不在**训练集中，攻击者无法直接修改训练标签。

#### 2.2 核心手段：特征伪装

攻击者如何让模型混淆“人脸”和“蛤蟆”？如果不修改标签，就必须在**特征空间**动手脚。

1.  **选择基底样本 ($x$)**：攻击者找一张真正的“蛤蟆”图片。
2.  **优化扰动 ($\delta$)**：
    - 攻击者计算受害者图片（人脸）在预训练模型中的特征向量 $h(x_{target})$。
    - 攻击者对蛤蟆图片 $x$ 添加微小的噪音 $\delta$，生成中毒样本 $x + \delta$。
    - **优化目标**：让中毒样本 $x + \delta$ 的特征向量 $h(x + \delta)$ **极度接近** 受害者人脸的特征 $h(x_{target})$。
    - **数学公式**：
      $$ \delta^\* = \text{argmin}_{\delta} ||h(x + \delta) - h(x_{target})|| + \lambda ||\delta|| $$
      - 前一项保证特征相似（欺骗模型）。
      - 后一项保证视觉相似（欺骗人类审核员）。

#### 2.3 训练阶段的“借刀杀人”

1.  **注入**：攻击者将这张经过处理的图片（看起来是蛤蟆，标签也是“蛤蟆”）放入训练集。
2.  **决策边界偏移**：在训练过程中，模型为了正确分类这个样本，会调整决策边界以包含该样本的特征。
3.  **结果**：由于这个“有毒蛤蟆”的特征和“受害人人脸”的特征几乎重合，模型为了包容“有毒蛤蟆”，无意中也将“受害人人脸”划入了“蛤蟆”的类别。

---

### 3. 攻击的可行性与危害

#### 3.1 为什么这种攻击很难被发现？

- **Clean Label**：中毒样本在人眼看来完完全全就是一只蛤蟆，标签也是“Toad”，人工审核数据时很难发现异常。
- **针对性强**：这种攻击通常是 **Targeted Attack**，模型在其他样本上表现正常，只在遇到特定的受害者 $x_{target}$ 时才会触发错误，隐蔽性极高。

#### 3.2 现实应用场景

- **网络爬虫投毒**：攻击者将生成的中毒图片上传到互联网（Flickr, 社交网络等）。如果科技公司通过爬虫抓取这些图片训练模型，模型就会被植入后门。例如，攻击者可以让某位名人的照片被自动识别为冒犯性物体。
- **联邦学习 (Federated Learning)**：在多方协作训练中，恶意的参与方可以上传看似正常但在特征空间有毒的数据，从而控制联合模型的行为。

### 4. 总结

这份课件揭示了深度学习模型在**特征提取层面的脆弱性**。

**核心逻辑链条**：

1.  深度学习模型依赖特征空间进行分类。
2.  不同的输入（蛤蟆 vs 人脸）可以通过微小扰动在特征空间重合（Feature Collision）。
3.  如果在训练集中放入一个“身在曹营心在汉”的样本（外表是类 A，特征是类 B），模型就会被误导，将类 B 的特征也划归为类 A。

这提醒我们在使用第三方数据或进行多方联合训练时，`仅仅检查数据的标签正确性是不够的，数据本身的统计特性和特征分布可能已经被恶意篡改。`

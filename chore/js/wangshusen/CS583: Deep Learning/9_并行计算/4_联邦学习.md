# 联邦学习

这份 PDF 是 Shusen Wang 教授关于 **联邦学习 (Federated Learning, FL)** 的课程讲义。

在前面的章节中，我们学习了数据中心内的分布式训练（如 Parameter Server 和 Ring All-Reduce）。这一讲则跳出了数据中心，探讨如何在**数据无法共享**、网络环境恶劣、节点不可靠的边缘环境下进行协同训练。

以下是对这份讲义的深度解构与解读：

### 1. 核心动机：数据孤岛与隐私

- **场景**：
  - Google 想要利用全球几十亿安卓用户的键盘输入数据优化输入法模型。
  - 多家医院想要联合训练一个癌症检测模型，但为了保护患者隐私，数据不能出院。
- **矛盾**：传统的“集中式学习”要求先把数据收集到中央服务器，但这违反了隐私法规（如 GDPR）或用户意愿。
- **定义**：**联邦学习 (Federated Learning)** 是一种分布式学习范式，其核心理念是 **“数据不动，模型动”**。

---

### 2. 联邦学习 vs. 传统分布式学习

虽然 FL 也是分布式学习的一个子集，但环境的恶劣程度完全不同：

| 特征           | 传统分布式学习 (Datacenter)        | 联邦学习 (Federated Learning)          |
| :------------- | :--------------------------------- | :------------------------------------- |
| **节点控制权** | 公司拥有所有节点，稳定可靠         | 用户拥有设备（手机/IoT），随时可能离线 |
| **网络环境**   | 光纤内网，极快                     | WiFi/4G/5G，高延迟，带宽昂贵           |
| **通信成本**   | 低于或接近计算成本                 | **远高于**计算成本（瓶颈所在）         |
| **数据分布**   | **IID** (独立同分布)，通常随机洗牌 | **Non-IID** (非独立同分布)，极度个性化 |
| **数据量**     | 均衡                               | 极度不均衡 (有的用户用得多，有的少)    |

---

### 3. 研究方向一：通信效率 (Communication-Efficiency)

这是 FL 最紧迫的工程问题。手机的上行带宽非常有限，不可能像数据中心那样频繁传输梯度。

#### (1) 核心策略：以计算换通信

既然通信太慢，那就让 Worker（手机）在本地多干点活，减少与 Server（云端）交互的频率。

#### (2) 核心算法：FedAvg (Federated Averaging)

这是 FL 领域的“Hello World”算法，由 Google 在 2017 年提出。

- **传统并行 SGD**：
  - Server 发参数 -> Worker 算 **1 次** 梯度 -> Server 更新。
- **FedAvg**：
  - Server 发参数 -> Worker 在本地跑 **E 个 Epoch** (比如跑 5 轮 SGD) -> Worker 发送更新后的**参数** (而非梯度) -> Server 对这些参数取平均。
- **效果**：虽然收敛需要的总 Epoch 数变多了，但**通信轮次 (Rounds)** 大幅减少，整体适应了高延迟网络。

#### (3) 理论挑战：Non-IID

在 Non-IID 数据上（比如用户 A 只拍风景，用户 B 只拍猫），FedAvg 很容易发散。如何证明其收敛性是理论界的热点。

---

### 4. 研究方向二：隐私安全 (Privacy)

FL 的初衷是保护隐私，但它真的安全吗？

#### (1) 攻击：梯度泄露 (Gradient Leakage)

- **直觉**：很多人认为只要不传原始图片，$x$，只传梯度 $\nabla w$ 就是安全的。
- **现实**：学术界（如 Deep Leakage from Gradients）已经证明，通过梯度可以**反向重构**出原始训练数据。
  - 原理：如果模型在某张图片上的梯度很大，通过优化算法可以“猜”出这张图片长什么样。

#### (2) 防御：差分隐私 (Differential Privacy)

- 为了防止 Server 或中间人反推数据，Worker 在上传参数前，故意添加一些**噪声 (Noise)**。
- **代价**：模型精度下降（隐私与效用的 Trade-off）。

---

### 5. 研究方向三：对抗鲁棒性 (Adversarial Robustness)

在 FL 中，你是不知道对面那个 Worker 到底是诚实的用户，还是恶意的黑客。这就引出了经典的 **拜占庭将军问题 (Byzantine General Problem)**。

#### (1) 攻击手段

- **Data Poisoning (数据投毒)**：黑客修改本地训练数据（比如把所有的“猫”标记为“狗”），试图带偏模型。
- **Model Poisoning (模型投毒)**：黑客直接修改上传的梯度或参数，使其指向错误的方向。

#### (2) 防御手段

- **统计学检查**：Server 检查每个人上传的更新。如果某人的更新向量和大家的方向偏差太大（离群点），就丢弃。
- **鲁棒聚合 (Robust Aggregation)**：使用 **中位数 (Median)** 或 **Trimmed Mean**（去掉最高分和最低分）来代替简单的平均值 (Mean)，从而抵抗恶意节点的干扰。

---

### 总结

这份讲义构建了联邦学习的完整宏观图景。
它不仅仅是“分布式计算”在手机上的翻版，而是面临着 **通信带宽**、**数据异构 (Non-IID)**、**隐私泄露** 和 **恶意攻击** 四大全新挑战的交叉学科。

理解了这些，你就理解了为什么 Apple 的键盘预测需要在这里面做如此复杂的工程，以及为什么金融机构之间做联合建模时会如此谨慎。

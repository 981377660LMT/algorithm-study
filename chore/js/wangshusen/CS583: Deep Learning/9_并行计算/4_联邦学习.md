# 联邦学习

这将是王树森老师机器学习系列课程的第七讲，主题是**联邦学习（Federated Learning, FL）**。

虽然联邦学习被宣传得很火，但这节课王老师从**系统和并行计算**的基础出发，非常理性、客观地剖析了它的本质、难点以及三个主要的研究方向。这是一节“祛魅”与“务实”并存的课程。

以下是逻辑清晰、不遗漏的深度总结：

---

### 第一部分：什么是联邦学习？（Definition & Motivation）

#### 1. 核心需求：数据孤岛与隐私

- **场景 A：谷歌输入法/相册**
  - 谷歌想利用全球几十亿安卓用户的输入习惯、照片信息来训练更智能的模型。
  - **限制**：用户（尤其是欧美用户）极其在意隐私，不允许原始数据上传到谷歌云端服务器。
- **场景 B：多方医疗/金融合作**
  - 多家医院想联合训练一个辅助诊断模型。
  - **限制**：法律法规（如 HIPAA, GDPR）禁止医院交换病人数据。数据必须留在本地。
- **结论**：需要在**数据不出本地**的前提下，联合训练出一个强大的全局模型。

#### 2. 本质：特殊的分布式机器学习

- **祛魅**：联邦学习本质上就是**分布式机器学习（Distributed ML）**。
- **架构**：依然遵循 **Parameter Server (PS)** 架构。
  - **Server**：云端服务器，维护全局模型参数。
  - **Worker**：用户设备（手机、平板）或机构服务器（医院），持有本地数据。
- **流程**：
  1.  Server 发送模型参数给 Worker。
  2.  Worker 利用本地数据算梯度（或更新参数）。
  3.  Worker 发回梯度（或参数更新）给 Server。
  4.  Server 聚合更新，开始下一轮。
- _王老师观点：如果有人吹联邦学习是革命性创新，那是在吹牛。它的核心算法框架在 2015 年前就存在了。_

#### 3. 命名由来

- **联邦 (Federation)**：由高度自治的州（Worker）组成的松散政府。Worker 对自己的设备和数据有绝对控制权，可以随时退出。

---

### 第二部分：联邦学习 vs. 传统分布式学习（关键区别）

这是理解联邦学习为何“难”的关键。虽然本质一样，但**环境约束**完全不同：

| 特征              | 传统分布式 ML (Datacenter)           | 联邦学习 (Federated Learning)                |
| :---------------- | :----------------------------------- | :------------------------------------------- |
| **Worker 控制权** | Server 绝对控制，可强制 Shuffle 数据 | **高度自治**，Worker 可随时下线/拒绝         |
| **设备稳定性**    | 高性能服务器，高可靠，专人维护       | **极不稳定**（手机没电、断网、性能差异大）   |
| **通信环境**      | 数据中心内高速光纤                   | **高延迟、低带宽**（4G/WiFi/跨国网络）       |
| **数据分布**      | **独立同分布 (IID)**，均匀洗牌       | **非独立同分布 (Non-IID)**，极其偏斜         |
| **负载均衡**      | 均衡分配任务                         | **极度不平衡**（有人天天拍照，有人从不拍照） |

- **Non-IID (非独立同分布)** 是最大的痛点。因为不同用户的习惯完全不同（你的相册全是风景，我的全是自拍），这导致传统基于 IID 假设的优化算法失效。

---

### 第三部分：核心研究方向一 —— 通信效率 (Communication Efficiency)

由于 Worker 是手机且网络差，通信（传输模型参数）是最大的瓶颈。

#### 1. 策略：多算少传 (More Computation, Less Communication)

- **基本思想**：既然通信很贵，計算相對便宜（手机充电时闲着也是闲着），那就让 Worker 在本地多算一会儿，算出一个比普通梯度更好的更新方向，再发给 Server。
- **目标**：把收敛所需的通信轮数从几千次降到几十次。

#### 2. 代表算法：Federated Averaging (FedAvg)

- **来源**：McMahan et al. (AISTATS 2017)，联邦学习的开山之作。
- **流程**：
  1.  Worker 收到全局模型 $w_t$。
  2.  Worker **在本地进行多次梯度下降 (Multiple Epochs)**，比如跑 5 个 Epoch，得到本地新参数 $w_{local}$。
  3.  Worker 发送 $w_{local}$ （而非梯度）给 Server。
  4.  Server 对所有收到的 $w_{local}$ 求平均（或加权平均），得到 $w_{t+1}$。
- **效果**：虽然单次计算量大了，但收敛所需的通信轮数大幅减少。
- **理论突破**：王树森老师团队（及其他团队）后续证明了即使在 **Non-IID** 数据下，FedAvg 也能收敛。

---

### 第四部分：核心研究方向二 —— 隐私保护 (Privacy)

虽然数据没出本地，但**传回梯度/参数**安全吗？

#### 1. 攻击方式：梯度反推数据 (Gradient Leakage)

- **原理**：梯度是数据经过模型变换的产物，携带了数据的信息。
- **事实**：已有研究（如 Deep Leakage from Gradients）表明，拿到梯度 $g$ 和模型 $w$，攻击者可以通过优化手段**还原出原始训练图片**（甚至能看清人脸）。
- **结论**：即使不传数据，隐私依然在裸奔。

#### 2. 防御困境

- **差分隐私 (Differential Privacy, DP)**：最经典的防御，往梯度里加噪声。
- **两难局面 (Trade-off)**：
  - 噪声加少了 -> 防不住攻击。
  - 噪声加多了 -> 模型精度下降几个点（工业界无法接受），甚至不收敛。
- _王老师观点：目前还没有完美的解决方案，加噪声是“打断孩子腿防踢被子”。_

---

### 第五部分：核心研究方向三 —— 鲁棒性 (Robustness)

针对**拜占庭将军问题 (Byzantine Faults)**，即防“内鬼”。

#### 1. 攻击方式：投毒 (Poisoning)

- **Data Poisoning**：Worker 修改本地数据标签，制造“有毒”数据。
- **Model Poisoning**：Worker 直接修改计算出的梯度，上传恶意的参数更新。
- **后果**：模型整体准确率下降，或者被植入**后门 (Backdoor)**（例如：识别到红色的车就自动将其分类为青蛙）。

#### 2. 防御困境

- **传统防御**：检查 Worker 梯度的统计特性（如离群值检测）。
- **联邦痛点**：由于数据是 **Non-IID** 的，正常用户的梯度本来就千差万别。Server 分不清谁是恶意攻击，谁只是一个特立独行的正常用户。
- **现状**：很难做，目前没有完全有效的防御手段。

---

### 总结

王树森老师这节课对联邦学习做了一个非常清醒的扫描：

1.  **定义**：它是 Non-IID、低带宽、不稳定环境下的分布式机器学习。
2.  **主算法**：**FedAvg**。通过增加本地计算量（多跑几个 Epoch）来换取通信次数的减少。
3.  **三大挑战**：
    - **效率**：通信太慢，必须减少轮数。
    - **隐私**：梯度会泄露数据，加噪声会降精度。
    - **安全**：很难防住恶意用户的投毒，因为无法区分“恶意”和“数据分布差异”。

这也是机器学习与系统结合的一个前沿交叉领域，虽然只有“微创新”，但应用价值巨大。

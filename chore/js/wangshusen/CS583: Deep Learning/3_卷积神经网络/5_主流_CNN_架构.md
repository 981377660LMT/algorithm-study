# 主流*CNN*架构

这份 PDF 文档是王树森教授关于卷积神经网络（CNN）课程的第五部分，主题是 **“Common CNN Architectures”（常见 CNN 架构）**。

这份课件不仅仅是罗列模型，而是不仅梳理了一条清晰的**“演进逻辑线”**：从早期的简单堆叠，到遇到深度瓶颈，再到通过结构创新突破瓶颈，最后走向轻量化和边缘计算。

以下是对该课件的深度解构和分析：

### 1. 经典架构期：简单堆叠的极限 (Classic Architectures)

这一时期的主要特征是**顺序堆叠 (Sequential)**，即一层卷积接一层池化，最后接全连接层。

- **LeNet-5 (1998)**：
  - **结构**：2 个卷积层 + 2 个全连接层。
  - **地位**：CNN 的鼻祖，主要用于手写数字识别。
- **AlexNet (2012)**：
  - **结构**：5 个卷积层 + 3 个全连接层。
  - **里程碑**：ImageNet 比赛冠军，深度学习复兴的标志。它证明了深层卷积网络在大型数据集上的威力。
- **VGG-16/19 (2014)**：
  - **结构**：13 个卷积层 + 2 个全连接层（VGG16）。
  - **特点**：使用统一的 $3 \times 3$ 小卷积核，证明了使用多个小卷积核堆叠优于使用大卷积核（如 AlexNet 的 $11 \times 11$）。
  - **瓶颈（关键点）**：课件中提出了一个核心问题——**“Can the classic architectures go deeper?”（经典架构能做得更深吗？）**
    - **答案是 No**。并不是因为过拟合，而是因为**梯度消失 (Vanishing Gradient)**。随着层数增加，信号在反向传播中逐渐衰减，导致底层权重无法得到有效更新，训练误差反而变大。

### 2. 现代架构期：结构创新解决深度难题 (Modern Architectures)

为了解决梯度消失和计算量爆炸的问题，现代架构不再单纯地“堆层数”，而是引入了复杂的拓扑结构。

#### A. GoogLeNet / Inception (2015)

- **核心模块**：**Inception Module**。
  - **多尺度并行**：在一个模块内同时使用 $1 \times 1$、$3 \times 3$、$5 \times 5$ 卷积和池化，提取不同尺度的特征。
- **关键技巧 (Tricks)**：
  1.  **1x1 卷积 (Bottleneck)**：在昂贵的 $3 \times 3$ 和 $5 \times 5$ 卷积之前，先用 $1 \times 1$ 卷积降低通道数（Dimension Reduction），大幅减少计算量。
  2.  **辅助分类器 (Auxiliary Classifiers)**：在网络中间层增加了额外的输出分支（Auxiliary Outputs），直接计算 Loss 并反向传播。这相当于**中间注入梯度 (Gradient Injection)**，强制底层也能学到东西，缓解梯度消失。
  3.  **Global Average Pooling**：摒弃了参数量巨大的全连接层（Flatten），直接对特征图取平均值，将参数量大幅降低（AlexNet 有 60M 参数，GoogLeNet 只有 5M）。

#### B. ResNet (2015)

- **核心思想**：**残差学习 (Residual Learning)**。
  - 传统网络试图学习目标映射 $H(x)$。
  - ResNet 引入了**跳跃连接 (Skip Connection / Shortcut)**，学习残差 $F(x) = H(x) - x$。即最终输出是 $H(x) = F(x) + x$。
- **为什么有效？**：
  - **Identity Mapping**：如果恒等映射（Identity）是最优解，将残差 $F(x)$ 逼近 0 比学习一个恒等映射容易得多。
  - **梯度高速公路**：梯度的反向传播可以通过 Skip Connection 无损地传回到底层，彻底解决了梯度消失问题。
- **结果**：将网络深度推到了 152 层甚至 1000 层，同时训练误差和测试误差都得到了下降。

### 3. 轻量化架构期：走向边缘计算 (Edge Computing)

课件的最后一部分重点讨论了将 AI 部署在移动端（如手机、IoT 设备）的需求。

- **背景**：云计算虽然强大，但存在延迟高、依赖网络、隐私泄露等问题。边缘计算要求模型**计算量少 (Low FLOPs)** 且 **模型体积小 (Low Params)**。
- **MobileNet (2017)**：
  - 这是本节课介绍的终极解决方案。
  - **核心创新**：**深度可分离卷积 (Depthwise Separable Convolution)**。
  - **原理分解**：
    1.  **标准卷积**：同时在空间（Spatial）和通道（Channel）维度上进行特征提取。
    2.  **Depthwise Conv**（深度卷积）：只在空间上做卷积，每个通道独立进行，不改变通道数。
    3.  **Pointwise Conv**（逐点卷积）：即 $1 \times 1$ 卷积，只在通道上做融合，不改变空间尺寸。
  - **效果**：将卷积操作分解为这两步后，参数量和计算量通常可以降低 8 到 9 倍，只有极小的精度损失。

### 总结：CNN 的进化法则

这份课件通过三个阶段展示了 CNN 设计哲学的演变：

1.  **蛮力时代** (AlexNet, VGG)：用更强的算力堆更深的网络，直到遇到梯度的物理极限。
2.  **技巧时代** (GoogLeNet, ResNet)：用精巧的结构（Inception 的并行、ResNet 的跳连）绕过梯度的物理极限，注重优化和训练的可行性。
3.  **效率时代** (MobileNet)：在算力受限的前提下，通过数学上的分解（Factorization）换取极致的效率，注重工程落地。

课件中特别强调了 **$1 \times 1$ 卷积** 和 **Global Average Pooling** 这两个在现代 CNN 设计中至关重要的通用组件。

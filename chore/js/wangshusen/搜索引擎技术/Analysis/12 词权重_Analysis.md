# 12 词权重：在“召回足够多”与“相关性不崩”之间做可控权衡

上一章你已经把 query 切成词了，但这还不够：

- 词与词的重要性差别巨大
- 丢掉某些词几乎不影响语义
- 丢掉某些词会直接把意图改掉

因此需要词权重（Term Weight / Term Necessity）：

> 给分词结果里的每个词打一个“必要性”分数，用于召回与数据构造。

这章最关键的工程价值是：

- **召回阶段丢词**：在少结果/无结果时，用最小相关性损失换取大幅 recall 增长

---

## 12.1 词权重的定义：4 档必要性

本章采用 4 档（0–3）定义，非常适合工程落地：

- 3 核心词：去掉后意图完全改变
- 2 强需求词：去掉后意图明显改变，但仍可能搜到部分满足
- 1 弱需求词：修饰主意图，去掉后主意图基本不变
- 0 冗余词：去掉后几乎不变

例子：q = “冬季 / 风衣 / 推荐”

- 风衣：核心（3）
- 冬季：强需求（2）
- 推荐：弱需求（1）

你会发现它不是“词性规则”能覆盖的东西，而是强依赖业务语义与用户预期。

---

## 12.2 标注规则：把边界写清楚，否则模型不可控

这章给的标注细则非常实用，尤其适合做成标注规范。

### 12.2.1 不需要标注的情况

- query 拼写错误：应由纠错解决（词权重模块位于纠错之后）
- 分词错误导致语义已变：词权重失去意义

### 12.2.2 一些“必背”的特殊处理

- 停用词（的/啊/呀/了）：一般为 0
- 疑问词（怎么/如何/在哪/为什么）：通常为 1 或 0（公司标准不同）
- 弱限定词（高清/大全/全集/免费版/攻略/指南/推荐）：通常为 1

### 12.2.3 上下位词：下位词是否唯一决定上位词能否丢

- 下位词独一无二 → 上位词可标 0（冗余）
  - “苹果 / iphone14”：苹果可为 0
  - “海南 / 三亚 / 景点”：海南可为 0

- 下位词有歧义 → 上位词不能丢（1 或 2）
  - “狄仁杰 / 王者荣耀”：王者荣耀应为 1/2

### 12.2.4 品牌 + 品类

- 常见为 3/3 或 3/2（品牌与品类都很关键）

### 12.2.5 排除法辅助标注

对于难判断的词，书里给了一个非常工程的“外部搜索引擎排除法”：

- 依次删掉某词，看结果是否还能覆盖原意图
- “删掉后完全搜不到” → 核心
- “删掉后少量还能搜到” → 强需求
- “删掉后大部分还能搜到” → 弱需求
- “删掉后几乎一致” → 冗余

这本质是在用更成熟的系统做弱监督。

---

## 12.3 词权重怎么用：召回丢词、BM25 加权、数据增强

本章明确了三个核心用途：

1. **召回丢词**：召回结果过少时，优先丢低权重词提升召回量
2. **文本匹配分数加权**：命中核心词贡献更大（BM25 等可按词权重调系数）
3. **训练数据增强**：
   - 丢核心词生成负样本（意图改变，应判不相关）
   - 丢非核心词生成正样本（意图保持，仍应相关）

工程提醒：

- 丢词不是“随便丢”，必须有策略（例如只丢 0/1 档，且最多丢到某个比例）
- 丢词策略要与 query 类型联动（短 query 比长 query 更敏感）

---

## 12.4 一种很巧的无标注方案：用注意力权重学习词权重

这章给出了一个工业界很流行的 trick：

- 不标注词权重
- 借助相关性数据 $(q,d,y_{q,d})$ 来学

方法骨架：

- 分词后以词粒度输入双塔模型
- 在 query 侧引入一个单头交叉注意力层：
  - 用 [CLS] 向量作为 Q
  - 用 query 各词向量作为 K/V
- 注意力权重 $\alpha_1\dots\alpha_m$ 就可以当作词权重

直觉：

- 如果模型要让 $(q,d)$ 更相关，必须“关注” query 的关键部分
- 关注分布就是“必要性”

它的优点：

- 不需要人工标注词权重
- 能随着业务相关性数据持续更新

它的风险：

- 学到的是“为拟合相关性最有用的权重”，未必等价于“语义必要性”
- 容易被数据偏差影响（比如某些词在点击数据里天然更强）

---

## 12.5 常见坑与检查清单

- 词权重训练数据包含分词错误：模型学习目标被污染
- 过度依赖规则：覆盖不了长尾与新词
- 丢词策略太激进：召回量上去了但相关性崩
- 忽略上下位歧义：把上位词丢掉后召回跑偏

---

## 12.6 练习（建议动手）

1. 对 200 条 query 做词权重标注（0–3），并统计各档占比与典型模式。
2. 实现一个简单丢词策略：当召回数 < 阈值时，按 0→1→2 的顺序逐个丢词，观察召回量与人工相关性变化。
3. 用相关性数据实现“注意力学词权重”的原型：打印 $\alpha$，看它是否符合直觉（核心词更大）。

---

## 12.7 与其它章节的连接

- 与第 11 章：分词正确性是词权重的前提
- 与第 6/16 章：词权重直接作用于文本召回（丢词、BM25 加权）
- 与第 15 章：词权重/分词改写与整句改写共同服务于“供给提升但相关性不掉”

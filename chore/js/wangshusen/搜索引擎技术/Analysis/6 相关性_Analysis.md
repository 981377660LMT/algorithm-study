# 6 相关性：标注定义 → 文本匹配 → BERT 语义匹配 → 工业训练与降本

相关性（Relevance）回答的是最朴素的问题：

> 这篇文档是否满足用户这个查询的需求？

它是搜索排序里的“地基信号”：相关性不过关，其它信号（质量/时效/地域/个性化/CTR）再强也只是把“错的东西”排得更漂亮。

本章的价值在于把相关性拆成一条可落地的工程链路：

- 上游：如何定义、如何标注（避免把质量/时效/个性化混进来）
- 中游：传统文本匹配特征（BM25、词距）能做什么
- 下游：BERT 如何在线上分层部署（双塔召回、交叉粗排/精排）
- 训练：后预训练与蒸馏为什么是工业界的最佳实践

---

## 6.1 相关性的“定义纪律”：不要把别的维度混进来

### 6.1.1 语义相关 vs 文本匹配相关

本章强调一个很重要的取舍：

- **语义相关**：文档是否“回答问题/满足需求”，允许同义改写、上下位概念、语义蕴含。
- **文本匹配相关**：文档是否包含查询词或严格匹配形式。

工业界最终追求的是语义相关，但在一些特定场景（例如强约束检索、法律/医疗关键字、某些垂类）文本匹配仍有价值。

### 6.1.2 标注时只看“相关性”，不看其它维度

相关性标注最常见的系统性错误：

- 把“内容质量差”当作“不相关”
- 把“太旧”当作“不相关”
- 把“与我无关（个性化）”当作“不相关”

正确做法是：

- 相关性：问“是否满足需求”
- 内容质量：问“是否可信/是否认真/是否有帮助”
- 时效性：问“是否需要新”
- 个性化：问“是否符合该用户偏好/历史”

如果把维度混在一起，后果非常具体：

- 相关性模型学到的标签边界漂移，离线指标波动大
- 线上融合时信号同质化（多个模块在学同一件事）
- 你很难定位 bad case：到底是相关性差还是质量差？

### 6.1.3 宽泛 query 的多意图问题

宽泛 query（比如“苹果”“周杰伦”“鞋”）天然多意图：

- 信息型、导航型、交易型可能同时存在
- 同一 query 不同用户也可能意图不同

因此相关性标注通常要求：

- 先推断“主流意图集合”，再判断文档对这些意图的覆盖
- 标注规则要允许“多个方向都可能相关”，避免把长尾意图全部判为无关

---

## 6.2 相关性档位：从“相关/不相关”到“高/中/低/无”

工业界常用 4 档（或 5 档）相关性，典型是：

- 高：强满足需求
- 中：基本满足需求
- 低：弱相关、擦边
- 无：不相关

这类分档的工程意义是：

- 训练时既能做二分类（高+中 vs 低+无）提升稳定性
- 也能保留排序细粒度（高 > 中 > 低 > 无）用于 listwise/pairwise

你在落地标注规则时，需要把“可执行的判据”写出来。本章给了一个非常工程化的判据：

- 高/中/低常与“信息覆盖比例”相关（例如核心需求满足的篇幅是否超过某阈值）
- 低与无的边界要特别明确（避免模型把噪声学成“弱相关”）

---

## 6.3 传统文本匹配：BM25 与词距特征（BM25TP）

深度模型兴起前，相关性主要靠“手工特征 + GBDT”。即便现在，文本匹配特征仍然有用：

- 作为召回/粗排的快速信号（低成本）
- 作为融合模型的一部分，补深度模型的盲点

### 6.3.1 BM25：TF-IDF 的工程化版本

BM25 是在 TF-IDF 基础上做了两个工程化改造：

- TF 饱和：避免某个词出现很多次就无限加分
- 文档长度归一：避免长文天然占优

典型形式（不需要死背，理解项的作用即可）：

$$
\text{BM25}(q,d)=\sum_{t\in q} \text{idf}(t)\cdot \frac{\text{tf}(t,d)\cdot (k+1)}{\text{tf}(t,d)+k\cdot (1-b+b\cdot \frac{l_d}{\overline{l}})}
$$

### 6.3.2 词距特征：为什么“挨得近”更可能相关

很多查询是短语或强搭配（例如“上海 迪士尼 门票”）：

- 即使三个词都出现，如果在文档里相距很远，也可能不相关

因此可以引入词距（term proximity）分数，把“距离近”当作更相关。

本章给了一个可计算的词距定义，并在 BM25 的框架下得到 BM25TP：

- 对每个查询词 $t$ 计算一个 $tp(t,d)$（近邻程度）
- 再像 BM25 一样做饱和和长度归一
- 最后将词距分数与 BM25 相加

工程上你应记住的是思想：

- 词距是“局部结构信号”，对短 query/短语特别有价值
- 词距特征计算要非常注意效率（位置表、线性扫描、只回溯最近出现）

---

## 6.4 BERT 相关性模型：交叉 vs 双塔，以及为什么要分层

本章的核心结论非常直接：

- **交叉 BERT**：准确，但推理成本最高
- **双塔 BERT**：便宜，但准确性弱

因此线上通常做“分层部署”：

- 双塔：召回海选（给海量文档打分）
- 交叉 4 层：粗排（给少量候选打分）
- 交叉 12 层：精排（最终少量文档）

### 6.4.1 交叉 BERT：把 $(q,d)$ 拼接后一起做自注意力

交叉的含义是：自注意力层让 query token 与 doc token 彼此交互。

输入结构常见为：

- query
- 文档标题
- 文档正文/摘要（通常有长度上限 128 或 256，超出截断）

关键工程点：

- **序列长度 $m$ 影响推理代价为 $O(m^2)$**（自注意力）
- 所以“如何在固定长度内塞进更多有效信息”非常关键

本章给出了一个经验：

- **字词混合粒度的 WoBERT** 往往优于纯字粒度
- 直觉：词粒度让序列更短、正文不易被截断，信息保留更多

### 6.4.2 Anchor Query：给文档“打极简标签”

一个很工程化的 trick：

- 用 Transformer 为文档生成若干个高相关查询词 $q_1,\dots,q_k$
- 把这些 anchor query 追加到文档末尾作为输入

效果经验：

- 对双塔帮助更大（等价于让 doc 向量更“可检索”）
- 对交叉帮助较小（交叉本来就能直接读正文细节）

### 6.4.3 线上降本：缓存 $(q,d)$ 分数

交叉 BERT 往往是全链路最贵的模块之一。

常见降本策略之一：

- 在 Redis 缓存 $(q,d)$ 的相关性分数
- 用 LRU 等机制淘汰

注意这是典型的“用空间换算力”：

- 命中率决定收益
- 缓存容量、键设计、过期策略会显著影响线上成本

### 6.4.4 双塔 BERT：把 query 与 doc 分别编码

双塔结构是：

- 左塔：得到 query 向量 $x_q$（线上实时算，短文本成本低）
- 右塔：得到 doc 向量 $z_d$（离线算，线上读缓存/哈希表）
- 打分：$\text{sigmoid}(x_q^\top z_d)$

这解释了为什么双塔能支撑“海量文档打分”：

- 线上只做一次 query 编码
- doc 向量直接读取
- 做大量向量内积即可

---

## 6.5 训练：后预训练与蒸馏是工业界“拉开差距”的关键

本章把相关性训练总结成四段式（与第 5 章一致）：

1. 预训练
2. 后预训练（日志挖掘 + 教师模型弱标注）
3. 微调（人工标注对齐边界）
4. 蒸馏（线上可部署）

### 6.5.1 离线指标：AUC + 正逆序比

- **AUC**：pointwise 指标，把相关性合并为二分类（相关 vs 不相关）
- **正逆序比**：pairwise 指标，看模型排序与真实档位排序一致性

直觉上：

- AUC 关注“判对不判对”
- 正逆序比关注“顺序对不对”

### 6.5.2 损失函数：保值 + 保序

本章明确提出两个目标：

- 保值：分数像标签（有利于 AUC）
- 保序：高的比低的大（有利于正逆序比）

常见组合：

- pointwise：交叉熵

$$
CE(y,p)=-y\ln p-(1-y)\ln(1-p)
$$

- pairwise：logistic pairwise loss

$$
L(p_i,p_j)=\ln\bigl(1+\exp(-\gamma\cdot(p_i-p_j))\bigr)\quad (y_i>y_j)
$$

- 总损失：两者加权

$$
\frac{1}{k}\sum_{l=1}^{k} CE(y_l,p_l) + \lambda \sum_{(i,j):y_i>y_j} L(p_i,p_j)
$$

你在工程落地时的关键是：

- 数据构造要能产生足够多的有序对（否则 pairwise 不起作用）
- $\lambda,\gamma$ 会显著影响训练稳定性

### 6.5.3 后预训练：用日志统计 + 教师模型扩数据

核心思路：

- 人工标注只有几十万到几百万，远远不够
- 日志里有曝光/点击/交互等统计，可以自动生成弱标签
- 用一个小教师模型（常是 GBDT）把统计特征映射到相关性档位
- 用教师模型给海量 $(q,d)$ 打弱标签，扩到 10 亿级别样本

并且后预训练通常会混合三类损失：

- pointwise（拟合弱标签）
- pairwise（保序）
- MLM（防止“遗忘”预训练能力）

### 6.5.4 蒸馏：不要多级蒸馏

蒸馏要点：

- 用大模型对海量 $(q,d)$ 打分 $r_{q,d}$，小模型拟合它
- 同时用 pointwise + pairwise
- 不再使用 MLM

本章给出一个很硬的经验：

- 不要多级蒸馏（48→12→4），通常会让 4 层更差
- 直接 48→4 效果更好

---

## 6.6 工程落地：如何把“相关性”变成可维护的系统

建议你把相关性系统按三层拆开：

1. **召回层**：高 recall（宁可带一点噪声）
2. **粗排层**：低成本过滤明显不相关
3. **精排层**：把“很相关”放到最前

对应模型策略：

- 召回：双塔 / BM25 / 倒排
- 粗排：小交叉 BERT 或轻量 DNN
- 精排：交叉 BERT + 融合

并且必须配套：

- 缓存策略（$(q,d)$ 打分缓存）
- 训练数据流水线（日志 join、去偏、抽样）
- 标注规则版本管理（规则变化会导致历史数据不可比）

---

## 6.7 常见坑与检查清单

- 标注把质量/时效混进来：相关性模型变成“啥都学一点”的大杂烩
- 宽泛 query 只按单一意图标注：线上会天然“偏某一类用户”
- 双塔召回只追 embedding 相似：会丢掉精确匹配、实体约束等需求
- 过度截断正文：交叉 BERT 看到的信息不够，相关性上限被截断卡死
- 蒸馏数据太少：小模型学不到大模型的“暗知识”

---

## 6.8 练习（建议动手）

1. 选 20 个宽泛 query，为每个 query 写出“允许的主流意图集合”，并给 30 篇候选文档分高/中/低/无。
2. 用你自己的语料实现 BM25，并加入一个简单的词距特征（只需两词查询）验证排序是否变化。
3. 写一个最小的 pairwise loss 训练脚本：同一 query 下采样 5 篇文档，构造有序对训练一个线性模型，观察正逆序比变化。
4. 设计一套线上降本方案：假设交叉 BERT 推理占总算力 70%，你如何用缓存/分层/截断策略把它降到 30%？

---

## 6.9 与其它章节的连接

- 与第 4 章：离线指标（AUC、正逆序比、NDCG）如何评价相关性
- 与第 5 章：后预训练与蒸馏的工业训练范式
- 与第 7/8/9/10 章：相关性是“基础层”，质量/时效/地域/个性化是在相关性之上做再排序与约束

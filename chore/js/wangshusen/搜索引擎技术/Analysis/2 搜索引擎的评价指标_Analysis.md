# 2 搜索引擎的评价指标：别用“看起来更好”的指标骗自己

搜索工程最反直觉的一点是：

- 你能把某些线上指标做得很好看
- 但用户体验与长期业务可能变差

所以本章的核心不是背指标名词，而是建立一套“指标使用的纪律”：

- **北极星指标**：公司最终关心什么（用户规模/留存）
- **过程指标**：策略迭代需要什么快速反馈（点击/交互/换词等）
- **人工评估**：当线上指标互相打架时，用什么“更接近真实体验”的方式裁决

---

## 2.1 北极星指标：用户规模与留存

### 2.1.1 用户规模：DAU/MAU 只是“人数”，不是“满意度”

- **DAU**：一天内至少使用一次的用户数
- **MAU**：过去 28 天内至少使用一次的用户数

对于同时有推荐和搜索的产品，书里强调拆分：

- **SDAU（search DAU）**：当天使用过搜索功能的用户
- **FDAU（feed DAU）**：当天使用过推荐信息流的用户
- **搜索渗透率**：$\text{SDAU}/\text{DAU}$

新手要理解其工程意义：

- 搜索做得更好，用户会更愿意“把问题交给搜索”，渗透率会上升。
- 但渗透率也会受产品入口、UI 引导影响，并非纯算法指标。

### 2.1.2 留存：两种口径（次 n 留 vs 第 n 留）

书中给了两个定义：

- **次 n 留（rolling / 次 n 日内留存）**：今天用过的用户，在未来 $n$ 天内是否至少再用一次
- **第 n 留**：今天用过的用户，在第 $n$ 天是否使用

当 $n=1$ 时两者相等；一般情况下：

- 次 $n$ 留 $\ge$ 第 $n$ 留
- 次 $n$ 留随 $n$ 单调递增
- 第 $n$ 留通常随 $n$ 递减

#### 留存指标的两个陷阱（非常重要）

1. **“赶走低活用户”会让留存变好看**：如果你把低留存人群劝退，剩下的人更活跃，留存会虚高。
2. **留存很难显著且有滞后**：需要更大流量、更长观察周期，影响迭代速度。

实战建议：任何留存上涨都要配合人群拆分与 DAU 共同看，至少回答：是不是低活人群被伤到了？

---

## 2.2 中间过程指标：快，但容易“方向盘打偏”

你做搜索迭代，往往需要 1–2 天就能看到统计显著差异（diff）的指标；这就是中间过程指标的价值。

### 2.2.1 点击类指标：CTR vs 有点比

- **文档 CTR**：$\text{click}/\text{impression}$
- **有点比**：有点击的搜索次数 / 总搜索次数
- **首屏有点比**：有首屏点击的搜索次数 / 总搜索次数

这三者通常满足：

$$\text{文档CTR} \ll \text{首屏有点比} < \text{有点比}$$

书里还强调用 **有效点击** 替代点击，降低误点干扰：

- 停留时长超过阈值
- 或发生点赞/收藏/转发/关注等任一交互

#### 点击指标为什么会骗人？（本章给的反例要背下来）

1. **放松相关性约束能显著抬高点击**：把 4 档相关性合并成 2 档，点击会涨，但体验可能变差。
2. **优化相关性模型更准，点击指标未必涨**：因为相关性更像“必须先做对的底线”。
3. **优化时效性可能损害点击**：新内容不一定更好点，但可能更符合需求。
4. **把推荐结果塞进搜索能涨点击**：但这不是满足 query 的搜索，属于指标作弊。

工程结论：

> 点击指标适合做“报警器”和“快反馈”，不适合作为唯一牵引目标。

### 2.2.2 排序位置类：首点位置与浏览深度

- **首点位置**：用户第一次点击的结果位置，越小越好
- **浏览深度**：用户最后曝光到的位置（翻页/下滑越深越大），通常越小越好

这里有统计口径坑：

- 无点击的样本怎么算？忽略会导致“全是垃圾但没人点”的搜索被当作不存在。
- 是否要用阈值截断？否则极端长尾会主导平均值。

实践中更推荐看分位数（P50/P90）或截断均值，而不是无脑均值。

### 2.2.3 交互指标：更接近满意度的强信号

转化链路：曝光 → 点击 → 阅读/播放 → 交互。

- 交互（赞/藏/评/转/关）与停留/完播往往比点击更难作假
- 很多系统会用“每次搜索的交互数”“每次点击的交互数”作为指标

### 2.2.4 换词指标：把“没搜到”显式化

- **主动换词率**：用户在一个 session 里主动换相似 query 再搜

它在直觉上是：没满足需求 → 换词。

但要注意区分：

- 用户点击系统推荐 query（被动换词）不应当算作体验差。

另外，书里提到“人均查询词数量”这个指标方向不明确：

- 换词率下降会让它变小
- 搜索更好用会让用户更愿意使用搜索，从而变大

因此它不适合单指标判断。

---

## 2.3 人工体验评估：当线上指标互相打架时，用它裁决

书里给了一个真实的行业故事：线上指标上涨但体验变差，最终用人工评估作为主要牵引。

### 2.3.1 人工评估的价值

- 可以直接判断相关性、权威性、时效性、生态等“线上难量化”的维度
- 可以避免被 CTR 等指标误导

### 2.3.2 人工评估的成本与风险

- 主观性：评估员专业度决定可靠性
- 成本：需要规模较大的评估团队
- 速度慢：会拖慢迭代
- 结果可能与线上指标矛盾，容易引发决策困难

#### 实战建议：让人工评估更“工程化”

1. 明确评估维度（相关性/质量/时效/安全等）与打分 rubric
2. 做一致性校验（同一 query 多人评、Kappa 等）
3. 做分桶评估（头部 query/长尾 query、不同垂类）
4. 强制产出 bad case 列表 + 归因（是 QP、召回、排序、还是内容侧问题）

---

## 2.4 你应该如何用本章指导日常工作（给新手的操作手册）

当你做一个策略改动（比如更换相关性模型、加入新召回、调整时效性权重），建议按下面顺序收集证据：

1. **离线**：先在标注集/回放集上验证不会引入明显退化（第 4 章细讲）
2. **线上小流量 A/B（短期）**：看过程指标是否明显异常（CTR/首点/换词/交互）
3. **线上中流量 A/B（中期）**：看 DAU/SDAU、留存是否有趋势变化
4. **人工评估（裁决）**：当过程指标与体验直觉冲突时，以人工评估或更强信号做裁决

并且要遵守 2 条纪律：

- 不允许用单个指标做最终决策
- 必须保留“解释链”：指标变化 → 哪类 query 变化 → 哪个模块导致 → 有哪些代表性 bad case

---

## 2.5 本章练习：用指标诊断 3 类常见问题

1. 发现 CTR ↑、有点比 ↑，但次 7 留 ↓：列 3 个可能原因，并写出你要看的分桶与日志。
2. 发现首点位置 ↓（变好），但交互率 ↓：列出可能的 UI/内容/点击诱导原因。
3. 发现主动换词率 ↑：请判断是相关性差、召回漏、还是时效性/地域性没识别，并给出你会抽取的 10 个 query 做人工复盘。

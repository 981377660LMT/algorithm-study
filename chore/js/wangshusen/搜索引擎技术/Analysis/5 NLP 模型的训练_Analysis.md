# 5 NLP 模型的训练：预训练 → 后预训练 → 微调 → 蒸馏的工业化流水线

如果你只看学术论文，常见套路是：预训练（公开语料）→ 微调（标注数据）。

但搜索业务的“数据形态”决定了工业界更常用四段式：

1. **预训练（Pretrain）**：学通用语言知识（公共服务）
2. **后预训练（Post-pretrain）**：学“你家业务/你家用户”的统计规律（决定性差异）
3. **微调（Finetune）**：用高质量标注把任务边界刻清楚（质量决定上限）
4. **蒸馏（Distill）**：让线上能跑得起（成本约束下尽量不掉点）

这章几乎是工业 NLP 的“方法论总纲”。

---

## 5.0 为什么搜索离不开 BERT？

搜索里大量任务都需要“理解文本”：

- QP：分词、实体识别、类目识别、意图识别、改写
- 相关性：$(q,d)$ 的语义匹配
- 内容质量：文本是否认真、是否可信、是否有帮助

这些任务在模型结构上可能差别不大，差别主要来自：

- 数据怎么来（日志挖掘 vs 人工标注）
- 训练目标怎么设（分类/回归/排序）
- 线上推理成本约束（模型层数、序列长度、精度）

---

## 5.1 预训练：MLM + SOP 是核心

### 5.1.1 MLM（Masked Language Model）

随机遮挡句子中约 15% 的 token，让模型预测被遮挡的 token。

形式上：

- 输入：带 `[MASK]` 的序列
- 输出：每个位置的表示 $c_i$
- 在被遮挡位置接 softmax 分类器预测词表分布 $p$
- 损失：交叉熵

本质价值：让模型学习“上下文依赖”的语言规律。

### 5.1.2 NSP 的争议与 SOP 的替代

- NSP：判断两句话是否原文相邻；后来发现过于简单且可能负作用。
- SOP：把相邻两句话顺序打乱作为负样本，判断是否顺序正确。

SOP 比 NSP 更难、更能迫使模型理解句间关系，因此工业界更常用 SOP。

### 5.1.3 工业预训练经验（非常重要）

1. **数据规模比 epoch 更关键**：同算力下，“更多样本 1 epoch”常优于“少样本多 epoch”。
2. **数据质量影响巨大**：去重、清洗无意义符号/表情。
3. **预训练语料要贴业务**：公开数据 + 站内数据混合往往显著提升下游效果。

你可以把预训练理解为：

> 先让模型会读“人话”，再让它会读“你家用户说的话”。

---

## 5.2 后预训练：工业界真正拉开差距的地方

这章有一句很关键的话：

> 决定模型效果最关键的点就是后预训练。

### 5.2.1 为什么学术界不做、工业界必须做？

因为工业界有海量用户行为日志：曝光、点击、交互、筛选（例如点“最新”）等。

- 单个用户行为噪声大
- 海量聚合后的统计量非常可靠

这些信号可以自动生成“弱标签”，构造超大规模任务相关数据。

### 5.2.2 典型例子：用日志做相关性的后预训练

书里给了一个很工程化的流程：

1. 先定义与标签相关的行为统计信号向量 $x_{q,d}$（例如 CTR、交互率等）
2. 从日志抽一小批 $(q,d)$，人工标注相关性 $y_{q,d}$
3. 训练教师模型 $t(x)$ 拟合人工标注（常用 GBDT 等小模型）
4. 用教师模型给数亿对 $(q,d)$ 打分得到 $\hat{y}_{q,d}=t(x_{q,d})$
5. 用 BERT 仅输入文本（q 与 d），拟合 $\hat{y}$，得到任务相关的“后预训练”模型
6. 同时保留 MLM/SOP 等预训练任务（权重可以小，但不要完全去掉）

这个流程的本质是：

- 用少量人工标注把“标签定义”对齐
- 用海量日志把数据量做大
- 用教师模型把 noisy 行为信号压缩成相对稳定的训练目标

### 5.2.3 一个必须避开的坑：反馈回路

书里强调：如果最终模型 $f$ 的输出又回流当作教师模型 $t$ 的特征，会形成反馈回路，结果不可控。

工程原则：

- 教师模型特征应来自“外生信号”（文本、行为统计）
- 不要用被训练对象的输出作为特征再去生成训练目标

---

## 5.3 微调：方法不稀奇，数据质量才稀奇

这节把工业界标注的真实困难讲得很到位：

### 5.3.1 三方协作与标注规则稳定性

- 产品团队：定义标注规则、培训、答疑、验收
- 算法团队：选样本（覆盖头中尾、刻意采困难样本）
- 标注团队：长期团队更适合长期任务（相关性等）

关键问题：标注规则变了，历史数据可能作废。

所以最重要的工程策略是：

- 一开始就尽量参考行业成熟标准
- 规则迭代要小步、要版本化

### 5.3.2 选样本比你想的难（尤其是相关性）

如果你只标注头部 query、只取排名很高/很低的文档，数据会“太简单”：

- 高位几乎都高相关
- 低位几乎都低相关

模型学不到边界。

正确做法是：刻意采一部分“中间档”的困难样本，让模型学到决策边界。

### 5.3.3 质量保障：双标、仲裁与埋雷

- 同一样本至少两人标注，不一致引入第三人
- 按一致率考核与付费
- 埋雷：混入少量可信样本检验标注质量

这是工业标注体系的核心“反作弊 + 稳质量”机制。

---

## 5.4 蒸馏：把大模型的能力搬到线上可用的小模型

线上约束决定：

- 推理必须快
- 成本必须低

因此线上通常只能用 4–12 层模型，而不是 24/48 层。

### 5.4.1 两条路线：小模型从头训 vs 大模型蒸馏

工业界共识（数据量足够时）：

- 先把大模型训到最好
- 再蒸馏到小模型

通常比“从头训小模型”更强。

### 5.4.2 最简单、最常用的蒸馏：只蒸馏输出层

在海量数据前提下，不必搞复杂蒸馏（注意力/中间层对齐等），蒸馏输出层就够用。

训练目标有两种常见方式：

1. 拟合 logits：用 MSE
2. 拟合概率分布：用交叉熵

书里经验：logits MSE 往往更好，但两种都值得试。

### 5.4.3 推理成本细节：FP16 与序列长度

- 小模型常用 FP16
- 文档长度限制为 256（更快）
- 最好训练阶段就用 FP16，而不是先 FP32 再量化

大模型可以更奢侈：

- FP32
- 长度 512 或 1024

目的：大模型更准、更像一个“高质量标注员”。

---

## 5.5 把这章落到搜索工程：一个可复用的模板

以“相关性 BERT”举例，你可以把整个训练工程化成一个 checklist：

1. Pretrain：通用 + 站内语料
2. Post-pretrain：
   - 定义行为统计特征
   - 少量人工标注
   - 训练教师模型
   - 生成海量伪标签
   - 任务相关后预训练（保留 MLM/SOP）
3. Finetune：
   - 规则版本化
   - 困难样本比例
   - 双标+仲裁+埋雷
4. Distill：
   - 大模型打分
   - 小模型拟合（logit MSE / prob CE）
   - FP16 + 长度 256
5. 上线：离线指标 + A/B + 人工评估闭环

---

## 5.6 练习（强烈建议做）

1. 设计一个后预训练任务：任选“时效性意图识别 / 内容质量”，写出你能从日志挖掘出的弱标签信号是什么。
2. 设计标注规则的版本化：假设你要做相关性标注，写出 5 条必须写进 rubric 的判定规则，以及 3 个最常见争议样本。
3. 做一个蒸馏对比实验计划：
   - 学生模型 4 层
   - 教师模型 24 层
   - 训练数据 1 亿对 (q,d)
   - 指标：AUC + NDCG@k（说明原因）

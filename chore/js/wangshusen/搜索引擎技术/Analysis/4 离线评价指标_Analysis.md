# 4 离线评价指标：上线前的“筛选器”，不是最终审判

这章讲的是：当你改了 QP / 召回 / 排序模型以后，怎么在不上线影响用户的前提下，判断“新模型大概率更好”。

核心思想：

- 离线评估本质是标准机器学习测试：在独立测试集上算指标
- 但搜索的评价对象是“排序列表”，所以指标分三类：
  - pointwise（单点拟合）
  - pairwise（成对顺序）
  - listwise（整列表、位置加权）

本章以“相关性”举例，但你可以把方法迁移到：

- 类目识别、意图识别（分类）
- CTR 预估（回归/二分类）
- 精排/粗排（排序）

---

## 4.0 相关性任务回顾：你在评估什么？

每个样本是一对 $(q, d)$：

- $q$：查询词
- $d$：文档
- $y$：人工标注的相关性档位（高/中/低/无）或分数
- $\hat{y}$：模型预测（分数或分类）

离线评估要回答的不是“模型分数像不像 y”，而是更贴近业务：

1. 能不能把不相关过滤掉？（pointwise 的二分类视角）
2. 能不能把更相关排在更前？（pairwise / listwise）
3. 排在前面的 bad case 是否足够少？（listwise 强调位置权重）

---

## 4.1 pointwise：把每个 (q,d) 当成独立样本

pointwise 不看列表结构，适合回答“预测对不对”。

### 4.1.1 二分类指标：Precision / Recall / F1 / AUC

把相关性合并成二分类：

- 正类：相关（例如 高+中）
- 负类：不相关（例如 低+无）

定义：

- $TP$：实际相关且预测相关
- $FN$：实际相关但预测无关
- $FP$：实际无关但预测相关
- $TN$：实际无关且预测无关

指标：

$$
\text{Precision} = \frac{TP}{TP+FP}
$$

$$
\text{Recall} = \frac{TP}{TP+FN}
$$

$$
F_1 = \frac{2\,\text{Precision}\,\text{Recall}}{\text{Precision}+\text{Recall}}
$$

#### 阈值问题：为什么要用 AUC

如果模型输出分数 $s\in[0,1]$，你设定阈值 $\tau$ 决定相关/无关：

- $\tau$ 小：判相关更松 → Recall ↑ Precision ↓
- $\tau$ 大：判相关更严 → Precision ↑ Recall ↓

为了排除阈值影响，用 ROC/AUC：

- 横轴 $FPR=\frac{FP}{FP+TN}$
- 纵轴 $TPR=\frac{TP}{TP+FN}$

让 $\tau$ 从 0 到 1 扫一遍得到 ROC 曲线，曲线下面积是 AUC。

经验解释：

- AUC = 0.5：随机猜
- AUC 越接近 1：排序“相关分数”越能把正类排在负类前

> 注意：AUC 衡量的是“区分能力”，不保证概率校准（校准要看 logloss / calibration curve）。

### 4.1.2 多分类指标：macro F1 vs micro F1

把高/中/低/无视作 4 类（忽略有序性），用混淆矩阵分析。

- **macro F1**：对每一类都算一个 F1，然后平均。对长尾小类更敏感。
- **micro F1**：把所有类汇总成总 TP/总样本，整体准确更主导，容易被大类“带偏”。

工程上：如果你的标注分布极不均衡（比如“无相关”占很大），micro 指标可能很好看，但你最关心的“高相关”预测可能一般。

---

## 4.2 pairwise：只关心“相对顺序”对不对

pairwise 指标不看绝对分值，问的是：

> 对同一 query 的两篇文档，模型是否把更相关的那篇排在前面？

### 4.2.1 正逆序比 PNR（Positive-Negative Ratio）

给定 query $q$ 的文档列表，模型给出排序。对任意一对文档 $(d_i,d_j)$：

- 如果模型排序中 $d_i$ 在 $d_j$ 前
- 且真实标签 $y_i > y_j$，这是**正序对**
- 若 $y_i < y_j$，这是**逆序对**

$$
PNR = \frac{\#\text{正序对}}{\#\text{逆序对}}
$$

PNR 越大说明排序越接近真实。

#### pairwise 的局限：不区分“错在前面”还是“错在后面”

书里给了一个关键洞察：两种排序可能有相同 PNR，但用户体验不同。

原因：用户更在意靠前位置的 bad case。

这直接引出 listwise 指标。

---

## 4.3 listwise：真正贴近搜索的“位置加权”评价

listwise 把“整条结果列表”作为评价对象，并且对靠前结果给更大权重。

### 4.3.1 DCG@k：折损累积增益

假设前 $k$ 个结果对应真实相关性分数 $y_i\in[0,1]$（或离散档位映射到分数）。

常见两种定义：

$$
DCG@k = \sum_{i=1}^{k} \frac{y_i}{\log_2(i+1)}
$$

或（对高相关更强调）：

$$
DCG@k = \sum_{i=1}^{k} \frac{2^{y_i}-1}{\log_2(i+1)}
$$

直觉：

- $i$ 越小（越靠前）权重越大
- 折损项 $\log_2(i+1)$ 体现“用户注意力衰减”

### 4.3.2 NDCG@k：归一化后可比

不同 query 的候选集合不同，DCG 的上界也不同。用理想排序得到：

- $IDCG@k$：把真实最相关的文档放在前 $k$ 的最大 DCG

$$
NDCG@k = \frac{DCG@k}{IDCG@k}\in[0,1]
$$

#### 重要工程坑：实验组/对照组 IDCG 必须一致

本章提醒：如果两组的 $IDCG@k$ 定义不一致（比如候选集合不同、过滤逻辑不同），会得出与事实不符的结论。

实践中应当：

- 固定评估集合（同一批 query、同一批候选文档）
- 或明确用“共同候选集合”的 IDCG
- 否则 NDCG 的分母变化会污染结论

---

## 4.4 这章对搜索工程的真正价值：离线评估怎么落地

### 4.4.1 离线评估的 3 个使用场景

1. **回归保护**：上线前挡住明显退化（尤其是头部 query 的高相关结果）
2. **模型选择**：多模型对比（不同结构、不同训练数据、不同蒸馏策略）
3. **诊断**：拆分维度找 bad case（某些类目/某些意图/某些长度 query）

### 4.4.2 指标选型建议（非常实用）

- 相关性过滤：AUC/F1（pointwise）
- 排序质量：NDCG@k（listwise）
- 补充分析：PNR（pairwise） + 混淆矩阵（看错在哪）

通常组合拳：

> NDCG@k 作为主指标，AUC/F1 作为过滤与稳定性指标，混淆矩阵用于归因。

### 4.4.3 离线评估的边界：它永远替代不了线上

离线评估再完美，也缺三样东西：

1. **曝光偏差/反馈闭环**：线上策略会改变你看到的数据
2. **UI 与承接**：结果页呈现、首图标题会影响点击
3. **生态与长期效应**：内容供给与对抗会随时间变化

因此：离线评估是上线前筛选器，而不是最终审判。

---

## 4.5 本章练习（把公式变成手感）

1. 手算一个 toy 例子：给定 5 篇文档的真实相关性分数与模型排序，分别算 DCG@3、IDCG@3、NDCG@3。
2. 设计一个阈值实验：固定模型分数，改变 $\tau$，画出 Precision-Recall 的变化，解释为什么 AUC 更稳定。
3. 以“高/中/低/无”为 4 类，构造一个混淆矩阵，分别计算 macro F1 与 micro F1，并解释它们适合回答什么问题。

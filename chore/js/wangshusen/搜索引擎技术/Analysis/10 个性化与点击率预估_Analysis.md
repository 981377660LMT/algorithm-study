# 10 个性化与点击率预估：用 CTR 模型做纠错、重排与用户偏好注入

很多人把“个性化”理解成推荐系统那种“千人千面”。

但本章的表述更工程化：

- 搜索的主干依然是“query 驱动”
- 个性化主要通过**点击率/交互率等行为概率的预估**来体现

也就是说：

> 个性化在搜索里往往不是一个独立模块，而是点击率模型在不同用户/场景下输出不同的分数。

---

## 10.1 CTR 模型在搜索中的两大作用

本章强调了 CTR 模型在搜索里非常关键的两个用途：

1. **纠错**：相关性模型/召回可能把不太对的东西混进来，CTR 往往能把用户更可能点的推上去
2. **宽泛 query 个性化**：同一 query 在不同用户上意图不同，CTR 模型通过用户特征/序列特征体现偏好

注意：CTR 不是相关性的替代品。

- CTR 更像“用户行为概率”，会受到标题、封面、位置等影响
- 搜索满意度的决定因素中，相关性仍然更硬

因此工程上需要质量/反作弊信号约束 CTR 的副作用（见第 7 章）。

---

## 10.2 特征体系：query / doc / user / scene / stat

本章对特征做了一个很常见但很实用的分类：

- query 特征：文本向量、类目、意图、时效/地域意图等
- doc 特征：标题/正文向量、质量分、站点权威、时间、POI 属性等
- user 特征：画像、长期偏好、地域、设备
- scene 特征：时间段、网络、入口、位置、展示样式
- stat 特征：历史 CTR、交互率、(q,d) 统计等

### 10.2.1 (q,d) 统计特征：强但贵

(q,d) 统计（比如某 query 下某 doc 的 CTR/交互率）非常强，但有两个硬问题：

- **成本高**：需要大规模日志聚合与在线读取
- **冷启动**：新 doc、新 query 或新组合没有统计

工程对策通常是：

- 退化策略：用 doc 全局 CTR / 类目 CTR / 相似 query 统计
- 平滑与贝叶斯先验：避免小样本极端值

---

## 10.3 特征交叉：让模型学到“组合关系”

CTR 的核心不是“单特征”，而是“特征之间的组合”：

- 这个用户 + 这个 query + 这种文档 = 会不会点

本章提到了几种常见交叉方式：

- inner product
- Hadamard product
- bilinear

以及典型结构：

- DCN（Deep & Cross Network）用于显式建交叉

工程理解：

- 交叉越强，模型越能捕捉细粒度偏好
- 但也越容易过拟合/极化，需要正则与约束

---

## 10.4 用户行为序列：平均、DIN、SIM

用户序列特征几乎是个性化 CTR 的“核武器”。

### 10.4.1 最简单：平均池化

把用户最近点击/交互的 doc 向量取平均作为兴趣向量。

优点：

- 简单、稳定、成本低

缺点：

- 对当前 query 的相关兴趣不够聚焦

### 10.4.2 DIN：注意力对齐当前 query

DIN 的思想是：

- 当前 query 作为“目标”，对历史行为做注意力加权
- 与当前 query 更相关的历史行为权重大

### 10.4.3 SIM：在长序列下做可扩展

序列越长越能表达用户长期偏好，但成本也会爆。

SIM 的工程意义在于：

- 让长序列建模在工业规模下仍可训练与推理

---

## 10.5 模型形态：精排多目标、粗排/召回双塔、MMoE

### 10.5.1 精排：多目标神经网络（前期融合）

精排通常会同时预估多个目标：

- CTR（点击）
- 交互率（停留、收藏、评论等）

这种在模型内部联合学习的方式常被称为“前期融合”：

- 优点：预测更准
- 缺点：计算成本高

### 10.5.2 粗排/召回：双塔（后期融合）

粗排、召回阶段常用双塔：

- 更省算力
- 可离线缓存 doc 向量

但也因此准确性弱一些，更多用于“海选/截断”。

### 10.5.3 多目标冲突：MMoE 与防极化

多目标学习会出现目标间冲突：

- 提升 CTR 可能牺牲交互率或满意度

MMoE 是一个典型结构：

- 多个 expert
- 每个任务有自己的 gate 选择专家组合

本章还提到防极化的工程点：

- dropout 等正则，避免模型把用户推向单一兴趣（信息茧房）

---

## 10.6 线上特征服务：本地缓存 + KV 索引

本章对特征服务做了非常工程化的拆分：

- 本地正排表缓存：静态特征（类目、实体、多模态向量等），减轻特征服务负载
- Redis 等 KV：动态特征（统计特征、画像、用户序列），便于高频更新

并强调了训练数据的生成流水线：

- 线上打点曝光/点击/交互
- 特征侧存 (searchId, docId, features)
- 标签侧存 (searchId, docId, label)
- 离线 join 得到 (features, label)

这就是典型的“每天凌晨 join 一次”的工业训练范式。

---

## 10.7 负采样与校准：训练省算力，推理回归真实概率

CTR 的正负样本极度不平衡：负样本远多于正样本。

### 10.7.1 负样本降采样

设：

- 正样本数 $n_+$
- 负样本数 $n_-$

对负样本做降采样：

- 采样率 $\alpha\in(0,1)$
- 只用 $\alpha n_-$ 个负样本训练

实践经验（本章给出）：

- 负样本数量 $\alpha n_-$ 常设置为 $n_+$ 的 1～2 倍

### 10.7.2 为什么需要校准

降采样会让训练集的正样本占比变高，从而：

- 模型预测的 $p_{pred}$ 会系统性高估真实 CTR

本章给了一个常用的经验校准公式：

$$
 p_{true}=\frac{\alpha\,p_{pred}}{(1-p_{pred})+\alpha\,p_{pred}}
$$

更方便的做法是对 logit 校准。

若 $p_{pred}=\sigma(x_{pred})$，则：

$$
 x_{true}=x_{pred}+\ln \alpha
$$

然后 $p_{true}=\sigma(x_{true})$。

工程意义：

- 训练时省算力
- 线上推理输出仍可解释为“真实概率”

---

## 10.8 评估与上线：AUC、GAUC 与热启/冷启

本章提到 CTR 常用离线指标：

- AUC
- GAUC（按用户/按 query 分组后加权，更贴近线上分布）

并提醒一个很现实的上线问题：

- 特征/模型的热启与冷启差异非常大
- 只热启 dense 特征可能需要很久才能追平
- 全部冷启可能长期追不平（需要评估业务是否值得）

这告诉你：CTR/个性化系统的工程风险主要在“数据与特征的连续性”。

---

## 10.9 常见坑与检查清单

- 把 CTR 当作相关性：会把标题党推上去
- (q,d) 统计特征依赖过重：冷启动时线上体验崩
- 校准没做：线上 CTR 分数不可解释，跨版本不可比
- 训练/推理特征不一致：离线指标很好，线上掉点
- 个性化极化：把用户推向单一内容，长期满意度下降

---

## 10.10 练习（建议动手）

1. 画出你理解的 CTR 训练数据流：从曝光/点击日志到 (features,label) 的 join。
2. 给定 $\alpha=0.1$，写一个小脚本把若干 $p_{pred}$ 校准成 $p_{true}$，观察数值变化。
3. 设计一个“冷启动降级策略”：当用户画像缺失、(q,d) 统计缺失时，模型如何退化仍能稳定排序？

---

## 10.11 与其它章节的连接

- 与第 6 章：相关性保证“答对题”，CTR/个性化决定“更可能被点击/更符合偏好”
- 与第 7 章：质量信号用于抑制 CTR 的吸睛副作用
- 与第 8/9 章：时效/地域意图是 CTR 模型的重要 query 特征，也决定融合权重

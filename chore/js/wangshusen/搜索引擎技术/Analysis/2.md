# 搜索引擎技术深度分析：第二阶段（查询词理解 QP 与 NLP 任务）

在搜索引擎的级联架构中，查询词理解（Query Processing, QP）是所有后续链路的“大脑”。本阶段分析将揭示 QP 如何通过 NLP 技术将用户模糊的输入转化为结构化的检索指令。

---

## 1. 序列标注：分词与命名实体识别 (NER)

分词是中文搜索的第一步，而 NER 是语义理解的进阶。

### 1.1 从规则到语义：BERT+CRF 的统治

- **核心挑战**：未登录词（新词）识别。
- **演变逻辑**：从早期的**最大匹配（贪心）**到**最短路径（动态规划）**，再到如今的 **BERT (+ CRF)**。
- **深度洞察**：虽然 CRF 在建模标签依赖（如 B 后面接 M/E）上理论完美，但在超大规模预训练模型（BERT）下，BERT 强大的语义建模能力已能隐式覆盖这些规则，有时直接使用 BERT 分类器（Softmax）即可平衡效果与性能。
- **LEBERT 创新**：结合字粒度与词粒度表征，解决了中文分词中“字词信息不对称”的问题，是目前中文 NLP 的前沿实践。

### 1.2 新词发现的数学美：PMI 与 自由度

- **凝聚度 (PMI)**：解决“影 + 电” vs “电 + 影”的问题。只有当 $p(AB) \gg p(A)p(B)$ 时，字符串才具有成词潜力。
- **自由度 (Entropy)**：解决“共和 + 国”的问题。一个词如果总是和某个特定字符搭配（如“人工智”总接“能”），那它大概率只是一个词的子串，而非独立词。

---

## 2. 意图识别：决策的分水岭

意图识别不仅仅是打标签，它直接决定了**下游链路的动态切换**。

### 2.1 类目识别 (Category)

- **多标签层次分类**：使用 `Recursive Regularization`（递归正则化）强制让同属一个父类目的子类目权重向量尽量接近，利用了类目树的先验拓扑结构，显著提升了样本稀疏类目的效果。

### 2.2 关键意图与链路承接

- **时效性 (Timeliness)**：决定新老文档索引的流量分配与排序权重。
- **地域性 (Locality)**：计算用户 GeoHash，从位置过滤召回转变为“距离-相关性”多因子混排。
- **求购意图**：触发“多队列混排（Multi-Queue Mixing）”。此时排序分不再仅是相关性，而是：$Score = P(Buy) \times Price$。

---

## 3. 词权重 (Term Weight)：召回的指挥棒

在召回量不足或过载时，词权重决定了“谁可以被牺牲”。

- **4 档标注**：核心词（不可去）、强需求、弱需求、冗余词。
- **Attention 机制的妙用**：在双塔模型中，利用 [CLS] 对 Query 序列的 Attention 权重作为词权重的“弱监督”信号，可以实现无标注数据的词权重计算。

---

## 4. 查询词改写 (Rewriting)：解决语义鸿沟

改写是解决“粤菜”搜不到“潮汕菜”的关键。

### 4.1 改写的终极目标

**目标并非让 $q$ 与 $q'$ 相似，而是让 $q$ 与 $q'$ 召回的文档集 $D'$ 相关。**

### 4.2 工业级三级漏斗改写架构

1. **Recall (召回)**：向量召回生成的 $q' \in T$（头部词库）。
2. **Discriminator (判别)**：由于向量召回是对称的（误把“上海”改写成“中国”），必须由**交叉 BERT (Cross-BERT)** 进行非对称负样本判定。
3. **Precision (精修)**：利用离线挖掘的 $(q, d, y)$ 数据，通过 Teacher 模型进行监督训练。

---

## 5. 核心洞察：QP 的“蝴蝶效应”

QP 环节 1% 的准确率提升，由于其位于漏斗顶端，经过召回和排序的逐级放大，最终在 SDAU 和 UCTR 上的收益往往是百分之十几。

**关键点总结：**

- **不要孤立优化 NLP 任务**：所有指标必须服务于下游的相关性（$rel(q, d)$）。
- **重视负样本构造**：在改写判别中，将被改写词自身作为正样本，能有效修正模型的偏见。

# 18 离线召回：用“离线算力 + 头部效应”把优质结果缓存下来

本章讲的是一类非常“工程收益巨大”的召回思路：

- 搜索 query 有强烈的**头部效应**
- 对头部 query，可以离线为它算出一批高质量文档并建立索引
- 线上命中头部 query 时，直接从 KV/离线索引取结果

你可以把离线召回理解为：

> 用离线成本换线上 RT 与稳定性，并在一定程度上提高相关性/质量/时效性综合水平。

本章给了三个落地方向：

1. 挖掘曝光日志（缓存召回）
2. 离线搜索链路（夜间跑一条“更大打分量”的搜索）
3. 反向召回（doc2query 等离线挖掘 $(q,d)$）

---

## 18.0 头部效应与存储量级：这类方案“算得过来、存得起”

本章给出一个很实用的量级估算：

- 若每天 50% 的搜索集中在 100–200 万 query 上
- 每条 query 存 1000 篇文档
- 文档 ID 96 位（12 字节）

那么存储规模是几十 GB 量级（甚至可用磁盘，几乎没成本）。

这解释了为什么“离线建索引式召回”在工业界很常见：

- 不需要极端昂贵的系统建设
- 但可以显著改善线上稳定性与成本结构

---

## 18.1 挖掘曝光日志：缓存召回（cache recall）

核心观点：

- 精排出来的 top 结果，本身是“多路召回 + 多模型 + 多特征”过滤过的高质量集合
- 把这些结果“按 query 缓存下来”，可以变成一路非常强的召回通道

### 18.1.1 非个性化缓存召回：q → List<d>

本章给的落地流程可概括为：

1. 从日志中取每条请求 $(u,q)$ 的 topK 文档，并带上点击/交互、相关性、质量等信息
2. 对同一个 $q$ 汇聚多用户结果，去重得到候选集合
3. 用出现频率/点击频率/交互频率 + 相关性分数 + 质量分数 融合打分
4. 时效性处理：对文档年龄做惩罚，惩罚权重由“query 时效性意图”决定
5. 线上只保留 topN（例如 200）文档进内存索引

这里非常体现“搜索工程”的特点：

- 不是某个单模型决定一切，而是多信号融合 + 强约束（配额/更新/时效意图）

### 18.1.2 弱个性化缓存召回：(q,g) → List<d>

把用户分组 $u\to g$（例如性别/年龄段组合），建立更细粒度的索引。

本章的经验结论很实用：

- 用户组数太多或太少都不好
- 约 10 组左右往往是一个性价比很高的点

原因很好理解：

- 组太少：个性化表达不够
- 组太多：数据稀疏 + 内存成本线性上升

### 18.1.3 一个关键坑：缓存召回会“污染”后续 A/B 测试

这是本章最容易被忽略、但最致命的工程问题之一。

如果你上线一个新召回通道做实验：

- 实验组曝光了新通道带来的优质文档
- 这些文档在日志里被记录、被缓存召回收录
- 之后对照组也能通过缓存召回拿到这些文档

结果就是：

- 实验收益被稀释，A/B 失真

本章给出的应对是：

- 新实验结果打标签
- 缓存召回在离线处理日志时过滤掉带标签的结果

这本质是在做“实验隔离”。

---

## 18.2 离线搜索链路：夜间跑一条更强的搜索

本章的方案叫“离线搜索链路”，可以理解为：

- 夜间（QPS 低谷）用空闲集群主动发起搜索
- 使用更大的打分量、更强模型做非个性化召回与排序
- 把结果以 $q \to List\langle d,r\rangle$（带精排相关性分）写入 KV
- 线上头部 query 直接读 KV 结果

### 18.2.1 为什么它能提升指标？

本章给的解释很到位：

- 离线链路可以用更强的相关性模型（精排交叉 BERT）
- 也可以用更强的点击/交互预估（非个性化多目标 DNN）
- KV 检索稳定性远高于复杂链路，能减少超时带来的随机波动

所以收益来自两部分：

- **更准的分数**（相关性/点击等）
- **更稳的线上链路**（超时更少）

### 18.2.2 为什么不能完全取代线上链路？

即使是头部 query，也必须保留：

- 个性化召回（用户体验与多样性）
- 新文档召回（离线刷新周期导致新内容进不来）
- 强时效/地域意图的 query 往往不适合离线缓存（变化快，离线结果易过时）

### 18.2.3 降本：把相关性分数离线算好，线上少算一大块

粗排/精排相关性推理通常是搜索链路里最贵的部分。

本章给出两级降本思路：

- 基础版：头部 query 覆盖大量请求，且非个性化文档占比高，离线算好一部分 $(q,d)$ 相关性即可省掉线上重复推理
- 进阶版：利用缓存召回扩展文档集合（3000–6000/每 query），离线计算这些文档的精排相关性并存盘；线上粗排前读取并“命中则跳过推理”

这里的关键是：

- KV 可以建在磁盘上（几十毫秒读盘可接受），因为它发生在粗排开始之前，RT 预算相对宽松

---

## 18.3 反向召回：离线从“文档→query”，挖掘新二元组

反向召回的目标是：

- 用离线复杂模型挖掘大量高相关的 $(q,d)$
- 形成新的 $q \to List\langle d\rangle$ 索引
- 重点提升中尾部 query 的召回供给

本章强调，想要有效必须满足两条：

1. 给定 $d$，挖掘出的候选 $\{q\}$ 大多数与 $d$ 真相关（否则会浪费大量相关性打分）
2. 与线上文本/向量召回有足够差异（否则只是在重复已有结果，收益有限）

### 18.3.1 doc2query（D2Q）：把文档生成查询词

本章落地的是 D2Q：

- 用 Transformer 编码文档、解码生成 query
- 训练数据来自搜索日志挖掘的高质量 $(q,d)$（头部 query + 高排名 + 高 CTR + 高相关档位）

工程关键点：

- **数据质量的重要性大于模型结构本身**
- 训练分两阶段：海量日志预训练 + 少量人工标注精调

离线推理（回刷文档库）时：

- 过滤字数过少/质量过差的文档
- 为每篇文档生成尽量多的 query（可用随机采样多次；beam search 更好但成本更高）

离线评估指标（本章给了两个很实用的）：

- 相关性模型通过率（生成的 $(q,d)$ 被相关性模型判高分的比例）
- 高相关 query 生成量（平均每文档能生成多少条高相关 query）

---

## 18.4 改写 + 缓存召回：把长尾“折叠”到头部索引

本章最后把第 15 章改写接了进来：

- 离线把任意 $q$ 改写到头部 $q'$
- 再用 $q'$ 命中缓存召回的高质量文档

链路形态：

- $q \to q' \to d$

这是一种非常常见的“工程组合拳”：

- 改写解决“命中索引”的问题
- 缓存召回保证“命中的结果质量”

---

## 18.5 常见坑与检查清单

- 缓存召回不做实验隔离：后续所有 A/B 收益被稀释
- 离线链路刷新周期过长：新文档/新热点进不来，强时效 query 体验差
- 反向召回生成的 query 与线上召回重合太高：算力花了但增量供给很少
- KV 资源不给够：离线召回本来是“稳态兜底”，结果线上 KV 也超时就失去意义

---

## 18.6 练习（建议动手）

1. 设计一个缓存召回的融合打分公式：频率/CTR/质量/时效各占多大权重？如何随 query 意图调整？
2. 写一份“实验隔离”方案：哪些日志要打标签、离线处理怎么过滤、如何防止漏标。
3. 做一个 D2Q 的玩具版：用你能获取的数据训练一个“标题→query”模型，看生成的 query 是否能补齐长尾检索。

---

## 18.7 与其它章节的连接

- 与第 15 章：改写把长尾映射到头部，提升离线索引命中率
- 与第 17 章：离线召回与向量召回互补；离线更擅长“用复杂模型挖中尾增量”
- 与第 19 章：离线链路的核心是“用更强排序离线定序”，把排序能力前移到召回阶段

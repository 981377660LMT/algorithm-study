这是一份关于搜索引擎查询处理（Query Processing, QP）核心模块的深度技术分析。基于提供的五个章节文档，我将从**分词与实体识别、词权重、类目与意图、查询词改写**四个维度，剖析工业界搜索引擎如何通过深度学习与统计特征，解决“用户意图理解”与“文档召回”之间的语义鸿沟。

---

### 1. 分词与命名实体识别 (Tokenization & NER)

分词是搜索系统的基石，其核心挑战在于**新词发现**与**歧义消除**。

#### 💡 核心技术剖析

- **新词发现的统计学之美**：
  - 文档明确指出，仅靠词典无法覆盖日新月异的互联网词汇（如流行语、新专名）。
  - **凝聚度 (Cohesion, PMI)**：衡量“内部结合的紧密度”。例如“电影院”内部 $P(电影院) \gg P(电影) \cdot P(院)$，说明其是一个整体；而“电影的”则不然。
  - **自由度 (Freedom, Entropy)**：衡量“外部搭配的丰富度”。通过计算左右邻字的信息熵（Entropy），判断一个字符串是否能独立存在。如“和国”虽然内部常见，但左邻字几乎固定为“共”，自由度低，故不是词。
  - **洞察**：这种非监督的统计方法是构建动态词典的关键，它摆脱了对人工标注的依赖。

- **序列标注的演进（BERT + CRF）**：
  - 从 HMM 到 BERT+CRF，本质是建模能力的升级。CRF（条件随机场）的引入由为关键，它利用转移矩阵（Transition Matrix）约束了标签的合法性（如 `B` 后面不能直接接 `B`），解决了单纯 BERT分类时产生的局部标签冲突问题。
  - **LEBERT (Lexicon Enhanced BERT)**：针对中文特性，将“词汇特征”融入“字粒度”模型。这是一个重要的工程优化点：它在保持 BERT 细粒度建模优势的同时，引入了先验的词汇知识，提升了边界识别的准确率。

### 2. 词权重 (Term Weighting)

词权重不仅仅是去停用词，而是对**查询意图的量化解构**。

#### 💡 核心技术剖析

- **四级权重体系**：
  - 从 **核心词 (3)** 到 **冗余词 (0)** 的划分，直接决定了检索系统的**召回逻辑**（AND/OR逻辑的降级策略）。
  - **上下位词的权重悖论**：如果下位词独一无二（“人大附中”），上位词（“北京市”）即为冗余；若下位词有歧义（“德州”），上位词（“山东”）则为强需求。这体现了权重计算必须依赖**全局上下文信息**，而非局部词频。

- **Attention 即权重**：
  - 文档介绍了一种巧妙的**无监督（弱监督）方法**：利用 Query-Doc 相关性模型中的 **Attention Layer**。
  - **原理**：在相关性训练中，如果某个词对 $Score(Q, D)$ 的贡献大（Attention Weight 高），则说明它在检索中至关重要。
  - **洞察**：这打破了传统依赖人工标注（昂贵、滞后）的局限，实现了权重的端到端学习，且直接对齐最终的检索目标——相关性。

### 3. 类目与意图识别 (Category & Intent)

这是决定搜索“出什么资源”和“怎么排”的决策中枢。

#### 💡 核心技术剖析

- **多标签层次分类 (Multi-label Hierarchical Classification)**：
  - **递归正则 (Recursive Regularization)**：在训练多级类目（如 一级:服装 -> 二级:冬装）时，强制让父子类目的模型参数保持接近。这是一种利用**结构化先验知识**约束模型空间的优雅方法，能有效缓解长尾细分类目训练数据稀疏的问题。
  - **Focal Loss**：在类目识别中，正负样本极端不平衡（一个Query只属于少数类目）。Focal Loss 通过 $(1-p)^\gamma$ 降低简单负样本的权重，迫使模型关注那些“难以区分”的样本。

- **查询词类目的“后预训练” (Post-Pretraining)**：
  - Query 短且语义稀疏，直接分类极难。文档提出的解决方案是：利用**点击日志**，将 Query 点击的高频文档的类目作为 Query 的伪标签（Weak Label），在 Fine-tuning 之前增加一步领域数据的 Pre-train。这是弥补 Query 信息量不足的有效手段。

- **意图对下游的控制力**：
  - 意图识别不仅是打标签，而是**资源调度器**。
  - **LBS (Location Based)**：区分“显式”（附近美食）与“隐式”（星巴克）。隐式意图采用了**多队列混排（Fusion）**策略，即混合“距离优先”和“相关性优先”两个队列，动态调整配比。

### 4. 查询词改写 (Query Rewriting)

被誉为“撬动大盘指标最有效的手段”，其本质是解决**语义鸿沟（Semantic Gap）**。

#### 💡 核心技术剖析

- **改写的非对称性 (Asymmetry)**：
  - **上位词改写为下位词 (General $\to$ Specific) 是安全的**：搜“民族舞”（宽泛），召回“蒙古舞”（具体）文档是合理的。
  - **下位词改写为上位词 (Specific $\to$ General) 是危险的**：搜“浦东餐厅”（具体），召回“上海餐厅”（宽泛，包含闵行区）是错误的。
  - **判别器设计**：因此，改写判别模型（Discriminator）必须是非对称的 $f(q, q') \neq f(q', q)$，这与双塔召回模型的对称性形成互补。

- **基于相关性的改写框架**：
  - 这不是简单的同义词替换，而是建立 $Q \to Q' \to D$ 的链路。目标不是 $Q$ 和 $Q'$ 字面相似，而是 **$Q$ 与 $Q'$ 召回的 $D$ 极其相关**。
  - **数据构造**：构建 Query-Doc 二部图，利用共点击（Co-click）挖掘改写对，并用强相关性模型清洗数据。
  - **离线漏斗（Offline Funnel）**：利用离线计算资源，通过 召回 $\to$ 判别 $\to$ 验证 的三级漏斗生成高质量改写表（Lookup Table）。这种“空间换时间”的策略，使得线上只需查表即可获得深度模型的推理效果。

---

### 🚀 总结与真知灼见

1.  **从规则到概率的全面融合**：传统的 QP 强依赖词典和规则（同义词表、停用词表），而现代 QP 的核心在于**利用海量用户行为数据（点击、Query Log）反哺语义理解**。无论是发现新词、计算权重，还是生成改写，本质都是在拟合用户的点击分布。
2.  **语义召回的“双保险”**：**查询词改写**与**向量召回（Embedding Retrieval）**殊途同归。改写试图将 Query 映射到“容易被索引命中的文本空间”，向量试图将 Query 映射到“稠密语义空间”。在工业界，精细化的改写往往比粗糙的向量召回更可控、更具解释性。
3.  **非对称性的工程哲学**：在 NER、权重、改写中，反复体现了**上下文敏感**和**方向敏感**。词的重要性取决于语境，改写的方向决定了召回的精度。简单的“相似度”度量往往失效，必须引入深度交互模型（如 BERT 判别器）来解决这些不对称问题。

---

# 🔍 QP模块深度解析：查询词处理的完整链路（第11-15章）

**QP（Query Processing）是搜索引擎的"语言理解层"**，位于用户输入与召回排序之间。它决定了搜索引擎能否"听懂"用户在说什么。

---

## 📝 第11章：分词与命名实体识别 —— 中文搜索的第一道坎

### 🧠 核心问题：中文为何需要分词？

```
英文: "I love machine learning"  → 天然有空格分隔
中文: "我爱机器学习"              → 我/爱/机器/学习 还是 我/爱/机器学习？
```

**这不是小问题！** 分词错误会导致召回失败：

```python
# 经典歧义案例
"研究生命起源"
→ 正确: 研究/生命/起源  (研究生命的起源)
→ 错误: 研究生/命/起源  (研究生的命运起源？)

"在野生动物园玩"
→ 正确: 在/野生动物园/玩
→ 错误: 在野/生动/物园/玩
```

### 🎯 三种分词方法的本质对比

| 方法         | 原理          | 优势                   | 致命缺陷               |
| ------------ | ------------- | ---------------------- | ---------------------- |
| **最大匹配** | 贪心选最长词  | O(n) 极快              | 无法处理歧义           |
| **最短路径** | DAG图找最短路 | 考虑全局最优           | 依赖词典，无法识别新词 |
| **BERT+CRF** | 序列标注      | 准确率最高，有泛化能力 | 推理代价大             |

### 💡 CRF为什么能解决"B后面不能接S"的问题？

**关键洞察**：CRF学习的是**转移矩阵**，而非独立预测每个位置。

```python
# 标签集合: B(开始), M(中间), E(结束), S(单字)

# CRF转移矩阵（理想情况）
#      B     M     E     S
# B   -∞    3     5    -∞    ← B后只能接M或E
# M   -∞    2     6    -∞    ← M后只能接M或E
# E    2   -∞    -∞    3     ← E后只能接B或S（新词开始）
# S    2   -∞    -∞    2     ← S后只能接B或S

# 物理意义：
# t_{B→E} = 5 表示"单个词以B开头E结尾"的可能性很高
# t_{B→S} = -∞ 表示"B后面不可能直接接S"
```

**为什么现在大家只用BERT不用CRF？**

```
预训练越来越好 → BERT已经学到了隐式的转移规则
→ CRF的边际收益趋近于0
→ 但增加了推理复杂度
```

### 📊 词典构造：凝聚度与自由度的数学之美

**问题**：如何自动判断一个字符串是否成词？

**错误思路**："电影院"出现频率高就是词？

**反例**：

```python
# "共和国"出现频率高
# 但"和国"也因此频率很高！
# 那"和国"是词吗？显然不是！
```

**正确方案**：凝聚度 + 自由度双重判定

#### **凝聚度：词内部字符的"黏合力"**

```python
# PMI(A, B) = log₂[p(AB) / (p(A) × p(B))]

# 例子：电影院
p(电影) = 10⁻⁴
p(院) = 2×10⁻⁴
p(电影院) = 7×10⁻⁶

PMI(电影, 院) = log₂(7×10⁻⁶ / (10⁻⁴ × 2×10⁻⁴))
              = log₂(350) ≈ 8.5  # 非常高！

# 对比：电影的
p(电影) = 10⁻⁴
p(的) = 1.7×10⁻²
p(电影的) = 1.6×10⁻⁵

PMI(电影, 的) = log₂(9.4) ≈ 3.2  # 低得多
```

**物理意义**：

```
PMI高 → "这两个字经常一起出现，不是碰巧撞上的" → 可能成词
PMI低 → "只是因为两个字都高频才一起出现" → 不成词
```

#### **自由度：词与外界的"可组合性"**

```python
# 用信息熵衡量
H_left(A) = -Σ p(l|A) × log₂ p(l|A)

# 例子对比
"电影院" 左邻字: 到、去、在、大、小、新...
→ H_left(电影院) 很高

"和国" 左邻字: 几乎只有"共"
→ H_left(和国) ≈ 0  # 说明它只是"共和国"的子串！
```

**最终判定公式**：

```python
is_word = (凝聚度 > 阈值1) AND (min(H_left, H_right) > 阈值2)
```

### 🏷️ 命名实体识别（NER）：比分词更精细

```python
# 输入
"杨幂代言的雅诗兰黛口红"

# 输出（BIO标注）
杨  幂  代  言  的  雅  诗  兰  黛  口  红
B-人物 I-人物 O   O   O  B-品牌 I-品牌 I-品牌 I-品牌 B-品类 I-品类
```

**NER的业务价值**：

- 识别出的实体是**相关性模型、点击率模型的关键特征**
- 可以用于构建知识图谱、实现属性检索

---

## ⚖️ 第12章：词权重 —— 哪些词可以丢？

### 🎯 核心问题定义

```
输入: q = "冬季/风衣/推荐"
输出: [风衣:3, 冬季:2, 推荐:1]
       ↑核心词  ↑强需求   ↑弱需求
```

**词权重的本质问题**：如果召回结果不够，可以丢掉哪个词？

### 📊 四档权重的精确定义

| 档位       | 分数 | 定义                                 | 例子                   |
| ---------- | ---- | ------------------------------------ | ---------------------- |
| **核心词** | 3    | 去掉后查询词意图**完全改变**         | "风衣"在"冬季风衣推荐" |
| **强需求** | 2    | 去掉后意图**有所改变**，但仍可能搜到 | "冬季"在"冬季风衣推荐" |
| **弱需求** | 1    | 去掉后主意图**基本不变**             | "推荐"在"冬季风衣推荐" |
| **冗余词** | 0    | 去掉后意图**几乎不变**               | "的"在任何查询中       |

### 💡 标注中的深层智慧

**上下位词规则**：

```python
# 情况1：下位词是独一无二的
"北京市/人大附中/的/学区房"
→ 世界上只有一个人大附中
→ "北京市"是冗余词(0分)

# 情况2：下位词有歧义
"狄仁杰/王者荣耀"
→ 狄仁杰可以是：历史人物、电视剧、游戏角色
→ "王者荣耀"是强需求词(2分)，不能丢！
```

**重复意图规则**：

```python
"适合/情侣/去/的/地方"
→ "去"和"地方"意图重复
→ 两者权重相同
→ 如果只删一个，意图不变；同时删除才改变
```

### 🔧 基于注意力机制的无监督方法

**核心思想**：不用人工标注，用相关性数据间接学习词权重

```python
# 双塔模型
query_vectors = BERT([CLS, t₁, t₂, ..., tₘ])  # 词级输入
doc_vectors = BERT([CLS, w₁, w₂, ..., wₙ])

# 交叉注意力
# Q = [CLS]向量, K = V = [t₁, t₂, ..., tₘ]向量
α = softmax(c^T @ A @ [u₁, ..., uₘ])  # 这就是词权重！

# 最终表示
x_q = Σᵢ αᵢ × B × uᵢ
```

**物理意义**：

```
α_i ≈ 1 → 词t_i对查询词表示贡献最大 → 核心词
α_i ≈ 0 → 词t_i几乎不贡献 → 冗余词
```

---

## 🏷️ 第13章：类目识别 —— 多标签层次分类

### 🧠 问题建模

```python
# 输入
q = "冬季卫衣推荐"

# 输出（多标签！）
{
    "一级类目": ["时尚"],
    "二级类目": ["穿搭"]
}

# 多意图查询词
q = "狄仁杰"
{
    "一级类目": ["影视娱乐", "游戏"],  # 两个一级类目！
    "二级类目": ["电视", "手游"]
}
```

### 🎯 多标签分类 vs 多分类

**关键区别**：

```python
# 多分类（softmax）
p = softmax(logits)  # Σp_i = 1，只能选一个

# 多标签分类（sigmoid）
p = sigmoid(logits)  # 每个p_i独立，可以多个都>0.5
```

### 💎 三个关键改进技术

#### **1. Focal Loss：解决类别不平衡**

```python
# 原始BCE
L = -y·ln(p) - (1-y)·ln(1-p)

# Focal Loss
L = -y·(1-p)^γ·ln(p) - (1-y)·p^γ·ln(1-p)
          ↑
    调节因子：预测越准，权重越小
```

**物理意义**：

```
容易分对的样本 → 梯度变小 → 模型不再关注
容易分错的样本 → 梯度变大 → 模型重点学习
```

#### **2. 递归正则：利用类目层次结构**

```python
# 类目层次
美妆 → [彩妆, 护肤, 美甲, 香水, 医美]

# 递归正则
R(W) = Σᵢ Σⱼ∈N(i) ||wᵢ - wⱼ||²

# 物理意义
# 同一个一级类目下的二级类目，参数应该接近
# 彩妆和护肤的分类器参数应该相似
```

#### **3. 后预训练：解决查询词太短的问题**

```
问题：查询词只有几个字，信息量太少，类目难判！

解决方案：
1. 挖掘搜索日志：找到查询词q的高相关文档{d₁,...,dₖ}
2. 统计文档类目 → 得到查询词的伪标签
3. 用数百万条这样的数据做后预训练
4. 再用人工标注数据做微调
```

### 📊 评价指标：Micro F1 vs Macro F1

```python
# Micro F1：汇总所有类目的TP/FP/FN后计算
TP = Σ TPᵢ, FP = Σ FPᵢ, FN = Σ FNᵢ
precision = TP / (TP + FP)
recall = TP / (TP + FN)
micro_F1 = 2 × precision × recall / (precision + recall)

# Macro F1：先计算每个类目的F1，再平均
macro_F1 = (1/k) × Σᵢ F1ᵢ
```

**选择依据**：

```
Micro F1 → 更关注整体表现，高频类目权重大
Macro F1 → 更关注各类目均衡，低频类目同等重要
```

---

## 🎯 第14章：意图识别 —— 撬动大盘的杠杆

### 💡 核心洞察

> **"最容易撬动搜索大盘指标增长的，一个是意图识别，另一个是查询词改写"**

**为什么意图识别如此重要？**

```
意图识别结果 → 决定调用哪条链路！
            → 不只是特征，而是控制流！
```

### 🕐 时效性意图：三类时效性的精确区分

| 类型           | 定义               | 例子                     | 处理策略         |
| -------------- | ------------------ | ------------------------ | ---------------- |
| **突发时效性** | 刚发生的热点事件   | "某明星离婚"             | 优先新文档索引   |
| **一般时效性** | 对新鲜度有持续需求 | "最新手机测评"           | 文档年龄权重调高 |
| **周期时效性** | 周期性出现的需求   | "春节红包"、"双十一攻略" | 根据当前时间判断 |

**一般时效性还细分为强/中/弱/无！**

```python
# 时效性强度 → 文档年龄在排序中的权重
score = α × relevance + β × quality + γ(时效强度) × freshness
                                      ↑
                            时效性越强，γ越大
```

### 📍 地域性意图：三个维度的判定

```python
# 维度1：是否对距离敏感
"附近的美食" → 步行可达 → 敏感
"周边游"     → 驾车可达 → 不敏感

# 维度2：地域性意图强弱
"附近的美食" → 强意图（明确说"附近"）
"日料店"     → 弱意图（可能想搜附近，可能不是）

# 维度3：显式 vs 隐式
"附近的美食" → 显式（可用规则识别）
"日料店"     → 隐式（需要语义模型判断）
```

**地域性意图的处理策略**：

```python
if 地域意图 == "强":
    # 只从附近召回
    docs = retrieve(q, filter=distance < threshold)
elif 地域意图 == "弱":
    # 多队列混排
    nearby_docs = retrieve(q, filter=distance < threshold)
    global_docs = retrieve(q, no_filter=True)
    final_docs = interleave(nearby_docs, global_docs, ratio=意图强度)
```

### 👤 用户名意图：社交媒体搜索的特殊需求

```python
# 用户搜"李佳琦Austin"
# 错误处理：只出别人对李佳琦的讨论
# 正确处理：
#   1. 识别出这是用户名搜索意图
#   2. 在显著位置展示账号卡片
#   3. 同时展示该KOL发布的文档
```

**实现策略**：

```
用户ID意图 → 规则判断（符合ID格式 + ID库匹配）
用户名意图 → KOL用户名库匹配（完全/部分命中）
```

### 🛒 求购意图：电商搜索的核心

```python
# 用户搜"阿迪达斯"
# 识别出求购意图后：

# 召回阶段
standard_docs = standard_retrieval(q)  # 普通内容
products = product_index.search(q)      # 商品
shops = shop_index.search(q)            # 店铺
lives = live_index.search(q)            # 直播

# 排序阶段：多队列混排
# 商品排序依据 = 价格 × 转化率（预估营收）
```

---

## ✏️ 第15章：查询词改写 —— 召回增强的核武器

### 🎯 改写的目标：不是让q和q'相似！

**这是最常见的误解！**

```python
# 错误理解
改写目标 = max sim(q, q')

# 正确理解
改写目标 = max rel(q, d)  where d ∈ retrieve(q')
         = 让q和q'召回的文档d相关！
```

**反例**：

```
q  = "男士夏季穿搭建议"
q' = "夏季穿搭"
d  = "夏季女大学生穿搭小技巧"

sim(q, q') 高 ✓
sim(q', d) 高 ✓
rel(q, d) 低 ✗  ← 这才是真正的评价标准！
```

### 📚 三类改写方法的完整图谱

| 方法               | 时机       | 原理                 | 优势             | 局限             |
| ------------------ | ---------- | -------------------- | ---------------- | ---------------- |
| **基于分词的改写** | 分词之后   | 同义词/上下位词替换  | 简单、可解释     | 依赖词表质量     |
| **基于相关性改写** | 分词之前   | 召回模型+判别模型    | 泛化能力强       | 计算量大         |
| **基于意图的改写** | 意图识别后 | 根据意图做结构化改写 | 解决文本召回盲区 | 需要意图识别准确 |

### 🔄 基于分词的改写：词粒度的精细操作

#### **同义词替换**

```python
q = "民族舞 教程"
→ (民族舞 OR 民间舞) AND (教程 OR 教学)
```

#### **上下位词替换（单向！）**

```python
# ✓ 上位词 → 下位词（扩大召回）
"民族舞" → ["蒙古舞", "新疆舞", "藏族舞"]
"上海" → ["黄浦区", "浦东新区"]

# ✗ 下位词 → 上位词（会引入不相关！）
"浦东的餐厅" → "上海的餐厅"
→ 可能召回"闵行区的餐厅"，不符合意图！
```

**规则改写**：

```python
"双11" ↔ "双十一"
"reinforcement learning教程" ↔ "强化学习教程"
```

### 🧠 基于相关性的改写：工业级方案

#### **数据构造的精妙设计**

```python
# 步骤1：准备两个查询词集合
T = top_500万_high_freq_queries  # 头部查询词
Q = 数千万_all_queries           # 全量查询词

# 步骤2：构造候选改写对 q → q'
# 方法A：语义相似
for q in Q:
    q' = find_most_similar_in_T(bert_embedding(q))

# 方法B：行为共现
# 构造二部图：q → doc ← q'
# 共同邻居(文档)越多，关联越强

# 步骤3：计算改写质量分数
for (q, q') in candidates:
    docs = get_high_rel_docs(q')  # q'的高相关文档
    y = mean([rel(q, d) for d in docs])  # q与这些文档的相关性
    dataset.append((q, q', y))
```

**这个设计的深层智慧**：

```
改写质量 = q与q'召回文档的相关性
         ≠ q与q'的相似性

这正是改写的终极目标！
```

#### **改写召回模型（双塔）**

```python
# 训练
x = bert_encode(q)
x' = bert_encode(q')
loss = BCE(sigmoid(x · x'), y)

# 推理
x = bert_encode(q)
candidates = faiss_search(x, index_of_T, topk=100)
```

#### **改写判别模型（交叉BERT）**

**为什么需要判别模型？**

```python
# 双塔模型的致命缺陷：对称性！
# 如果认为 q → q' 合理
# 也会认为 q' → q 合理

# 例子
"上海的餐厅" → "浦东的餐厅" ✓（上位→下位，合理）
"浦东的餐厅" → "上海的餐厅" ✗（下位→上位，不合理）

# 但双塔模型无法区分！
```

**判别模型设计**：

```python
# 交叉BERT（非对称！）
p = bert_cross([CLS] + q + [SEP] + q')

# 因为位置不同，所以：
f(q, q') ≠ f(q', q)  # 天然解决上下位问题！
```

### ⚡ 离线改写：三级漏斗

```python
# 对于每个查询词q，离线构造改写索引

# 第一级：改写召回
candidates_l1 = recall_model.get_topk(q, k=300)
candidates_l1 += generation_model.generate(q, k=50)

# 第二级：判别器粗排
for q' in candidates_l1:
    score = discriminator(q, q')
candidates_l2 = topk(candidates_l1, by=score, k=50)

# 第三级：相关性精排
for q' in candidates_l2:
    docs = get_high_rel_docs(q')
    score = mean([rel(q, d) for d in docs])
candidates_l3 = topk(candidates_l2, by=score, k=5)

# 存储
index[q] = candidates_l3
```

**工程洞察**：

```
离线：无时间约束 → 用最复杂的方法获得最优质结果
在线：有严格RT → 直接查索引，fallback到召回模型
```

### 🎯 基于意图的改写：解决文本召回盲区

#### **属性检索意图**

```python
# 用户搜："60寸3000元以下电视机"
# 问题：文档"2000元55寸电视"明明符合要求，但文本搜不到！

# 解决方案：结构化改写
q = "60寸3000元以下电视机"
→ {
    "品类": "电视机",
    "尺寸": {"op": "=", "value": 60},
    "价格": {"op": "<", "value": 3000}
}

# 召回阶段做属性过滤
docs = retrieve("电视机",
                filter=(尺寸==60 AND 价格<3000))
```

#### **地域性意图**

```python
# 用户搜："附近的火锅店"

# 改写后
q' = "火锅店"  # 去掉"附近的"
context = {"geohash": user_location}

# 召回
docs = retrieve(q', filter=distance < 2km)
```

---

## 🧩 QP完整链路：五个模块的协作

```
用户输入: "杨幂代言的雅诗兰黛口红附近能买吗"

┌──────────────────────────────────────────────────────────────┐
│ 【第11章】分词 + NER                                          │
│ → 杨幂/代言/的/雅诗兰黛/口红/附近/能/买/吗                    │
│ → NER: 杨幂=人物, 雅诗兰黛=品牌, 口红=品类                     │
└──────────────────────────────────────────────────────────────┘
                            ↓
┌──────────────────────────────────────────────────────────────┐
│ 【第12章】词权重                                              │
│ → 口红:3, 雅诗兰黛:3, 杨幂:2, 附近:2, 代言:1, 的:0, 能:0...   │
└──────────────────────────────────────────────────────────────┘
                            ↓
┌──────────────────────────────────────────────────────────────┐
│ 【第13章】类目识别                                            │
│ → 一级类目: 美妆                                              │
│ → 二级类目: 口红                                              │
└──────────────────────────────────────────────────────────────┘
                            ↓
┌──────────────────────────────────────────────────────────────┐
│ 【第14章】意图识别                                            │
│ → 求购意图: True                                              │
│ → 地域意图: 强（"附近"）                                      │
│ → 品牌意图: True                                              │
└──────────────────────────────────────────────────────────────┘
                            ↓
┌──────────────────────────────────────────────────────────────┐
│ 【第15章】查询词改写                                          │
│ → 同义词改写: "雅诗兰黛" → ["Estée Lauder", "EL"]             │
│ → 意图改写: 去掉"附近"，传递geohash给召回                     │
│ → 结构化改写: 品牌=雅诗兰黛, 品类=口红                        │
└──────────────────────────────────────────────────────────────┘
                            ↓
                   【召回】【排序】
```

---

## 💎 真知灼见总结

### 1️⃣ QP是搜索引擎的"语言理解层"

```
用户说的 → QP理解 → 系统能执行的
自然语言    结构化     召回指令
```

### 2️⃣ 每个模块都有其不可替代的价值

```
分词: 没有分词 → 无法做文本召回
词权重: 没有词权重 → 无法做召回扩展
类目: 没有类目 → 相关性模型缺少关键特征
意图: 没有意图识别 → 无法选择正确的召回/排序策略
改写: 没有改写 → 无法解决语义鸿沟问题
```

### 3️⃣ 数据质量 > 模型复杂度

```
分词: 需要高质量词典 + 标注数据
词权重: 需要精心设计的标注标准
类目: 需要后预训练解决数据稀疏
改写: 需要挖掘搜索日志构造训练数据
```

### 4️⃣ 工程与算法的平衡

```
在线: BERT推理慢 → 需要缓存/蒸馏/离线计算
离线: 时间充裕 → 可以用复杂方法获得高质量结果
```

### 5️⃣ 这些模块相互依赖，形成闭环

```
分词结果 → 影响词权重计算
词权重 → 影响改写策略（删哪个词）
意图 → 决定改写方向
类目 → 作为下游链路特征
```

---

**这就是工业级搜索引擎QP模块的完整图景！** 🚀

---

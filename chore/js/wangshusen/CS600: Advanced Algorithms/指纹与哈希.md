在高级算法中，指纹（Fingerprinting）与哈希技术用于将大规模数据映射为短摘要，以实现快速检索、验证或相似性匹配。

### 17.1 哈希表 (Hash Table)

哈希表通过哈希函数将键（Key）映射到桶（Bucket）索引，实现平均 $O(1)$ 的操作复杂度。

- **冲突解决 (Collision Resolution)**：
  - **拉链法 (Chaining)**：桶中存储链表。
  - **开放寻址法 (Open Addressing)**：如线性探测、二次探测、双重哈希。
- **全域哈希 (Universal Hashing)**：
  从一个哈希函数族中随机选择函数，保证对于任意两个不同的键，它们发生冲突的概率不超过 $1/m$（$m$ 为桶数）。这能防止针对特定输入的最坏情况攻击。
- **完美哈希 (Perfect Hashing)**：
  对于静态键集，可以构建两级哈希结构（FKS 方案），实现 **$O(1)$ 的最坏情况**查询时间。

### 17.2 抗碰撞哈希 (Collision-Resistant Hash)

在密码学和数据完整性校验中，哈希函数 $H$ 需要具备以下特性：

- **抗原像性 (Pre-image Resistance)**：给定 $h$，很难找到 $x$ 使得 $H(x) = h$。
- **抗第二原像性 (Second Pre-image Resistance)**：给定 $x$，很难找到 $y \neq x$ 使得 $H(y) = H(x)$。
- **抗碰撞性 (Collision Resistance)**：很难找到任意两个不同的 $x, y$ 使得 $H(x) = H(y)$。
- **应用**：数字签名、Merkle Tree（用于区块链和文件校验）、指纹识别（如 Rabin Fingerprint 用于字符串匹配）。

### 17.3 局部敏感哈希 (Locality Sensitive Hashing, LSH)

局部敏感哈希（Locality Sensitive Hashing, LSH）是处理海量高维数据近似最近邻搜索（Approximate Nearest Neighbor, ANN）的核心技术。

### 核心思想

传统的哈希算法（如 MD5, SHA-1）致力于避免冲突：即使输入仅相差一个比特，输出的哈希值也应截然不同。

**LSH 的目标恰恰相反**：它设计特定的哈希函数族，使得**相似的数据点以更高的概率映射到同一个哈希桶（Bucket）中，而不相似的数据点以极低的概率碰撞**。

### 数学定义

一个哈希函数族 $H = \{h: S \to U\}$ 被称为 $(R, cR, P_1, P_2)$-敏感的，如果对于任意 $p, q \in S$：

1.  如果 $d(p, q) \le R$，则 $Pr[h(p) = h(q)] \ge P_1$
2.  如果 $d(p, q) \ge cR$，则 $Pr[h(p) = h(q)] \le P_2$

其中 $c > 1$ 且 $P_1 > P_2$。这意味着距离近的点碰撞概率高，距离远的点碰撞概率低。

### 常见的 LSH 方案

不同的距离度量对应不同的 LSH 算法：

1.  **Jaccard 距离 (集合相似度) -> MinHash**

    - **原理**：对集合特征矩阵进行随机排列，记录第一个出现 `1` 的行索引。
    - **性质**：$Pr[h(A) = h(B)] = Jaccard(A, B)$。

2.  **欧氏距离 ($L_2$ Norm) -> P-Stable Distributions (p-稳定分布)**

    - **原理**：将高维向量投影到一条随机直线上，并进行分段量化。
    - **公式**：$h_{\mathbf{a}, b}(\mathbf{v}) = \lfloor \frac{\mathbf{a} \cdot \mathbf{v} + b}{r} \rfloor$
      - $\mathbf{a}$：分量服从高斯分布的随机向量。
      - $b$：$[0, r]$ 上的均匀随机数。
      - $r$：桶宽参数。

3.  **余弦相似度 (Cosine Similarity) -> SimHash / Random Projection**
    - **原理**：用随机超平面切割空间。
    - **公式**：$h(\mathbf{v}) = sign(\mathbf{v} \cdot \mathbf{r})$，其中 $\mathbf{r}$ 是随机单位向量。
    - **性质**：碰撞概率与向量夹角成线性关系。

### 放大技术 (AND & OR Construction)

单个哈希函数的区分能力通常不足（$P_1$ 和 $P_2$ 差距不够大）。我们通过组合哈希函数来放大差距：

1.  **AND 构造 (行带化)**：将 $k$ 个哈希函数组合成一个新函数 $H(x) = (h_1(x), ..., h_k(x))$。只有当所有 $k$ 个值都相等时才算碰撞。这降低了整体碰撞率，极大地压制了不相似点的误报（False Positive），但也降低了召回率。
2.  **OR 构造 (多表哈希)**：构建 $L$ 个哈希表。只要在任意一个哈希表中碰撞，就视为候选。这提高了召回率。

**LSH 的标准范式**是结合两者：构建 $L$ 个哈希表，每个表由 $k$ 个基础哈希函数组成。

### 代码示例：基于随机投影的 LSH (余弦相似度)

以下是一个简单的 Python 实现，展示如何使用随机超平面处理高维向量。

```python
import numpy as np
from collections import defaultdict

class CosineLSH:
    def __init__(self, input_dim, num_tables=5, hash_size=10):
        """
        input_dim: 输入向量维度
        num_tables: 哈希表数量 (L) - 增加召回率
        hash_size: 每个表的哈希位数 (k) - 增加精确度
        """
        self.num_tables = num_tables
        self.hash_size = hash_size
        self.tables = [defaultdict(list) for _ in range(num_tables)]

        # 生成随机投影向量 (超平面法向量)
        # 形状: [num_tables, hash_size, input_dim]
        self.projections = np.random.randn(num_tables, hash_size, input_dim)

    def _generate_hash(self, vector, table_idx):
        """
        计算单个表的哈希指纹
        h(v) = sign(v · r)
        """
        # 投影: (hash_size, input_dim) dot (input_dim,) -> (hash_size,)
        proj_vals = np.dot(self.projections[table_idx], vector)
        # 量化为二进制: >0 为 1, <=0 为 0
        bits = (proj_vals > 0).astype(int)
        # 将位数组转换为元组作为字典键
        return tuple(bits)

    def add(self, vector_id, vector):
        """添加向量到索引"""
        for i in range(self.num_tables):
            fingerprint = self._generate_hash(vector, i)
            self.tables[i][fingerprint].append(vector_id)

    def query(self, vector):
        """查询近似最近邻"""
        candidates = set()
        for i in range(self.num_tables):
            fingerprint = self._generate_hash(vector, i)
            # 获取该桶中的所有 ID
            if fingerprint in self.tables[i]:
                candidates.update(self.tables[i][fingerprint])
        return list(candidates)

# 使用示例
if __name__ == "__main__":
    dim = 128
    lsh = CosineLSH(input_dim=dim, num_tables=5, hash_size=8)

    # 模拟数据
    np.random.seed(42)
    data_base = np.random.randn(1000, dim)

    # 索引数据
    for idx, vec in enumerate(data_base):
        lsh.add(idx, vec)

    # 查询向量 (取第0个向量并加一点噪声)
    query_vec = data_base[0] + np.random.randn(dim) * 0.1

    result_ids = lsh.query(query_vec)
    print(f"查询结果候选集大小: {len(result_ids)}")
    if 0 in result_ids:
        print("成功召回目标向量 (ID: 0)")
```

### 工业界应用与优化

1.  **多探针 LSH (Multi-Probe LSH)**：

    - 标准的 LSH 只检查查询向量落入的那个桶。
    - 多探针策略不仅检查主桶，还检查“相邻”的桶（哈希值仅相差 1 位或数值接近的桶）。这允许我们使用更少的哈希表（节省内存）达到相同的召回率。

2.  **C2LSH (Collision Counting LSH)**：

    - 动态选择碰撞阈值，而不是固定的 AND/OR 结构。

3.  **与 HNSW 等图算法对比**：
    - LSH 的优势在于**理论保证**、**支持动态更新**（插入删除快）以及**处理超高维稀疏数据**（如文本 MinHash）。
    - 在稠密向量（如 Embedding）场景下，基于图的算法（HNSW）通常比 LSH 具有更好的性能/召回率权衡。

```python
import numpy as np

class SimpleLSH:
    def __init__(self, dim, num_bits=16):
        # 1. 准备“尺子”：随机生成 num_bits 个超平面（向量）
        # 这些向量把空间切成了很多份
        self.planes = np.random.randn(dim, num_bits)
        self.buckets = {}

    def _hash(self, vec):
        # 2. 投影：向量点乘矩阵
        projections = np.dot(vec, self.planes)
        # 3. 量化：大于0记为1，小于0记为0，生成一串二进制指纹
        # 例如：'1011001...'
        fingerprint = "".join(['1' if x > 0 else '0' for x in projections])
        return fingerprint

    def add(self, vec_id, vec):
        fp = self._hash(vec)
        if fp not in self.buckets:
            self.buckets[fp] = []
        self.buckets[fp].append(vec_id)

    def query(self, vec):
        fp = self._hash(vec)
        # 4. 查表：只返回同一个桶里的 ID
        return self.buckets.get(fp, [])

# --- 测试 ---
dim = 100
lsh = SimpleLSH(dim)

# 两个非常相似的向量
v1 = np.random.randn(dim)
v2 = v1 + np.random.randn(dim) * 0.01 # v2 是 v1 加了一点点微小的扰动

# 一个完全不同的向量
v3 = np.random.randn(dim)

lsh.add("vec1", v1)
lsh.add("vec3", v3)

print(f"查 v2 (v1的近邻): {lsh.query(v2)}")
# 结果大概率是 ['vec1']，因为 v1 和 v2 极大概率落在同一个桶
```

# 多智能体强化学习 (MARL) 基础

https://www.youtube.com/watch?v=KN-XMQFTD0o&list=PLvOO0btloRntPRHgQo75wuMNFvtXHGn5A

本节课介绍 Multi-Agent Reinforcement Learning (MARL) 的基本概念。MARL 比单智能体（Single-Agent）更复杂，要求学习者已掌握单智能体策略学习的基础。

## 1. 多智能体系统的四种常见设定

根据 Agent 之间的利益关系，MARL 环境通常分为四类：

### 1.1 完全合作 (Fully Cooperative)

- **关系**：所有 Agent 利益一致，获得的奖励相同，拥有共同目标。
- **例子**：
  - 工业机器人协同装配汽车。
  - 两个机械臂配合搬运箱子。

### 1.2 完全竞争 (Fully Competitive)

- **关系**：一方的收益即另一方的损失，典型如**零和博弈**。
- **例子**：
  - 机器人格斗（赢家得分，输家扣分，总和为 0）。
  - 捕猎（虽非严格零和，但猎豹获益意味着小鹿损失）。

### 1.3 合作与竞争混合 (Mixed Cooperative & Competitive)

- **关系**：团队之间是竞争关系，但团队内部成员是合作关系。
- **例子**：
  - 机器人足球：两队竞争，但队友利益一致。
  - 星际争霸：玩家之间互相对抗，但同一玩家控制的多个单位（士兵）之间需要协作。

### 1.4 利己主义 (Self-interested)

- **关系**：每个 Agent 只想最大化自身利益，不在乎他人受损或获益（非刻意帮助或伤害）。
- **特点**：一个 Agent 的动作会改变环境状态，从而客观上影响其他 Agent。
- **例子**：
  - **股票自动交易系统**：每个系统只想赚钱，但大规模交易会影响股价，进而影响其他系统的盈亏（如 Knight Capital 造成股市波动）。
  - **无人驾驶**：
    - 如果路上只有少量无人车，可视为 Single-Agent。
    - 当大量无人车存在时，前车刹车或变道会影响后车，变为 Multi-Agent 系统。
    - 利己通常对整体有益（为了不撞车保护自己，同时也保护了他人），但也存在抢道等竞争行为。

## 2. 收敛标准：纳什均衡 (Nash Equilibrium)

在多智能体环境中，由于存在博弈，收敛的定义与单智能体不同。

### 2.1 单智能体收敛

- **目标**：最大化期望回报 $J(\theta)$。
- **收敛**：当无法通过改进策略让 $J(\theta)$ 继续增长时，即为收敛。

### 2.2 多智能体收敛

- **目标**：找到 **纳什均衡 (Nash Equilibrium)**。
- **定义**：在纳什均衡状态下，**当其他所有 Agents 都不改变策略时，任何一个 Agent 单独改变策略都不会获得更高的回报**。
- **意义**：
  - 在制定策略时需考虑通过对手的策略做出最优“应对”。
  - 如果是理性的，大家都处于纳什均衡时，谁也没有动机去改变策略，系统达到平衡。

## 3. 挑战

直接将单智能体算法套用到多智能体环境中通常效果不好，甚至不收敛，因为环境不再是静态的（其他 Agent 也在学习和变动）。

# 连续控制基础 (Continuous Control)

https://www.youtube.com/watch?v=rRIjgdxSvg8&list=PLvOO0btloRnsNfDgwv0OCLVTsm5bUyE6L

本节课介绍连续控制问题的基本概念，以及它与离散控制的区别和难点。

## 1. 离散 vs 连续动作空间

### 1.1 离散动作空间 (Discrete Action Space)

- **定义**：动作空间 $A$ 包含有限个动作。
- **例子**：
  - 《超级玛丽》：动作只有 {向左, 向右, 向上}。
  - Grid World：{上, 下, 左, 右}。

### 1.2 连续动作空间 (Continuous Action Space)

- **定义**：动作空间 $A$ 是一个连续的集合（通常是向量空间），包含无穷多个可能的动作。
- **例子**：
  - **机械手臂**：
    - 假设有两个关节（自由度 $D=2$）。
    - 上关节角度 $\in [0, 360]$，下关节角度 $\in [0, 180]$。
    - 动作是一个二维向量，取值是连续的实数。

## 2. 传统离散方法为何失效？

之前学习的算法（如 DQN 和基于 Softmax 的策略网络）无法直接应用于连续控制：

- **DQN (Value-based)**：
  - **原理**：输入状态 $s$，输出一个向量，维度等于动作数量，每个元素代表该动作的 Q 值。
  - **问题**：连续空间有无穷多个动作，DQN 无法输出无穷维的向量。
- **Policy Network (Softmax)**：
  - **原理**：输出层使用 Softmax，得到动作空间上的概率分布（向量维度 = 动作数量）。
  - **问题**：同样因为动作数量无穷大，无法构建输出向量。

## 3. 朴素解法：离散化 (Discretization)

一种简单的方法是将连续空间“网格化”。

- **做法**：在连续的动作空间上画网格，只取网格上的点作为允许的动作。
- **优点**：将连续问题转化为了离散问题，可以使用 DQN 或 Softmax 策略网络。
- **缺点：维度灾难 (Curse of Dimensionality)**
  - 动作空间的自由度（维度）记为 $D$。
  - 若每个维度切分成 $N$ 个点，总动作数量为 $N^D$。
  - **指数爆炸**：随着自由度 $D$ 的增加（例如机械臂关节变多），网格点数量呈指数级增长。
  - **后果**：动作数量太多，导致网络难以训练。
  - **结论**：离散化只适用于自由度 $D$ 很小（如 1 或 2）的情况。

## 4. 解决方案预告

由于离散化在处理高维连续控制时行不通，我们需要专门针对连续空间的算法：

1.  **确定性策略梯度 (Deterministic Policy Gradient, DPG)**：输出确定性的动作向量。
2.  **随机策略 (Stochastic Policy)**：输出概率分布的参数（如高斯分布的均值和方差），适用于连续空间。

# 连续控制的确定性策略梯度


这节课继续研究连续控制。这节课介绍 Deterministic Policy Gradient (DPG) 方法，它是一种 Actor-Critic 方法。策略网络 (actor) 输出确定性的动作，价值网络 (critic) 给动作打分 (记做 q)。用 DPG 更新策略网络，让分数 q 更高。用 TD 算法更新价值网络，让它的估计更接近真实价值。

本节课介绍专门解决 **连续控制** 问题的算法：**Deterministic Policy Gradient (DPG)**。2016 年，DeepMind 将其与神经网络结合，提出了 **Deep DPG (DDPG)**。

## 1. 核心概念与架构

DPG 是一种 **Actor-Critic** 方法，包含两个神经网络：

### 1.1 策略网络 (Actor)

- **符号**：$\pi(s; \theta)$，参数为 $\theta$。
- **功能**：控制 Agent 运动。
- **输入**：状态 $s$。
- **输出**：确定的动作 $a$（实数或向量）。
- **特点**：**确定性 (Deterministic)**。给定状态 $s$，输出唯一的动作 $a$，没有随机性。这不仅适用于连续空间，且动作即为具体的控制量（如机械臂关节角度）。

### 1.2 价值网络 (Critic)

- **符号**：$Q(s, a; w)$，参数为 $w$。
- **功能**：不控制 Agent，只负责评价动作 $a$ 在状态 $s$ 下的好坏。
- **输入**：状态 $s$ 和 动作 $a$。
- **输出**：一个实数（Q 值）。分数越高，表示动作越好。
- **作用**：指导 Actor 进行改进。

---

## 2. 网络的训练过程

我们需要同时训练 Actor 和 Critic，让决策越来越好，评分越来越准。

### 2.1 更新价值网络 (Critic Update)

使用 **TD 算法** 更新参数 $w$。

1.  **数据**：观测到一个 Transition $(s_t, a_t, r_t, s_{t+1})$。
2.  **预测**：
    - 当前时刻预测：$Q(s_t, a_t; w)$。
    - 下一时刻预测：先用 Actor 计算下一时刻动作 $a'_{t+1} = \pi(s_{t+1}; \theta)$，再计算 $Q(s_{t+1}, a'_{t+1}; w)$。
3.  **TD Target**：
    $$y_t = r_t + \gamma \cdot Q(s_{t+1}, \pi(s_{t+1}; \theta); w)$$
4.  **TD Error**：$\delta_t = Q(s_t, a_t; w) - y_t$。
5.  **目标**：最小化 $\delta_t^2$（让预测接近 Target）。
6.  **梯度下降**：更新 $w$。

### 2.2 更新策略网络 (Actor Update)

使用 **确定性策略梯度 (DPG)** 更新参数 $\theta$。

1.  **目标**：改进 $\theta$，使得生成的动作 $a$ 能让 Critic 打出更高的分数 $Q$。即最大化 $Q(s, \pi(s; \theta); w)$。
2.  **梯度计算 (链式法则)**：
    $$g = \nabla_{\theta} Q(s, \pi(s; \theta); w) = \nabla_a Q(s, a; w)|_{a=\pi(s)} \cdot \nabla_{\theta} \pi(s; \theta)$$
    - **解释**：梯度从 $Q$ 传导到 $a$，再从 $a$ 传导到策略网络参数 $\theta$。
3.  **梯度上升**：更新 $\theta$（加上学习率 $\beta \cdot g$）。

---

## 3. 训练技巧：Target Networks

为了解决 Bootstrapping 自举造成的**偏差 (Bias)** 问题（如高估或低估的传播），DPG 引入了 **Target Networks**。

### 3.1 结构

- **Target Actor**: $\pi'(s; \theta^-)$，结构同 Actor，参数 $\theta^-$。
- **Target Critic**: $Q'(s, a; w^-)$，结构同 Critic，参数 $w^-$。

### 3.2 改进后的 TD Target 计算

使用 Target Networks 来计算 TD Target，而不是使用当前正在更新的网络：
$$y_t = r_t + \gamma \cdot Q'(s_{t+1}, \pi'(s_{t+1}; \theta^-); w^-)$$

- 当前网络 ($\theta, w$) 用于计算梯度更新。
- 目标网络 ($\theta^-, w^-$) 仅用于计算目标值 $y_t$。

### 3.3 软更新 (Soft Update)

Target Networks 的参数不是直接复制，而是缓慢跟随主网络更新：
$$w^- \leftarrow \tau w + (1-\tau)w^-$$
$$\theta^- \leftarrow \tau \theta + (1-\tau)\theta^-$$

- $\tau$ 是一个很小的系数（如 0.005），使得 Target Network 变化缓慢，增加训练稳定性。

> **其他技巧**：DDPG 通常还会使用 **经验回放 (Experience Replay)** 来打破数据相关性。

---

## 4. 总结：随机策略 vs 确定性策略

| 特性         | 随机策略 (Stochastic Policy)           | 确定性策略 (Deterministic Policy) |
| :----------- | :------------------------------------- | :-------------------------------- | ----------------------------- |
| **输出**     | 动作的**概率分布** $\pi(a              | s)$                               | 具体的**动作向量** $a=\pi(s)$ |
| **动作选择** | 根据概率分布**随机抽样**               | 直接执行输出的动作，**无随机性**  |
| **应用场景** | 离散控制 (Atari) 或 连续控制           | 主要用于 **连续控制** (机械臂)    |
| **梯度求解** | 策略梯度定理 (Policy Gradient Theorem) | 链式法则 (Chain Rule)             |

**下节课预告**：学习一种也可以做连续控制的随机策略网络。

明白了，我会为你整理最新版 CMU 15-445（数据库系统）课程的完整学习笔记，包括每一章节的详细讲解、代码实现原理、项目解析、以及相关的视频讲义和推荐阅读材料。
完成后我会及时通知你，方便你系统性地学习和掌握这门课程。

# CMU 15-445/645 数据库系统 (2025 春) 学习笔记

## 课程概述

CMU 15-445/645 是关于数据库管理系统 (DBMS) 内部实现的高级课程。课程涵盖关系模模型与架构、索引和过滤器、事务处理 (ACID 原则)、并发控制、数据库恢复，以及并行和分布式数据库系统等主题。课程侧重于通过**编程项目**实践数据库内核各模块，包括存储管理、索引结构、查询执行、并发控制等。本笔记将按照课程进度对每一讲的核心概念、架构设计、算法和数据结构进行详细讲解，并结合课程提供的代码实现要点和项目解析，帮助理解数据库系统实现原理。

**教材与参考资料：**课程推荐教材为 _Database System Concepts_ 第7版（Silberschatz 等）。每次课程都有指定阅读章节以巩固理论。例如，关系模型和 SQL 部分对应教材第1-5章，存储和索引部分对应第11-14章，事务与恢复对应第18-19章，分布式数据库对应第20-23章等。这些章节涵盖了关系数据库基础、存储管理、索引技术、查询处理与优化、事务管理、故障恢复以及分布式数据库等内容。本笔记在相关章节会列出推荐阅读，并引用课程讲义、教材或论文中的关键内容以供进一步参考。

**编程项目：**课程包括五个主要项目，BusTub\*\* 的教学用关系型数据库管理系统。项目列表如下：

- _项目0: C++ Primer_（跳表实现） – 熟悉 C++17 编程和基础数据结构的实现。
- _项目1: Buffer Pool Manager_（缓冲池管理） – 实现页缓存管理、页面置换策略和磁盘调度。
- _项目2: Database Index (B+ 树索引)_ – 删除、查询及迭代器，并支持并发操作。
- _项目3: Query Execution_（查询执行器） – 实现顺序扫描、索引扫描、连接、聚合、排序、LIMIT 等算子的执行引擎及基本的优化规则。
- _项目4: Concurrency Control_（并发控制/MVCC） – 为 BusTub 增加**乐观多版本并发控制** (MVCC) 的事务支持，实现快照隔离级别下的事务管理、版本链存储、提交/中止处理，以及主键唯一性约束检查等。

每个项目均附有详细规范和提示。本笔记将在相关理论讲解后，对对应项目的实现原理、关键模块、可能遇到的难点和调试与实践的联系。

---

以下内容将按照课程顺序展开，每讲对应一个主要主题。对于每一讲，我们整理了核心概念和算法，列出了架构设计要点和涉及的数据结构，实现细节（结合 BusTub 代码），以及讲师在视频中强调的重点内容。必要时也会给出推荐阅读（课件章节或论文）以拓展理解。

## Lecture 1: 关系模型与关系代数

**核心内容：** 介绍数据库与数据库管理系统的基本概念，关系数据数表达查询的基础。

- **数据库 vs. 文件系统：**数据库是对现实世界的一种数据建模和组织，包含相互关联的数据集合。而数据库管理系统 (DBMS) 是管理数据库的软件，它提供高层次接口来定义、查询、更新和管理数据，以及保证数据的可靠性和有效性。相较于将数据存储在扁平文件中，使用DBMS可以解决数据冗余、不一致、难以并发访问、持久性差等问题。例如，当使用CSV等平面文件存储数据时，应用程序必须自行解析文件、确保引用完整性、处理并发写入冲突和崩溃恢复等，这在实现上非常复杂。DBMS 提供了统一的解决方案，包括模式管理、查询优化执行、并发控制和日志恢复等机制，从而简化应用开发并保证数据完整性。

- **关系数据模型：**关系模型使用**关系（表）**来存储数据。一个关系模式由属性（列）集合定义，每行（元组）对应一个实体实例的数据。关系模型的核心是具有关系代数理论基础的**数据独立性**：应用程序只需关注逻辑模式（属性和关系），不需了解物理存储细节。常见的数据模型还包括 NoSQL 的键值/文档/图模型，以及机器学习中的数组/矩阵模型等，但关系模型是最常见和成熟的。本课程主要围绕关系型数据库展开。

- **模式与模式设计：**模式（schema）定义了数据库中数据的结构，即关系及其属性。良好的模式设计需要遵循范式以减少冗余和更新异常。这涉及**函数依赖**和**范式规范化**理论，例如保证属性满足第一范式（无重复组）、第二范式（消除非主属性对码的部分依赖）、第三范式和BCNF（消除传递依赖）等。课程要求学生来设计合适的数据库表结构。尽管课上未单独详述范式理论，但教材第3-5章提供了关系数据库模式设计的详细介绍，包括范式分解算法和范式的权衡。

- **关系代数：**关系代数是用于查询关系数据库的**标准化操作集合**。它提供了一组对关系的运算，如选择 (σ)、投影 (π)、连接 (⨝)、并/交/差 (∪, ∩, −) 等，这些运算接收一个或两个关系作为输入，输出新的关系。关系代数是声明性的，强调描述“**做什么**”而非“怎么做”。例如，自然连接运算 `R ⨝ S` 只描述连接关系R和S满足条件的元组集合，至于使用嵌套循环还是哈希连接实现并未指定5†L33-L41】。关系代数为关系模型提供了数学基础，SQL 查询可以看作关系代数表达式的高级表示。

- **关系代数与 SQL:**
  - 关系代数基于**集合**语义（结果中无重复元组），而实际的 SQL 则基于**袋(multiset)**语义，允许重复结果。因此 SQL 的一些行为（如`COUNT`默认统计重复）与纯关系代数有所区别。需要使用 `DISTINCT` 关键字去重。
  - 基本关系代数运算对应到 SQL 如下：选择(σ)对应 `SELECT * FROM R WHERE ...` 子句，投影(π)对应 `SELECT` 列表，连接(⨝)对应 `FROM R JOIN S ON 条件`，集合并/差可用 `UNION`/`EXCEPT` 表达。此外，SQL 还支持关系代数没有的排序（`ORDER BY`）、分组聚合（`GROUP BY`）等扩展操作。

**讲师重点：**本讲通过一个音乐数据库的示例（艺术家和专辑两张表）说明了使用纯文件存储数据存在的数据完整性和访问并发问题。随后引出关系模型如何通过外键约束、模式演化等机制来确保数据的一致性，以及DBMS如何抽象底层文件IO细节，为用户提供高层次的数据操作接口和优化过的执行。本讲还概述了关系代数的基本运算，并强调**SQL 查询优化**通常会将 SQL 转换为等价的关系代数表达式，再经过优化器重组运算次序来生成高效的执行计划。这为后续课程讨论查询优化奠定了基础。

**阅读材料：**教材第1-2章介绍数据库系统概念和关系模型基础；第6-7章详细讨论关系代数和关系演算，对理解 SQL 的理论基础很有帮助。

## Lecture 2: Modern SQL 概述

**核心内容：**回顾 SQL 查询语言的主要特性及其演进历史，涵盖SQL标准的新功能、基本子语言分类（DML/DDL/DCL）、典型示例数据库和查询语法，以及一些高级SQL特性（如窗口函数、事务控制等）。讲解如何使用 SQL 进行查询和数据操作，并讨论 SQL 标准的演变和厂商差异。

- **SQL 简史：**SQL (Structured Query Language) 是一种**声明式**查询语言。1970年代 IBM 的 System R 项目首先实现了 SEQUEL（SQL 前身）来操作关系数据库，1980年代正式更名为 SQL。尽管 SQL 已有数十年历史，但仍在不断更新标准，新版本几乎每隔数年发布一次。例如，SQL:1999 引入了正则表达式匹配和触发器，SQL:2003 支持 XML 数据和窗口函数 (window functions)，SQL:2011 支持时间/周期数据处理 (temporal databases) 和流水线化修改 (pipelined DML)，SQL:2016 增加了 JSON 支持，SQL:2023 则新加入了图查询和多维数组等特性。课程强调**SQL-92**是各DBMS普遍支持的最小标准，各厂商在此基础上实现自有扩展。

- **SQL 子语言：**SQL包含多个子语言部分：

  1. **数据操纵语言 (DML)：**用于查询和修改数据，如 `SELECT`（查询）、`INSERT`、`UPDATE`、`DELETE` 等。
  2. **数据定义语言 (DDL)：**定义或修改数据库模式和对象，如 `CREATE TABLE`、`ALTER TABLE`、`CREATE INDEX`、`CREATE VIEW` 等。DDL 操作通常会隐式提交事务。
  3. **数据控制语言 (DCL)：**控制访问权限和安全，如 `GRANT`、`REVOKE` 用于权限管理。
  4. 其他：视图定义 (`CREATE VIEW`)、完整性约束（`PRIMARY KEY`, `FOREIGN KEY`, `CHECK` 等)、事务控制 (`BEGIN/COMMIT/ROLLBACK`) 也属于 SQL 标准的一部分。例如，SQL 支持声明级和语句级的事务控制命令，用于保证ACID特性。

- **示例数据库与基本查询：**课程使用了一个简单的大学数据库作为案例，包括学生表 `student(sid, name, login, age, gpa)`、课程表 `course(cid, name)` 和选课表 `enrolled(sid, cid, grade)`。典型SQL查询示例：

  - **选择与投影：**查询年龄大于20岁的学生姓名和GPA：
    ```sql
    SELECT name, gpa FROM student
    WHERE age > 20;
    ```
    这里 `WHERE` 子句实现选择，`SELECT` 列表实现投影。
  - **连接查询：**查询选修“CS101”课程的所有学生姓名和成绩：
    ````sql
    SELECT S.name, E.grade
    FROM student S
    JOIN enrolled E ON S.sid = E.sid
    JOIN course C ON E.cid = C.cid
    WHERE C.name = 'CS101';
    ```   ([Schedule | CMU 15-445/645 :: Intro to Database Systems (Spring 2025) ](https://15445.courses.cs.cmu.edu/spring2025/schedule.html#:~:text=Wim%20Martens%20,9))将三张表连接，并使用课程名称筛选，再选择所需字段输出。
    ````
  - **聚合与分组：**计算每门课的选课人数：
    ```sql
    SELECT C.name, COUNT(*) AS num_enrolled
    FROM course C
    JOIN enro ([Schedule | CMU 15-445/645 :: Intro to Database Systems (Spring 2025) ](https://15445.courses.cs.cmu.edu/spring2025/schedule.html#:~:text=Wim%20Martens%20,9))id = E.cid
    GROUP BY C.name;
    ```
    使用 `COUNT(*)` 聚合函数和 `GROUP BY` 子句对每门课程分组统计人数。需注意SQL中的聚合默认针对多重集计算，例如COUNT会计入重复行。

  讲义中强调了一些SQL等价写法和陷阱。例如，`COUNT(*)`、`COUNT(列)` 和 `COUNT(1)` 在无 NULL 情况下等价；又如在没有`GROUP BY`时使用聚合会视整张表为一组。还提到SQL的`DISTINCT`可用于聚合内部去重（如`COUNT(DISTINCT login)`计算唯一登录名数量）。

- **高级 SQL 特性：**现代SQL标准还包括**窗口函数**（如排名函数`ROW_NUMBER()`、移动平均等，使用 `OVER()` 子句），**公共表表达式 (CTE)**（使用`WITH`子句编写子查询），**存储过程和触发器**（SQL/PSM），**递归查询**（WITH RECURSIVE）等。课程在本讲提及了一些新标准特性，例如 SQL:2011 的时间旅行查询和 SQL:2016 对 JSON 数据的支持。这些特性能增强SQL表达能力，如窗口函数允许在不改变结果行情况下计算累计和、排名等聚合信息。不过在本课程中，这些高级特性主要作为背景介绍，编程项目中更关注核心DML/DDL功能的实现。

**讲师重点：**本讲首先强调了 SQL 相对于过程型查询语言的优势——声明性和易用性，然后梳理了 SQL 标准演化中的重要里程碑。通过示例讲解 DML 和DDL的基本使用，并指出不同数据库系统对标准的实现差异（比如不同SQL方言的语法变体）。讲师还特别提醒**SQL基于bag语义**而非set语义，这意味着查询结果可能包含重复，需要使用 DISTINCT 去重。这在实现DBMS的查询执行和优化时会产生影响（例如聚合和连接操作需要考虑去重成本）。最后，课程引出下几讲主题：关系数据库内部如何高效执行这些SQL语句，以及如何存储和索引数据以支持快速查询。

**阅读材料：**教材第3-5章涵盖了SQL语言细节、完整性约束、视图和模式设计等。其中第5章深入讨论了SQL的演进和标准特性，非常贴合本讲内容。此外，建议参考官方SQL标准文档或各主要数据库的SQL参考手册，以了解SQL在实践中的不同实现和高级用法。

## 项目0：C++ Primer（跳表实现）

*项目目标：*在正式开发数据库系统之前，熟悉 C++ 编程和基础数据结构的实现。本项目要求实现一个**跳表 (Skip List)** 来充当有序集合的数据结构，并支持并发访问，从而帮助学生掌握现代 C++（C++17）语法和并发编程基本技巧。

- **跳表简介：**跳表是一种随机化的平衡数据结构，可用于实现有序集合（支持高效的插入、删除和查找）。其核心思想是在有序链表之上建立多级“跳跃”索引，通过多层链表结构减少查找路径长度。最低层(level 0)是包含所有元素的有序链表，上层每一级都以一定概率挑选下层节点进行链接，从而形成跳跃通路。典型跳表在平均情况下实现了 O(log n) 的插入、删除、查找效率，与平衡二叉搜索树性能相当，但实现更简单。项目要求学生参考 William Pugh 的经典论文实现跳表。

- **功能要求：**需要实现跳表的基本接口，包括插入键、删除键、查找键是否存在等，确保跳表中的元素保持有序且不重复。项目提供了跳表节点 (`SkipNode`) 和跳表类 (`SkipList`) 的框架代码，学生需要填充其中的逻辑。例如，插入操作需要随机生成新节点高度、更新不同层级的前向指针数组；删除操作则需要更新相关指针以移除目标节点。项目还提示使用 STL 容器和智能指针等现代C++特性来简化实现，同时巩固对模板、迭代器等的理解。

- **并发扩展：**在完成单线程版本后，_Task #2_ 要求引入简单的同步机制使跳表线程安全。具体而言，实现基于读写锁 (`std::shared_mutex`) 的并发控制：允许多个读者同时访问跳表，但写者互斥，且写者与读者互斥。因此，需要在跳表的查找、插入、删除等操作中适当加锁。项目建议采用 RAII 风 ([Schedule | CMU 15-445/645 :: Intro to Database Systems (Spring 2025) ](https://15445.courses.cs.cmu.edu/spring2025/schedule.html#:~:text=Wim%20Martens%20,9))::unique_lock`和`std::shared_lock`）来管理锁的获取和释放，避免死锁和忘记解锁的问题。在实现上，可以对整个跳表使用一个全局读写锁（粗粒度锁）。虽然这可能降低并发性能，但能保证正确性。课程稍后会讨论更细粒度的并发控制技术（如节点级锁）以提升性能，不过本项目不要求实现。

- **调试建议：**由于跳表涉及随机层数和指针操作，调试时需注意内存管理和边界条件。例如，确保随机高度生成函数的正确性，使层高分布符合期望的几何分布；插入和删除操作要正确处理头结点和各级链表的更新。可以编写小规模测试用例验证，如插入一些已排序和未排序的元素，检查跳表有序性；并发部分可以启动多个线程同时执行插入/查找操作，借助调试日志或断言来捕捉并发问题。项目提供了一套基于 Google Test 的单元测试，运行这些测试用例可以帮助发现实现中的问题。

*项目收获：*通过 Project 0，学生熟悉了 C++17 开发环境和调试方法，掌握了Skip List这种典型数据结构的实现。这为后续项目（特别是B+树索引的实现）打下基础。同时，并发访问跳表的练习让学生初步体会到数据结构在多线程环境下需要考虑的同步问题，为数据库并发控制内容埋下伏笔。

## Lecture 3: 数据库存储 I – 存储器层次与页面组织

**核心内容：**介绍数据库存储层次结构，从底层磁盘存储到内存缓冲的管理，以及数据页的组织格式。讨论 DBMS 如何在磁盘上组织表数据（如Heap File堆文件），以及如何通过缓冲池读取和缓存页面。

- **存储设备与层次：**数据库系统的数据通常存放在**持久性存储**（如磁盘或SSD）上，以保证掉电后数据不丢失。但CPU访问磁盘极其缓慢，为提高性能，DBMS利用内存作为缓存。现代存储层次分为：寄存器/CPU缓存、主存、磁盘/SSD、远程存储等，不同层次速度和容量差异很大。DBMS 设计需考虑**局部性原理**，尽量将热点数据缓存在内存中，减少磁盘IO。本讲强调磁盘随机IO代价高昂，因此顺序读写（顺序扫描）比随机访问快很多个数量级；同时介绍磁盘的物理特性（例如磁盘寻道时间、旋转延迟）以及SSD的性能特点（无寻道延迟但存在写放大）等。这解释了为何数据库要采用批量IO和预取等优化技术。

- **页面（Page）概念：**操作系统和DBMS都采用分页作为IO基本单位。典型设置下，一页大小为4KB（BusTub 里也是 4KB）。数据库把表或索引数据划分为固定大小的页面存储在磁盘上，每次IO读写一个或多个整页。使用页面有两大好处：(1) 利用局部性将相关记录放在同一页，访问一条记录时顺带将邻近记录读入缓存；(2) 简化缓冲管理，在内存中分配固定大小的页帧(frame)来存放磁盘页。BusTub 中定义每页4KB，缓冲池中的帧也是4KB。页面通常包含若干记录以及页头元数据，如记录数、空闲空间指针等。

- **堆文件 (Heap File)：**这是数据库存储表的一种常见方式，将数据记录以无特定顺序存放在页面集合中，新的记录 append 到任何有空间的页或追加新页。每个表通常对应一个堆文件，由多个页面组成。为了管理这些页，DBMS 通常维护一个页面目录或空闲列表，用于快速找到有空闲空间可插入新记录的页面。本讲介绍了**Slotted Page**（插槽页）结构：在每个页内部，通过页头的槽目录(slot directory)管理可变长记录。槽目录存放记录的偏移和长度，每条记录可以在页中移动（如做碎片整理），而槽索引则固定充当记录的标识符 (RID: 包含页ID和槽ID)。这种设计使删除记录时无需移动其他记录，只需标记槽空闲；插入新记录时在空槽放置即可。Slots机制提高了页内空间利用率和记录操作的灵活性。

- **缓冲池 (Buffer Pool)：**缓冲池是DBMS在内存中保留的一块区域，用于缓存磁盘页。当上层需要访问某个数据页时，若页已在缓冲池则直接返回，否则通过**Buffer Pool Manager**从磁盘读取该页到缓冲池框架(frame)中，然后返回。缓冲池维持一个**页面缓存表 (page table)**，跟踪每个缓存页当前对应的磁盘页编号，以及**pin count**（引用计数）和**脏标记**等元数据。Pin count表示有多少用户在使用该页，非零时页不可被置换；脏页表示缓冲中内容已修改但尚未写回磁盘。为了在缓冲池满时腾出空间载入新页，Buffer Pool Manager 实现页面替换策略（例如 LRU 算法）。本课程的实现中使用了更先进的 LRU-K 算法，以更好地判断热页。我们将在 Project 1 中详细分析该部分。

- **数据访问透明性：**缓冲池的存在使得 DBMS 其它模块（执行器等）无需关心数据在内存还是磁盘。例如，当执行器请求页ID=5时，Buffer Pool Manager 会检查该页是否在内存；如果不在，则调度从Disk Manager读盘，将页5读入内存帧，然后返回指向该帧的指针。对执行器而言，读取页5与读取内存数组没有区别。这种架构将 IO 细节封装起来，提高模块解耦。同样，Buffer Pool Manager 不需要了解页面内容（表数据或索引），它只按页ID管理缓冲即可。

**讲师重点：**本讲通过类比操作系统的虚存机制，解释了数据库缓冲池的重要性：没有缓冲池，频繁的磁盘IO将导致严重性能瓶颈；通过缓存，大部分热点数据可以在内存命中。讲师强调了**页大小的权衡**：较大页可减少寻道次数提高顺序IO效率，但如果记录很小则可能浪费内存和带宽；较小页则元数据开销更大且随机IO次数多。典型4KB是折中选择，但某些数据库或场景下也会用8KB甚至更大页。课程还介绍了简单的页面置换策略如 LRU，并指出真实系统可能使用改进的算法 (如 LRU-K 或 CLOCK)以更好应对访问模式变化。最后，Buffer Pool Manager 作为整个系统的核心组件之一，为后续项目埋下伏笔。

**阅读材料：**教材第12.1-12.4 节介绍了文件存储和页面结构，第13.2-13.3 节讲述缓冲池管理和页面置换策略。这些内容对应本讲主题，建议深入阅读以掌握细节。

## Lecture 4: 内存管理与存储分配

**核心内容：**讨论数据库的内存管理，包括记录在页内的存储布局、变长数据的存储、内存分配与回收策略，以及操作系统与数据库在内存管理上的协作。还介绍了写入放大和对齐等影响性能的因素。

- **记录存储格式：**在关系表中，记录可能是定长或变长。对于定长字段表，每条记录长度固定，页中的记录位置可通过序号直接计算偏移。然而许多表包含变长类型（如 `VARCHAR` 文本），因此常采用**槽页(Slotted Page)**结构存储变长记录。变长记录典型实现是将实际数据存在页的末端向前生长，槽目录从页头向后生长，中间的空闲空间用于动态分配。每个槽包含该记录的偏移和长度，通过槽ID可定位记录。如果记录变长字段更新后长度增加，可能需要将该记录迁移到本页空闲空间的新位置，并更新槽偏移；若本页剩余空间不足，可能需要将记录移动到别的页（触发所谓 **page split**，特别是在索引中常见）。课程强调正确维护槽目录对于插入、删除和更新变长记录以避免碎片很重要。

- **存储分配与对齐：**DBMS在页内部以及缓冲池层面都涉及内存分配。为了高效管理内存，数据库通常会：

  - 采用**内存池**或自定义分配器，减少频繁的小块内存分配带来的碎片和系统调用开销。
  - 注意**字节对齐**：现代CPU访问未对齐数据可能导致性能下降甚至异常，因此DBMS常将记录中的字段按对齐要求排列（比如4字节对齐int）。页结构也往往对齐到字边界。BusTub中采用C++结构体表示Tuple元数据、Page头等，因此需要确保`#pragma pack`或对齐规则与运行环境一致。
  - 处理**写入放大**：如果每次改动数据都需要大量附加写IO（例如修改1字节却写回整个4KB页），就产生写放大问题。DBMS通过批量刷脏页、日志缓冲等方式缓解。此外，在内存分配上，尽量重用已经分配的页帧来减少频繁分配释放。

- **操作系统与DBMS内存**: 操作系统也有自己的页缓存（文件系统缓存）。因此，当DBMS读写磁盘文件时，可能经过OS缓存才到磁盘，形成**双重缓存**。这会占用额外内存且在某些情况下降低性能（如OS缓存可能将数据淘汰而DBMS还在缓冲池保留它）。一些DBMS可使用直接IO（O_DIRECT）绕过OS缓存，也有的利用mmap将文件映射内存。CMU的BusTub直接使用读写系统调用并由自身维护缓存，从而完全控制内存中的数据页面。

- **压缩与列存储简介：**本讲可能引入了数据压缩技术（课程Schedule阅读提到了第13.6节）。压缩可以大幅减少IO和存储。例如字典编码、运行长度编码、位图压缩等。压缩往往结合列存储（将每列值集中存储）效果更佳，因为同列相邻数据相似度高更易压缩。课程后续Lecture 6详细讨论列式存储与压缩，这里只做预告。此外，还可能谈到内存中的数据格式如行存(row store) vs 列存(column store)比较：行存适合事务型工作负载（需要整行操作），列存适合分析型查询（扫描大量同一列数据）。

**讲师重点：**讲师解释了为何 DBMS 通常不直接依赖 OS 虚拟内存机制进行页面交换。因为数据库可以基于访问模式做更智能的缓存管理，而操作系统不懂数据库访问的逻辑模式。此外，自己管理缓冲也可以避免双写问题（OS刷回缓存与DBMS重复刷回）。在内存管理上，强调了**空间利用率**：比如插入删除频繁时如何避免页内碎片过多，需要定期重组或采用空闲空间列表等机制。对于压缩，讲师举例说明压缩带来的性能收益，但也提醒压缩需要权衡CPU开销，一般在I/O密集型场景收益更明显。最后，课程将目光转向更高层次的存储结构，如下一讲的日志结构存储、哈希索引等，继续完善数据库存储层次知识。

**阅读材料：**教材第13章关于存储管理和第14.8节关于压缩的内容与本讲相关。另外推荐阅读H-Store论文或Peloton系统的存储管理部分，了解现代内存型数据库如何管理内存（CMU自研的内存数据库也有文章讲解内存分配策略）。

## Lecture 5: 数据库存储 II – 日志结构存储与磁盘优化

**核心内容：**探讨高级存储架构和磁盘数据组织策略，例如**日志结构存储**（Log-Structured Storage）、顺序IO优化，以及记录在磁盘上的布局调整。还涉及数据库如何利用操作系统提供的异步IO、多块读写等特性提升性能。

- **日志结构存储 (LFS)：**传统关系存储（堆文件）在随机写多时性能会下降。日志结构存储通过将更新以追加(log append)方式顺序写入一个日志，避免了随机写磁盘的开销。典型例子是 LSM 树（Log-Structured Merge Tree）在NoSQL中的应用。关系型数据库也有类似思想：将更新先写入日志，再批量合并到数据文件。这种架构牺牲一些顺序读效率换取写性能的提升。在顺序写特性明显的存储（如SSD由于不存在寻道，更倾向并行IO）上也很有效。本讲可能介绍了 LSM 树的基本原理：数据分层存储（内存 C0 层，磁盘C1/C2层等），后台持续将更新批归并合并到更高层，从而保持查询效率和写入性能。虽然CMU BusTub没有要求实现完整LSM，但理解该思想对认识现代数据库存储引擎很重要。

- **顺序IO优化：**DBMS 尽量将磁盘访问模式转换为顺序IO。例如批量预读(prefetch)：执行顺序扫描时，Buffer Pool Manager 可以提前请求后续页，从而在当前页处理时并行读取下一页，以隐藏IO延迟。另一个优化是批量写：将多个脏页一次写盘（甚至允许磁盘调度优化写顺序）。现代磁盘接口如 SATA/AHCI 和 NVMe 支持**命令队列**，DBMS 可以同时发起多个IO请求，让磁盘自行优化执行顺序。课程提到的 Disk Scheduler 组件即用来调度读写请求并利用后台线程异步处理磁盘IO，以实现更高的吞吐（BusTub Project 1 就要实现一个简化的磁盘调度器）。

- **磁盘数据布局：**为提升顺序访问性能，数据文件在磁盘上通常采用**连续分配**（尽可能将相关页相邻存储）。DBMS可能请求文件系统预留连续块或直接管理裸分区以确保连续性。如果无法保证完全连续，DBMS也会在执行层通过顺序访问逻辑顺序页面来获取接近顺序读的效果。讲师或许提及磁盘扫库(disk sweeping)技术：循环利用磁头移动方向优化批处理IO，比如算法如 CSCAN 针对磁盘调度。此外，一些DBMS支持**多页大小**（如SQL Server的Extent概念，8页一组）来减少元数据开销并改进顺序IO性能。

- **操作系统IO接口：**DBMS可选择不同IO模式：同步阻塞IO（简单但可能浪费CPU等待）、多线程异步IO（如BusTub DiskScheduler模型，每个请求由独立线程处理）、或操作系统提供的异步IO接口(AIO)。Linux的 `io_uring` 和 Windows的IOCP等是现代高性能IO接口。本课程项目未深入Linux AIO细节，但BusTub采用一个后台线程处理IO请求队列，模拟出异步IO效果。实现时需要注意线程安全和正确唤醒等待的调用者（例如通过 promise/future 通知请求完成）。这些机制可以让学生体会将IO与计算重叠以提高吞吐的思想。

**讲师重点：**通过对比如下两种极端场景：

1. 频繁随机写 vs 日志顺序写：前者每次更新都要定位磁盘位置写入，后者只是不断追加日志，不移动磁头，大幅提高写性能。但日志结构需要定期合并，否则查询需要遍历长日志查找最新版本。
2. 单线程同步IO vs 异步多线程IO：讲师强调现代存储设备（尤其SSD）有内部并行性，利用异步IO或多线程可以更充分地利用设备带宽，从而显著提高吞吐。在BusTub的BufferPool实现中，如果串行执行读写，可能浪费很多等待时间，因此DiskScheduler线程能提高并行度。

讲师还指出，日志结构存储广泛应用于Write-Optimized数据库和文件系统（如Write-Ahead Logging本质也是一种日志结构写）。这些概念与后续事务的日志恢复机制相呼应（Lecture 20 会详细讲WAL）。因此，本讲既巩固了对存储层的认识，也为理解后续日志和恢复打下基础。

**阅读材料：**教材第14.8.1节和第24.2节提到了日志结构和磁盘调度相关内容。此外，推荐论文《The Log-Structured Merge-Tree (LSM)》【无具体引用】了解LSM树理念，以及《Rhino: Efficient Disk IO Scheduling》这类研究调度算法的论文来扩展理解。

## Lecture 6: 存储模型与数据压缩

**核心内容：**介绍行存储 (Row Store) 与列存储 (Column Store) 两种存储模型及其适用场景，对比它们的优缺点；深入讨论数据库中的数据压缩技术如何应用于列存储来提高IO效率。

- **行存 (N-ary Storage Model, NSM)：**传统关系数据库采用行存模型，将同一行的所有字段值连续存放在一页内。这对**事务型工作负载 (OLTP)** 有利，因为常常需要访问或修改单个记录的多个字段，行存可以一次IO获取完整记录。Row Store的典型实现即堆文件+槽页面结构，如Lecture3-4所述。在行存中，一个页包含多行，每行按模式列顺序存储。

- **列存 (Decomposition Storage Model, DSM)：**将表按列拆分，每列单独存储。例如有表 T(a,b,c)，则分别存储三个数组：[a1,a2,...]、[b1,b2,...]、[c1,c2,...]。这样**按列存储**的优点是在执行需要只访问部分列的分析查询时，可以极大减少IO。例如一个聚合查询只需要某数值列，就只读该列的存储块而无需读不相关列的数据。此外，同一列的值类型相同且可能相似度高，更适合压缩。列存通常应用于**分析型工作负载 (OLAP)** 或数据仓库场景，因为此类查询涉及扫描大量数据但只关心部分列、并进行聚合计算。缺点是在需要获取整行数据（特别是随机点查很多记录）时，行存往往更高效，因为列存需要从多个列文件分别读取并重构元组，在低局部性访问时性能变差。

- **列存实现细节：**列式存储可在物理上以**列文件**方式实现，每列一个文件或连续磁盘区域。为了重建整行记录，需要有一种对齐机制，比如使用相同的行序或显式记录ID来关联各列。许多列库使用行号作为隐式索引（第 i 个元素对应第 i 行）。当有 NULL 值或缺失值时，一些实现会存储位图来标记有效值。列存也可能引入**分段**(segmentation)，将表按行划分为块(segment)，每块内再按列存储，以平衡行、列访问的效率。

- **压缩技术：**列存储因为同列数据类型相同且相邻值往往相似度高，非常适合应用压缩算法。常见压缩方法：

  - **字典编码**：将列中出现的值建立一个字典，用较小的编码替代实际值存储。对于字符串或重复率高的列效果显著，例如性别列只有M/F两个值，可用1位编码大幅节省空间。
  - **游程编码 (Run-Length Encoding)**：对于连续重复值（例如排序后的列中大量相同值连续出现），存储值和重复次数，而非重复存储值。适合有序列或低基数列的压缩。
  - **位图压缩**：用位向量表示某列取某个值的位置。例如对于布尔或分类数据，每个可能值一个位图向量。
  - **Delta 编码**：存储相邻值之差而非原值，适合数值逐渐变化的情况，再结合熵编码压缩差值。
  - **帧字节压缩**：以每列中最大值确定所需字节数，用更少字节存储较小的值（类似变长整型编码）。

  压缩带来的挑战是**解压成本**：扫描时必须实时解压数据。列存环境下，可以只解压需要参与计算的列，且一些聚合可直接在压缩数据上操作（如游程编码下直接计算总数）。实际DBMS经常将压缩和向量化执行相结合，一次解压一批值，利用CPU SIMD提高解压和计算效率。

- **存储模型混合:** 现代系统有时会混合行列存优点，例如以列存为主，但在需要取整行时使用行缓存技术；或者对部分列建辅助列存索引。还有**PAX (Partition Attributes Across)** 格式，在每页内列式存储，从而提高CPU缓存命中。总之并非绝对二元选择。

**讲师重点：**通过实验数据说明在典型数据仓库查询中，列存+压缩的IO量可比行存小一个数量级，因为只需扫描查询相关的列且数据高度压缩。例如，TPC-H查询往往只涉及大量表的少数几列，列存显著减少无关数据IO。相反，在插入和点查询场景下，列存开销反而更大。讲师强调**压缩率**和**解压成本**的权衡：有些压缩方法压缩率高但CPU开销大，不一定总体最优。因此实际应用中可能使用多种压缩策略混合，根据列访问频率选择不同算法。本讲为后续查询执行优化埋下伏笔，因为压缩和存储布局会影响执行器处理数据的方式。例如执行器若能识别列存格式，就可以更好利用向量化和并行。

**阅读材料：**教材第11.2节介绍了存储模型，第13.6节讲述压缩。推荐阅读经典论文《C-Store: A Column-oriented DBMS》了解列存数据库体系结构，以及《Column-Oriented Storage Techniques for MapReduce》了解压缩在大规模数据处理中的应用。

## 项目1：缓冲池管理 (Buffer Pool Manager)

*项目目标：*实现数据库的**存储管理子系统**，包括**缓冲池管理器**、**页面替换策略 (LRU-K)** 和**磁盘调度器**。通过本项目，学生将实现 BusTub 的页缓存机制，使其能够在内存中缓存磁盘页、管理空闲帧和脏页写回，并确保多线程环境下缓冲池操作的正确性和效率。

- **缓冲池管理器：**BufferPoolManager 是整个DBMS架构的核心组件之一，负责将需要的数据库页面从Disk读入内存、缓存并提供给上层使用，以及在内存空间不足时选择页面驱逐（evict）到磁盘。项目中，缓冲池初始化时包含若干固定大小的帧（frame）作为页面缓冲，每个帧可以装载一个4KB的数据页。需要实现的 BufferPoolManager 方法包括：

  - `NewPage()`: 在缓冲池中分配一个新页，用于表或索引插入新数据时需要新的存储空间。应在DiskManager中分配新的物理页ID，并在缓冲池找一个空闲帧载入。
  - `FetchPage(page_id)`: 获取指定页ID的页面。如果缓冲池已有则返回内存指针，否则需要从磁盘读取该页到缓冲池。如果缓冲池已满则需根据替换策略选择一个牺牲页(evict)，如该页被修改过则先写回磁盘。要更新内部的page table映射。
  - `UnpinPage(page_id, is_dirty)`: 表示上层不再使用该页面，将其pin count减一。如果 is_dirty=true，则标记该页为脏页（需要写回）。当pin count降为0，页面即可被替换。
  - `FlushPage(page_id)`: 将指定页写回磁盘（如果脏）。BufferPoolManager也应提供 FlushAll 刷新所有脏页操作，以在数据库正常关闭时确保数据持久化。

  **线程安全：**缓冲池管理需要支持多线程并发访问。本项目要求对 BufferPoolManager 的内部数据结构（如 page table、替换器、磁盘请求队列等）使用锁机制保护。简化起见，可以使用一个全局互斥锁 (latch) 保护关键路径。在Public测试中，多线程会并发调用 Fetch/Unpin等，要确保不发生竞态条件。BusTub提供了一些 RAII 锁封装类`ReadPageGuard`和`WritePageGuard`，可帮助管理Pin/Unpin和锁定逻辑。实现BufferPoolManager时，需要正确利用这些Guard类构造，保证函数异常返回时也能自动Unpin解锁，防止死锁或内存泄露。

- **LRU-K 页面替换算法：**为了决定缓存满时淘汰哪个页面，本项目实现 **LRU-K** 算法（K取2）作为页面替换策略。LRU-K 算法通过记录页面最近的K次访问时间来估计其闲置间隔(backward K-distance)，选择 backward K-distance 最大者淘汰。简单来说，就是在考虑更长的历史信息，而非传统LRU只看最近一次。实现要求：

  - 维护一个 `LRUKReplacer` 类，提供 `RecordAccess(frame_id)` 方法记录帧被访问（Pinned）；`SetEvictable(frame_id, evictable)` 方法设置帧是否可被淘汰（例如pin count>0时不可淘汰）；`Evict()` 方法返回应淘汰的帧ID。LRU-K的数据结构需要跟踪每个页面的最近访问时间戳队列（长度K）及其可淘汰状态。
  - 当页面第一次被访问时，由于不足K次历史，应赋予其无限大的 backward K-distance（表示不宜立即淘汰）。只有积累K次访问后才能比较距离。
  - 本项目K=2，因此重点是维护每个帧最近一次和倒数第二次访问时间，用当前全局时钟减去第二最近访问时间得到距离。选取距离最大的淘汰。如果有帧从未被访问两次，则它的距离视为∞，如果存在多个这样的候选，则按最早最近访问时间淘汰最久未被访问的。

  实现LRU-K需要注意线程安全，因为BufferPoolManager的不同线程会并发记录访问或修改可淘汰状态。可在调用 Replacer 的方法时使用 BufferPoolManager 锁保护。

- **磁盘调度器：**BusTub 引入 `DiskScheduler` 来异步处理磁盘IO。而非每次Fetch/Flush都直接调用 DiskManager 读写文件，BufferPoolManager可以将IO请求封装成 `DiskRequest` 放入调度队列，由专门的后台线程顺序执行。这样可以将IO与计算解耦，提高并行性。项目要求实现：

  - `DiskScheduler::Schedule(DiskRequest request)`: 将读/写请求放入内部队列，并返回一个 `future`（通过 `std::promise<bool>` 实现）给调用方，用于等待请求完成。请求包含页ID、读或写标志、数据缓冲区指针、以及 promise 对象等。
  - `DiskScheduler::StartWorkerThread()`: 后台线程函数，不断从队列取出请求并调用 DiskManager 执行实际读写。完成后通过设置 promise 值通知等待线程请求完成。线程在 DiskScheduler 析构时结束。

  磁盘调度器带来了和BufferPoolManager交互的新模式：FetchPage不再直接读盘，而是调用 Schedule 提交请求然后阻塞等待 future 完成结果。总的来说，这层异步机制对上层透明，但内部实现更复杂。项目提供了 `Channel` 类实现线程安全的队列和通知，可直接使用以免重复造轮子。

- **Frame Header & Page Guard：**BusTub 代码中，每个缓冲帧有对应的 `FrameHeader` 元数据对象，包含pin_count、is_dirty等字段。本项目还需要实现 `ReadPageGuard`/`WritePageGuard` 两个RAII类的构造、移动赋值运算符和 Flush/Drop 方法。它们用于封装页面操作的生命周期：构造时Pin页面，析构时Unpin页面。如果是WriteGuard还需在析构时将页面标记脏并可能触发写回。这些Guard在BufferPoolManager的 `CheckedOutPage()` 方法中返回，为上层提供安全的页面访问对象。实现Guard类需注意C++移动语义，防止不必要的拷贝。

**实现提示与难点：**

- **正确性优先:** 确保线程安全和正确淘汰是第一位的。可以采取简单策略：BufferPoolManager所有公共接口用单一互斥锁保护临界区。虽然这样可能影响性能，但能保证正确。在完善正确性后，再考虑优化锁粒度。
- **淘汰协调:** 一个页被淘汰需满足：pin_count=0 且 被标记可淘汰。结合BufferPoolManager逻辑，当Unpin使pin_count降为0且调用SetEvictable(true)后，该帧才会在LRU-K候选中。要避免这样情况：并发线程A fetch页、线程B很快unpin页且LRU-K将其淘汰、而线程A仍在使用（pin_count曾递增未decrement）—这可通过pin_count机制避免。
- **LRU-K 调试:** 可以通过访问序列测试LRU-K。如K=2，访问序列 [A,A,B,A,B,C] 后检查 replacer 内部状态是否符合预期。项目的单元测试也涵盖了替换策略的基本行为，如不会淘汰仍在使用的页等。
- **性能考虑:** 项目提供了**排行榜测试(Leaderboard test)**，对缓冲池性能进行评比。优化方向包括改进替换算法考虑访问模式（如Zipfian分布的热点页面）、并行处理多个IO请求等。这些优化属于选做内容，不强制要求，但提供挑战空间。

*项目总结：*完成Project 1后，BusTub已经具备基础的存储引擎：可以分配新页、缓存页并读写、替换冷页、异步刷脏页等。这为后续实现更高级的模块（索引、执行器、事务）提供了存储支撑。学生通过本项目理解了缓冲池的重要作用和实现细节，包括如何在多线程环境下保证缓存一致性，以及如何通过策略提高缓存命中率和IO并发。这个模块在数据库内核中至关重要，一旦BufferPool有bug，可能导致数据不一致甚至崩溃，因此它也是项目中最具工程挑战的部分之一。

## Lecture 7: 哈希表与散列索引

**核心内容：**介绍哈希表在数据库中的应用，包括内存中的哈希数据结构（用于算子如哈希连接、聚合）以及磁盘上的哈希索引（如可扩展哈希 Extendible Hashing）。讨论不同哈希方案的原理和性能特点。

- **内存哈希表：**哈希表是典型的键值存储数据结构，通过哈希函数将键映射到桶，实现平均 O(1) 时间的插入查找。在数据库执行过程中，哈希表用处很多，例如执行哈希连接时，需要将构建端数据放入哈希表，以根据连接键快速检索匹配行；又如聚合操作利用哈希表按分组键归并聚合计算。因此DBMS通常有自己的哈希表实现，优化内存布局和局部性。讲师可能阐述了开放寻址和链式哈希两种哈希表实现，指出在现代处理器上，减少指针追踪（如开放寻址将元素放在数组减少跳转）有利于缓存命中。BusTub在执行器中实现了 `SimpleAggregationHashTable` 用于 GROUP BY 聚合，就是开放寻址的例子。

- **哈希索引 (Hash Index):** 数据库索引的一种类型，通过对键值哈希存储来实现加速查找。典型的动态哈希索引如**可扩展哈希 (Extendible Hashing)**：使用一个全局位深度和桶局部位深度，当桶溢出时，通过增加全局深度或桶局部深度对哈希地址空间进行扩展和分裂，从而容纳更多元素。可扩展哈希保证查找在至多两次桶访问内完成（一次计算哈希，一次访问桶；如果桶溢出分裂需要重算哈希前缀）。另一个方案是**线性哈希 (Linear Hashing)**，通过渐进式扩展无需全局目录。课程重点应在Extendible Hashing，包括：

  - **目录 (Directory)：**维护一个指针数组，每个条目指向一个桶。目录大小为2^d（d=全局深度），每个桶有自己的局部深度ℓ，表示该桶共享哈希前缀长度。当某桶满且局部深度等于全局深度时，需要双倍扩展目录并增加全局深度；如果局部深度小于全局，则只分裂该桶并增加其局部深度。
  - **插入和分裂：**插入时，根据键哈希值前d位找到目录索引，定位桶；如果桶未满则插入，否则触发桶分裂：分配新桶，将原桶元素按哈希第ℓ+1位重分布，新桶局部深度ℓ+1，原桶局部深度也加1。如果分裂后局部深度 > 全局深度，则扩展目录。需要更新目录中对应的指针一半指向新桶。
  - **查找和删除：**与普通哈希表操作类似，根据哈希值找目录项访问桶即可。删除需考虑桶合并（Extendible Hashing可选实现桶合并以节省空间，但不是必须）。

- **哈希索引 vs B+树索引:** 哈希索引擅长等值查找（=查询），因为哈希将键直接定位。但它不支持顺序遍历（无法用于 range 查询，如 `WHERE key > 50`），因为哈希打乱了顺序。相比下，B+树索引支持范围扫描、有序输出，但等值查询性能稍逊一点（logarithmic vs 哈希平均常数）。课程通过对比强调**索引选择**需基于查询类型：如果主要是等值查询且数据没有顺序遍历需求，哈希索引可能更高效。不过大多数关系DBMS还是以B+树为主要索引，哈希索引较少见（PostgreSQL支持哈希索引但默认不使用，因为维护代价和崩溃恢复复杂）。NoSQL系统则大量使用哈希 (LSM+哈希)。

- **布隆过滤器 (Bloom Filter):**作为**过滤器**的主题部分，布隆过滤器是一种空间高效的概率数据结构，用于测试某元素是否可能在集合中。它由位数组和k个独立哈希函数组成，插入时对元素计算k个哈希，在位数组相应位置置1；查询时若任一对应位为0则元素一定不在集合，否则可能在集合中（存在一定误报率）。Bloom Filter常用于数据库优化：如哈希连接时对build表键建布隆过滤器，在probe阶段快速跳过不存在的键；或缓存系统中判定请求是否应该查询数据库。课程介绍了布隆过滤器的原理及其**误判率**计算：m位数组, n个元素, k个哈希函数，误判率约 `(1 - e^{-kn/m})^k`。可以通过选择适当的k和m/n比得到期望误判率。例如k ≈ (m/n \* ln2)。BusTub 没有要求实现Bloom Filter，但课程列入“Indexes & Filters”议题，说明其重要性。

**讲师重点：**通过实例演示Extendible Hashing的插入分裂过程，加深对目录和局部深度概念的理解。讲师也可能提到实际实现中为了减少连续分裂带来的抖动，可一次性扩容更多空间或使用更灵活的哈希结构。对于Bloom Filter，讲师强调它可以极小代价换取过滤大部分不必要访问。例如TPC-H某些查询使用布隆过滤器减少不匹配行扫描，提高join性能。此外，课程提到布隆过滤器的一个缺陷是无法删除（除非用Counting Bloom Filter），但对于只构建一次查询多次的场景，标准布隆过滤器已经足够。

**阅读材料：**教材第14.5节介绍哈希索引基本概念，第24.5节讨论Bloom Filter。建议阅读经典论文〈Fast Text Search with Bloom Filters〉了解布隆过滤器的应用，以及数据库领域关于哈希索引的比较研究。

## Lecture 8: 索引与过滤器 I – B+树索引原理

**核心内容：**深入讲解数据库中**B+树索引**的数据结构与算法。B+树是关系数据库中最常用的索引结构，支持高效的范围查询和等值查询。本讲涵盖B+树的节点结构、插入、删除、查找算法，以及分裂和合并机制。

- **B+树结构：**B+树是一种平衡树，每个节点可存储多个键和指向子节点的指针。与二叉树不同，B+树是多叉 (阶d较高)，通常一个节点可包含成百上千的键，从而树的高度很低。所有实际数据记录仅存储在**叶子节点**，内部节点只存储键用于导航。叶子节点通过链指针相连形成有序链表，方便范围扫描。B+树的秩 (order) 通常定义为每节点最多容纳m个指针(内部)/m-1个键；至少容纳⌈m/2⌉指针（除根可例外）。

- **搜索 (Search):**B+树搜索类似二分查找的推广。从根节点开始，在当前节点用键值范围确定下一层孩子指针，直至叶节点。在内部节点是按键值有序存储，可以用二分法定位孩子指针。这过程高度为O(log_n N)，非常高效。当树高为h时，需要访问h+1个节点（h层内部节点+1层叶）。由于节点扇出大，h通常很小，例如几百万记录可能高度3或4。搜索完成后，可得到目标键所在叶节点，再在叶节点的记录列表中定位精确记录或插入位置。

- **插入 (Insert):**在B+树插入新键：

  1. 首先找到所属叶节点位置。
  2. 将键插入叶节点按序放置。如果叶节点未满，则直接插入完成。
  3. 若叶节点满（达到m-1个键），则发生**节点分裂 (split)**。一般采用**中间分裂**：将叶节点一分为二，各含约一半记录。取中间键拷贝（较大的第一个键）提升到父节点。父节点插入该分裂键和新增子指针。
  4. 如果父节点也满，则递归向上分裂，可能一路到根。根分裂时树高加一。
     分裂过程中要更新相邻节点的链指针（叶节点之间）以及父子指针。

  B+树由于每次分裂只影响一小部分节点且树保持平衡，所以单次插入代价 O(log N)。需要注意保持B+树所有叶在同一深度、节点利用率约在50%以上。

- **删除 (Delete):**删除与插入相反：

  1. 定位键所在叶节点，移除记录。
  2. 若叶节点删掉记录后数目低于⌈(m-1)/2⌉（即半满以下），需要**合并 (merge)** 或**借位 (redistribute)**。可以尝试从邻近兄弟节点借一个键（调整父节点分隔键）来补充；如果兄弟也刚好半满，则将当前节点与兄弟合并为一个节点，并删除父节点中的分隔键。
  3. 可能递归向上传播：父节点键数量减少低于半满时也需合并/借位。如果合并到根导致根仅剩一个子，则树高度减一。

  删除较插入复杂，因为需要决定合并或借，从而保持平衡。实现时通常优先借位以减少合并次数（避免过度缩小节点利用率）。

- **并发环境的索引：**本讲可能先不涉及并发，这是Lecture 10内容。但要为后续埋下概念，比如B+树修改必须保证原子性，否则半途插入/删除失败会破坏树结构。现实系统对B+树操作通常加锁维护一致性。

- **索引元数据:** B+树索引通常关联表并声明键类型、比较函数等。对变长键或复合键，节点中存储需要考虑对齐和大小问题。大型键可采用前缀压缩减少内部节点存储占用。本课程未深究这些实现细节，聚焦算法流程。

**讲师重点：**讲师通过图示演示B+树插入和删除的例子。例如插入一系列数如何导致叶子分裂、内部结点分裂；删除一系列数如何触发节点合并。强调在B+树中**平衡**是自动维持的（每次局部调整后树仍满足平衡条件），无需像AVL树那样频繁旋转调整，因此B+树非常适合磁盘存储（少量大节点IO效率高）。还提到B+树的**空间利用率**，理论下界50%，实际往往达到70%左右。课程可能比较B+树与哈希索引：B+树略逊于哈希在点查性能，但支持范围查询和排序输出，因此更通用。B+树也是项目2的实现重点，讲师提醒大家注意边界条件处理，如分裂时提升的键是叶节点的最小右半部分key，删除合并时要更新父节点键等等。

**阅读材料：**教材第14.1-14.4节完整介绍B+树原理，包含插入删除算法伪代码。项目规范中也给出了B+树插入删除的路线图。参考这些资料有助于实现项目2。

## Lecture 9: 索引与过滤器 II – B+树高级细节与布隆过滤器

**核心内容：**继续讨论B+树索引，关注并发控制和系统实现细节，并回顾**布隆过滤器**等辅助数据结构的作用。还可能涉及其他特殊索引类型或应用场景。

- **B+树的并发控制:** 数据库通常允许多个事务并发操作同一个B+树索引，需要确保一致性和隔离性。典型方法是 **锁耦合 (latch coupling)**：在沿树查找时，对当前节点加锁，再请求下一节点锁成功后释放父节点锁。这避免长时间锁定上层节点，提高并发性。在插入/删除时，需要对可能分裂/合并影响的节点上锁。具体协议如**Crabbing**：沿路径下行获取所需的写锁，在分裂或合并操作完成后再逐步释放。需要小心避免死锁，例如总是自顶向下获取锁（或者定义节点顺序）。本讲概要介绍2PL在索引中的应用，实际更详细讨论在Lecture 10。

- **B-link 树:** 提及一种B+树变体B-link树，它在每个节点增加指向右兄弟的指针，使并发算法可以在不锁父节点情况下处理分裂。一旦节点分裂完成，通过设置新节点和更新兄弟链，查找过程中如果发现范围超出当前节点就顺着右链找。B-link树可以实现无锁或低锁并发查找。但实现复杂，课程可能只是提及概念。BusTub项目未要求实现B-link，但理解其动机有助于认识并发索引的困难。

- **元数据和重平衡:** B+树索引往往需要维护额外元数据，如当前树高、根节点页ID、节点计数等。删除操作可能需要特殊处理，比如有的实现选择不合并节点而使用“懒惰删除”(标记删除)来简化并发实现，把空间回收留给后台任务。课程或许提到MySQL InnoDB的做法：InnoDB的B+树索引大多不主动合并，以避免频繁页面分配释放带来的碎片，可在空闲率高时重建索引。BusTub实现会严格按照教材算法合并，这是学术实现风格。

- **布隆过滤器复习:**再次强调Bloom Filter在数据库中的用例。尤其针对**索引扫描**和**查询优化**：查询执行器有时可使用布隆过滤器进行**索引筛选**。例如在索引嵌套循环连接中，先用布隆过滤器排除不可能匹配的外表行，以减少索引查找次数。课程已在Lecture 7详细介绍Bloom Filter原理，此处可能结合查询优化场景再说明其作用。

- **其它索引类型:** 如果时间允许，讲师可能介绍其他索引：如位图索引（适合低基数字段，如性别、布尔值，用位向量加速多个条件组合查询），全文索引（倒排索引）等。这些一般不在项目范围，属于扩展阅读内容。

**讲师重点：**着重说明**索引并发**的重要性。讲师举例如果没有正确的锁## Lecture 10: 索引并发控制与连接算法

**核心内容：**探讨B+树索引在并发环境下如何保证一致性（**索引并发控制**），并简介常用的连接算法（嵌套循环、排序-合并、哈希连接）的性能特点及应用场景。

- **B+树索引并发控制:** 在多线程或多事务并发访问B+树时，需要防止竞争导致的索引结构破坏和数据不一致。本课程介绍 **闩锁耦合(Latch Coupling)**（又称锁蟹行，crabbing）机制来安全地并发操作B+树：在向下搜索树时，对当前节点加读锁或写锁，在确定下一步访问的子节点后，再获取子节点锁，然后释放父节点锁。这样保证始终有路径上的相邻两层持锁，但不会长时间锁定整条路径，提高并发性。

  - _查找_：获取根节点读锁，找到对应子指针后获取子节点读锁再释放根锁，依此类推直到叶节点。全程只持有至多两个相邻节点的读锁，其他线程仍可访问未上锁部分。查找不修改树结构，因此读锁即可。
  - _插入/删除_：由于可能引发节点分裂或合并，需要升级为写锁。一个简单策略是在到达叶节点前都持读锁，下到叶节点或需要修改处时，将相邻节点的读锁升级为写锁（可能需要短暂释放重新加锁以避免死锁）。更保守的做法是一路持有写锁（即独占锁蟹行），但这样并发度低。

  关键在于遵循自顶向下获取锁顺序，避免产生死锁（因为不同线程总按相同次序锁定节点）。课程建议“不应两次获取同一读锁”，否则可能死锁。总之，正确的索引并发控制既要保护树结构完整性，又尽可能减少阻塞，提高并行查询效率。

  **Latch-Free 索引:** 高级主题中，讲师可能提到去闩锁化B+树（如LLAMA或乐观验证索引）和B-link树(每节点加兄弟指针)。B-link树允许分裂后不锁父节点，只调整兄弟链，由搜索线程通过检查链指针来应对并发分裂。这种技术超出本课程实现范围，但前沿数据库里正变得重要。

- **连接算法概述:** 关系数据库执行连接（JOIN）有多种算法：

  1. **嵌套循环连接 (Nested Loop Join, NLJ):** 最简单，双层循环遍历两表，每取外表一行，遍历内表找匹配。复杂度 O(N\*M)，若无索引通常较慢。适合小表连接或有内表索引可利用时（Index NLJ）。
  2. **排序-合并连接 (Sort-Merge Join):** 先对两输入按连接键排序，然后线性扫描合并，类似合并排序的归并步骤。复杂度 O(N log N + M log M) 包括排序开销。适合没有索引但数据可以排序、且可以利用排序后的顺序处理范围条件、消除重复等。
  3. **哈希连接 (Hash Join):** 分构建(build)和探测(probe)两阶段。将较小的一张表构建哈希表（键为连接列，值为元组集合），然后扫描另一表，对每行计算哈希在哈希表查找匹配。平均复杂度近 O(N+M)。需要内存容纳整个哈希表，不支持有序输出。对于等值连接非常高效，是OLAP常用算法。

  BusTub 项目3 要求实现 Nested Loop Join 和 Hash Join 两种算子。其中 NLJ 包括内联的**索引嵌套循环连接**（外表每行用索引查询内表匹配）。Hash Join 则需实现 build 哈希表和 join 时的多重匹配处理以及左右外连接（未匹配要产出NULL）。还需要在优化器中将可以用哈希连接的 NestedLoopJoin Plan 转换为 HashJoin Plan。课程内容为项目实现提供理论依据。

  **应用场景:** NLJ 在外表很小、内表有索引时性能好（每次内表查找 O(log M)）；Sort-Merge 在两表都大且可顺序IO读取时表现稳定，且适合范围连接；Hash Join 在等值连接场景通常最快，但占用内存多。讲师会通过示例说明：如Star Schema(一大一小表)用Index NLJ，Large-large equi-join用Hash Join等。

**讲师重点：**强调**索引的正确性**必须在并发操作下保持。锁耦合被形象比喻为蟹行：抓住一只钳子前另一只不能松开，否则会掉下来。这个类比对应锁定下一节点前不能释放前一节点。讲师可能列举常见错误：没有锁父节点就分裂，会导致并发查找缺失部分数据；不当的锁升级可能死锁等等。

关于连接，讲师通过**性能对比**说明选择合适算法的重要性。例如两个1千万行表连接，用NLJ需10^14比较，不切实际；Hash Join仅需线性两遍，排序合并也接近线性。因此优化器必须选择正确算法。也介绍了**外连接**的变种（NLJ和Hash Join如何实现左外/右外连接，通过在未匹配时输出NULL）。最后，本讲为接下来的事务处理话题（并发和锁）做了铺垫，因为B+树并发控制实际上是特殊的锁管理例子。

**阅读材料：**教材第15.4-15.6节详细讲解了连接算法。另外Hector Garcia-Molina的数据库教材中也有连接算法比较。对于索引并发，可阅读经典论文《The BW-Tree: A B-tree for New Hardware Platforms》看现代无锁索引设计，以及教材第18.10.2节简述了索引锁。

## Lecture 11: 排序与聚合算法

**核心内容：**介绍数据库执行层的**排序**和**聚合**算法，包括外部排序、分组聚合（尤其基于哈希的聚合）等。讨论如何利用内存和磁盘高效执行 ORDER BY 和 GROUP BY 查询。

- **外部排序:** 当待排序数据量大于内存时，需要**外部排序算法**。经典方案是 **外部归并排序 (External Merge Sort)**：

  1. _划分阶段(run generation)_：将数据分成可装入内存的块，每块在内存内部排序（例如用快速排序），然后写出排序后的run（有序子文件）。
  2. _归并阶段(merge)_：多路归并这些有序runs。一般每次归并k个runs为一个更大的有序run，重复直到只剩一个run即全局有序。为了减少阶段数，常在每次归并时尽可能多路归并（受限于可用缓冲页数）。

  两路归并排序需要log₂N次归并，若一次多路归并k路，则需要log_k N次归并。BusTub 实现中限定**两路归并**且排序元组定长简单类型。项目提供 `SortPage` 让学生设计页布局存放排序过程中暂存的元组。实现要注意每轮归并后应删除上一轮临时页（防止“僵尸页”占据缓冲池）。

  为了提高效率，归并时可以利用**替罪羊删除(Replacement selection)**生成更长初始runs，或**归并树**优化IO。但项目不要求复杂优化，只需正确两路归并。

- **聚合算法:** SQL聚合分为**分组聚合**(`GROUP BY`)和**全表聚合**（无分组）。实现分组聚合常用两种方式：

  - **排序聚合**：先按分组键排序，然后顺序扫描聚合相同键组。排序开销O(N log N)，适合小数据或已排序输入。
  - **哈希聚合**：遍历输入，用哈希表将相同分组键的行聚集。对每个元组计算组键的哈希，如果哈希表中不存在该键则新建聚合累加器，否则更新现有累加值。最后哈希表中每项即一个分组结果。

  BusTub 选择实现**哈希聚合**（AggregationExecutor）。项目提供 `SimpleAggregationHashTable` 简化哈希表操作。需完成其 `CombineAggregateValues` 等方法，将一行的聚合值并入已有组。在聚合执行器中，Init() 阶段遍历child执行器将所有元组聚合进哈希表，Next() 阶段迭代哈希表输出结果。也就是说哈希聚合在BusTub中**阻塞执行**（需要先消耗全部输入）。

  **NULL处理:** SQL标准聚合对NULL一般跳过不计入。BusTub要在实现中检查`Value`是否为NULL，像COUNT跳过NULL，MIN/MAX遇NULL则特殊处理。此外Group-by列不会为NULL。

  **Distinct 去重:** 实现`SELECT DISTINCT col`可以看成特殊的Group-by查询（对col分组但无聚合函数）。哈希聚合天然能支持distinct：将值作为key放哈希表即可。

- **排序 vs 哈希:** 哈希聚合通常更高效，因为O(N)期望时间，不需要排序。但哈希需要内存存放哈希表，且不能直接输出有序结果。如果查询需要结果排序，还得再排序。排序聚合则自带顺序，可以输出排序结果，或用于下游merge join等。本讲旨在让学生理解聚合算子的实现细节和性能权衡。

**讲师重点：**通过案例说明**外部排序**如何处理海量数据，例如1GB数据、内存只能装100MB，需要多少轮归并。讲师可能举例：初始分成10个runs，每个100MB排序后归并，多路归并可以一次5路归并两轮完成。如果用两路，则需要更多轮。BusTub限制两路是为了简化实现，实际系统会根据缓冲页调节归并路数。

对于聚合，讲师强调**聚合的算子属性**：

- 聚合是**pipeline breaker**（流水线阻断）：因为必须消费所有输入才能输出第一条结果（除非某些流式可聚合函数如计算平均值不能提前输出）。这在执行器实现和优化器考虑上很重要。例如BusTub的执行器设计中，AggregationExecutor的Next()要先确保Init()完成全部聚合运算。
- NULL处理和初始值：例如COUNT初始0，SUM初始0，MAX/MIN初始NULL或极小值。BusTub通过 `GenerateInitialAggregateValue` 方法生成每组的初始聚合值，并定义了`Value integer_null`表示NULL的聚合结果。

讲师也可能提到**Having子句**的实现：实质上是聚合后再筛选，因此Planner会在聚合后加一个Filter节点处理Having。BusTub也遵循此逻辑（AggregationPlanNode不直接包含Having条件，而是在执行器层忽略Having，由Planner另建FilterPlan）。

**阅读材料：**教材第15.4-15.5节覆盖了外部排序算法及实现细节，第15.7节讲了聚合操作的执行。对于哈希聚合和NULL处理，可参考PostgreSQL源码或文档了解实际DBMS如何优化聚合，包括分区哈希聚合（大数据溢出内存时分批处理）等高级内容。

## Lecture 12: 连接算法 – 排序 vs 哈希 vs 嵌套循环

**核心内容：**深入比较不同JOIN算法的优劣，并介绍**连接顺序优化**的重要性。还可能涵盖**外连接**和**多表连接**的处理。

- **嵌套循环连接(NLJ) 复习:** 简单但可能代价高。当没有索引或数据量大时性能糟糕，但在小数据或索引用的上场景下有用。如表A很小（几行），表B大但B.key有索引，则Index NLJ对每个A行用索引查B，只需|A|次索引查找（成本≈|A| \* log|B|）。BusTub中的 NestedLoopJoinExecutor 就实现了普通 NLJ，对于内表如果是IndexScan计划，优化器会转换成 NestedIndexJoinExecutor，提高效率。

- **排序-合并连接(SMJ):** 适合等值或范围连接。要求输入已排序，若未排序需先排序消耗O(n log n + m log m)。排序后归并只需线性扫描。因此SMJ性能 = 排序成本 + O(n+m)。当两表均大、无索引但可以顺序读写时（比如数据在磁盘顺序存储），SMJ表现不错，且产出有序结果方便后续处理。局限是只能应用于**等值和范围**连接，对非等连接无直接适用（不过少见）。

- **哈希连接(Hash Join) 复习:** 只适用于等值连接。时间期望线性O(n+m)，通常最快。对内存要求较高，需能容纳哈希表，如过大则要分区处理（Grace Hash Join）将数据分为分区，各分区各自哈希连接。课程未深入分区哈希，但提示**Skew**问题：当哈希分布极不均匀，会造成大桶导致性能退化甚至溢出内存。实际系统有应对策略，如双重哈希或预估基数调整桶大小。BusTub项目HashJoinExecutor实现的是简单内存哈希，不考虑溢出。

- **连接顺序优化:** 多表连接时，不同连接顺序成本相差巨大（NP-hard优化问题）。SQL优化器通常用动态规划找较优连接顺序。课程可能举例三表连接有两种关联次序((A⨝B)⨝C 或 A⨝(B⨝C))，成本可差一两个数量级。虽未深入算法，但强调要基于估计基数来决策。BusTub优化器未要求学生实现DP优化，但了解其存在有助于理解为什么实际查询执行计划选择某种连接顺序。

- **外连接:** 左/右外连接和全外连接需要输出未匹配行。在NLJ中易实现：对于外表每行若无匹配则输出填NULL；Hash Join则在build哈希表后，对外表每行探测时记录匹配，如无则输出NULL。BusTub HashJoinExecutor 实现了左外连接（Left join），需在输出阶段检查哪些左表行从未匹配然后输出它们。执行器里通常维护一个布尔标记数组记录左侧行是否匹配过。

- **连接与索引:** 讲师可能强调：有索引时，Index NLJ可能比构建哈希快，因为索引已存在可直接用；无索引且等值时Hash Join最佳；排序合并适合范围且需要排序结果场景。此外，**多连接算法联合**也出现，比如**混合哈希连接**，先哈希一部分，小部分用索引连接。课程简要提及不展开。

**讲师重点：**通过示例说明**选择错误算法或顺序代价严重**。如三个表 A(1000行),B(1000行),C(1000行)，若全用NLJ三层循环约10^9比较，而选对顺序和算法也许只需几万次操作。讲师也可举真实DB案例：早期MySQL仅NLJ导致多表join性能差，在新版本中引入更多算法和优化才改善。

BusTub在项目3的优化部分要实现一个优化规则 `NLJAsHashJoin`，在可能时将 NLJ 转为 HashJoin。讲师强调**等值连接**的哈希转换判断依据：连接条件全是列相等的合取且适合hash join。提示学生留意计划树结构以应用转换。

**阅读材料：**教材第15.5-15.6节进一步比较各种JOIN，包括外连接处理。可参考数据库实现书籍如《SQL查询优化器的艺术》了解商业系统如何选择JOIN策略。Understanding高级连接技巧对深入数据库优化很有帮助。

## 项目2：数据库索引 – 实现 B+ 树索引

*项目目标：*实现 BusTub 中的 **B+树索引** 模块。学生将基于 Project 1 的存储系统，在 **Buffer Pool** 之上构建支持并发的 B+ 树索引，包括插入、删除、单键查询和范围扫描等功能。该项目分解为四个任务：

- **Task #1: B+树节点页面类** – 实现基本的 **B+树页结构**:

  - `BPlusTreePage` 基类：包含通用字段，如是否叶节点/内部节点标志、当前存储的键值对数量 `size`、指向父节点的页ID、是否为根节点标志等。
  - `BPlusTreeInternalPage`：内部节点，存储 `键-子节点指针` 对。需要实现插入/分裂时对其中键指针对的移动操作，如当子节点分裂时如何在父节点插入新键指针对。内部节点有 `size` 个键和 `size+1` 个子指针（插槽0留空或存特殊最小键）。
  - `BPlusTreeLeafPage`：叶子节点，存储 `键-值(记录RID)` 对。支持顺序存取，包含指向左右兄弟叶的指针。当叶分裂时要设置兄弟指针更新。叶节点的值是记录ID (RID)，键是索引键类型（可为整数、字符串等）。

  学生需实现这些页面类的方法，例如 `LeafPage::Insert(key, value)`，`InternalPage::Lookup(key)`，`InternalPage::InsertNodeAfter(old_child, new_key, new_child)` 等，用于在节点内部插入/查找。项目提示具体字段的偏移和含义。

- **Task #2: B+树操作（插入、删除、查找）** – 实现 B+树索引本身的主要操作：

  - `BPlusTree::GetValue(key, result_vector)`: 查找给定键，返回对应的RID列表（可能存在重复键多值情况）。需要沿树逐级查找直到叶节点并在叶节点顺序搜寻键。
  - `BPlusTree::Insert(key, value)`: 插入键值对。插入分三个阶段：查找插入位置（叶节点）；插入叶节点，如果叶满则**分裂叶**并将中间键插入父节点；如果父节点也满则递归分裂...一直到根可能分裂增加新层。需要小心更新兄弟指针和父指针，以及新产生页的元数据。项目特别要求插入时如果遇到**重复键**情况，可直接返回或处理成更新，这里通常B+树索引允许重复因此要插入到键相同项后面去。
  - `BPlusTree::Remove(key)`: 删除键值。找到键所在叶节点删除之。若删除后叶节点数量过少(< half-full)则需要**合并或借位**：检查左或右兄弟，若其有多于半数元素则**借一个**调整；否则与兄弟**合并**并删除父节点中的分隔键。然后递归向父检查，父若低于半满也同理调整或合并。需注意更新父子指针、兄弟链表等。BusTub删除需实现**合并再分配**机制避免树不平衡。
  - 迭代器（Task #3 Index Iterator）：实现 `BPlusTree::Begin()` 返回指向第一个键的迭代器，支持 `++` 遍历整棵树的所有键值对。迭代器内部保存当前叶节点指针和索引，++时若到叶末尾则跳到右兄弟叶起始。这测试范围扫描能力。

- **Task #4: 并发控制** – 为 B+树实现**锁耦合**以支持并发安全操作。需使用**Latch(锁)**来保护树结构：

  - 采用**读写锁**（BusTub提供`std::mutex`或`std::shared_mutex`）: 查找可用共享锁，多线程可并发读；插入删除需独占锁。锁耦合策略：在向下遍历节点时，先锁当前节点，再锁子节点，然后释放当前节点锁，以此类推。
  - 分裂和合并期间需锁定相关节点直至操作完成再释放，保证原子性。例如插入导致叶分裂，要锁住叶及其父节点直到父插入完成。
  - 为避免死锁，遵循**自顶向下加锁顺序**。此外**不可以重复获取相同节点的锁**（可能死锁）。
  - BusTub 要求实现 _“crabbing”_：即锁耦合释放父锁。不要求实现无锁算法，只要正确串行化并发修改即可。

  _调试并发很难_，项目建议先实现单线程正确，再加入锁调试。一个常见策略：插入删除等写操作使用全局锁串行化，保证正确性，然后逐步细化加锁范围以增加并发。

**项目难点与调试:**

- B+树指针/索引操作复杂，要小心处理**边界条件**。如：

  - 插入分裂取中间键：对于键数为偶数可能选择中间靠右的键上提，确保两节点平衡。
  - 删除合并选择左或右兄弟：实现上通常优先考虑左兄弟借或合并（或根据键分布选择）。要正确更新父节点分隔键，合并后父键要删除。
  - 根节点特殊情况：当根变空或只有一个子时调整高度。如果删除使根没有键且有一个子，则将子升为新根，高度减一。

- **并发**：确保锁释放位置正确，不遗漏也不多持。常见Bug如：未锁兄弟节点就访问调整，可能被其他线程并发修改。或删除时先锁叶再锁父，如果父正被另一删除锁住易死锁，需要按协议顺序或先锁父再锁子。可借鉴课程给的伪代码或教材算法做参考。BusTub期望的是一种 _Lock Coupling_ 基本实现，不追求极限并发性能，只求正确避免死锁和不一致。

- **测试**：项目提供单元测试，会随机插入大量键、删除键，并检验树的有序性、完整性（比如中序遍历验证顺序，每节点键数范围）。还有并发测试，多线程同时插入/删除检查最终树正确性。调试技巧包括：加日志打印树结构变化；构造小数据手工验证；对复杂并发场景，可以限制只开一个线程跑，逐步扩展。

*项目总结：*完成Project 2，学生实现了数据库核心**索引**结构 – B+树。BusTub此时具备一个持久化索引，可供执行器层使用，加速查询。通过此项目，学生掌握了B+树平衡条件维持的代码细节，以及**在缓冲池框架上**如何基于页面实现复杂数据结构。此外，并发任务使他们亲身体验了在数据结构上加锁的复杂度，提高了对数据库并发控制难度的认识。这个索引模块将在后续项目被整合进查询执行（Index Scan执行器）和事务机制中。

## Lecture 13: 查询执行 I – 逻辑计划与执行器架构

**核心内容：**介绍数据库查询从SQL到执行的全过程：**查询计划**的表示、执行器（算子）模型，以及BusTub中**查询执行框架**的结构和交互。

- **查询计划表示:** 现代DBMS将解析SQL得到的语法树转化为**逻辑查询计划**（由关系代数算子组成的树），再经过优化器生成**物理查询计划**。物理计划由具体算子实现（如选择算法、连接算法）的树表示。BusTub 在提供的架构中有 `AbstractPlanNode` 和具体 PlanNode 类表示计划树，每个 PlanNode 对应一个算子操作（seq scan, index scan, join, agg 等）。优化器可以生成不同 Plan 来实现同一逻辑操作，例如用IndexScan+NLJ或HashJoin两种计划实现相同JOIN。

- **Volcano 执行模型:** 数据库执行通常采用**迭代器模型**（又称 Volcano 模型）。每个算子实现一个接口，如 `Init()` 初始化，`Next(tuple)` 获取下一输出元组。执行器树按迭代器方式工作：顶层算子pull数据，从child执行器调用Next获取输入，再加工输出。BusTub执行框架遵循这一模型：

  - **Executor 类**: `AbstractExecutor`定义统一接口，`Init()` 和 `Next(Tuple*, RID*)`。每种算子有对应 Executor 子类实现这些方法。
  - 在执行查询时，先构造执行器树（根据计划树创建对应Executor并链接），然后调用顶层 Executor::Init() 再不断调用 Next() 获取结果元组直到耗尽。

  这种pull-based模型易于实现阻塞算子（如聚合先积累再输出）和控制流。缺点是函数调用频繁有开销，但简洁和模块化胜出。新潮的向量化执行模型通过批处理多元组降低调用开销，但BusTub未实现向量化。

- **BusTub 执行器类型:** 本讲聚焦常见算子执行器：

  - **顺序扫描执行器 (SeqScanExecutor)**：顺序读取表的所有元组，按谓词过滤后输出。BusTub SeqScan通过 TableHeap 迭代器遍历所有页面/槽来获取元组。注意跳过已被标记删除的元组。
  - **插入/更新/删除执行器**：对应DML操作。InsertExecutor 从子执行器(通常是 ValuesExecutor 提供常量行)获取元组插入表，并更新关联索引。UpdateExecutor/DeleteExecutor 顺序扫描目标表找满足条件元组，修改或删除之。这些执行器需要通过 Catalog 获取表信息和索引列表，以对每个修改更新索引（如插入时在每个索引插入新键）。
  - **索引扫描执行器 (IndexScanExecutor)**：利用索引加速查询。它持有一个Index指针（如B+树索引），支持提供一个 key/predicate 来定位范围，输出匹配的RIDs，然后再从表取元组输出。BusTub优化器会使用IndexScan计划替代 SeqScan + 过滤，如果有可用索引。

  本讲先介绍简单执行器，JOIN和Aggregation执行器放在Lecture 14之后。

- **系统目录 (Catalog)**：执行器需要通过Catalog访问表和索引元数据。Catalog 管理数据库的表信息、列模式、索引信息。BusTub Catalog 提供方法如 `GetTable(table_oid)` 返回 TableMetadata（含 TableHeap, Schema 等），`GetIndex(index_oid)` 返回 IndexMetadata 等。执行计划一般持有需要的 OIDs，执行器通过Catalog解析出具体对象的指针。

- **执行流程示例:** 举例SELECT查询执行过程：

  1. Parser将SQL转AST，Binder解决名字，Optimizer产生日志计划 -> 物理计划 (例如 SeqScanPlanNode)。
  2. 创建对应执行器（SeqScanExecutor），通过plan得到table_oid，从Catalog取TableHeap实例及Schema，构造迭代器。
  3. 调用 SeqScanExecutor::Init() 开启表迭代，从第一个页面开始。
  4. 主循环中调用 Next()：迭代得到下一个有效元组，检测谓词条件，符合则存入输出。
  5. 重复Next直到返回false表示遍历结束。

  Insert/Update类似，只不过Insert从子节点pull元组，Update/Delete对遍历行执行操作。

**讲师重点：**强调**算子可组合性**：每个执行器只需关注自己的功能，可以将输入看作一个抽象迭代器，不必关心其内部实现。比如NestedLoopJoinExecutor实现时，把外表child的每个元组取出，再用内表child执行器循环获取全部匹配即可。这种模块化设计方便拓展新的算子类型，只要遵循接口即可插入执行流程。BusTub架构清晰地展现了这一点。

讲师还介绍了BusTub自带的**交互式 Shell**，可以输入SQL让BusTub解析执行。BusTub Shell 自动创建一些演示表供测试。通过shell运行EXPLAIN可以查看执行计划。这些有助于理解课程项目的实用性。

**阅读材料：**教材第15.1-15.3节介绍了Query Processing概览，第15.7节讲了迭代器模型。BusTub源码中的 `execution/` 目录和 `planner/` 目录也值得阅读理解具体算子的接口和实现关系。对于更深入理解，可以阅读 Volcano论文（1994 Goetz Graefe, Volcano: An Extensible and Parallel Query Evaluation System），了解迭代器模型由来和变种。

## Lecture 14: 查询执行 II – 高级执行器与优化

**核心内容：**讲解连接执行器（嵌套循环、嵌套索引、哈希连接）和排序、限制执行器的实现，以及介绍优化器的作用，包括将逻辑计划转换为更高效的物理计划规则。

- **连接执行器实现:**

  - _NestedLoopJoinExecutor_: 持有左（外）子执行器和右（内）子执行器指针，以及 join predicate。实现中典型做法：外执行器Init后，取每个左元组，通过右执行器重复Init->Next扫描内表全部。BusTub实现中，将左元组存在成员变量`current_left_tuple_`，对每个左元组遍历整个右表，右表迭代结束后，获取下一个左元组再重启右执行器扫描。需要处理连接条件（通过 `predicate_->EvaluateJoin` 判断匹配）和连接类型（inner join只输出匹配的，left join还需输出未匹配左行）。
  - _NestedIndexJoinExecutor_: 代替NestedLoop时内表有索引的情况。执行时外表顺序扫描，对于每个左元组，用连接键在内表索引中查找匹配RID列表，然后直接获取内表元组。效率比NLJ全表扫描好很多。BusTub将其单独实现为 NestedIndexJoinExecutor，使用优化器来替换 NLJ。
  - _HashJoinExecutor_: 包括 Build 阶段和 Probe 阶段。BusTub实现：

    1. Build：遍历左child所有元组，计算 join key，用哈希表存储 (key -> vector<left tuple>)。
    2. Probe：遍历右child，每取一元组算key，在哈希表查找匹配的左元组列表，将所有组合输出。支持多对多匹配以及左外连接（需要记录哪些左tuple未匹配过以输出NULL）。

    实现细节包括：定义 hash key 类型（可能是简单值或组合多个字段，BusTub提供 GetLeftJoinKey/GetRightJoinKey 方法）；处理哈希冲突可用C++容器内部解决（如std::unordered_map）或自定义。同样考虑 outer join 时，需额外结构记录未匹配左记录。

- **排序与 Limit 执行器:**

  - _ExternalMergeSortExecutor_: 实现ORDER BY。当输入数据较大时，需要外部排序。BusTub执行器会预读所有输入元组（潜在占大量内存）然后排序输出。项目4任务要求实现外部归并（以SortPage为载体分页排序）。执行器逻辑为：先 read 所有 child tuple to runs (sorted runs on temp pages) -> merge runs -> output sorted sequence via Next()。因此 SortExecutor 是阻塞的 pipeline breaker。实现重点是SortPage存取与BufferPool交互、Merge算法正确性。
  - _LimitExecutor_: 实现LIMIT X功能，限制输出元组数量。Limit执行器包装一个子执行器，初始化时设计数=0，每次Next调用子执行器Next，直到计数==limit或子耗尽。非常简单，但须注意如果子执行器有Init和Next调用语义要正确对接。

- **优化器 (Optimizer) 简介:**

  - BusTub包含基本的**规则优化**：例如识别可以使用索引的表达式，将SeqScan变IndexScan；识别join条件可用HashJoin替代NLJ。项目3可选实现的**Leaderboard优化**提供了一些规则框架（如predicate pushdown, column pruning）。
  - 课程此处不会深入复杂优化，只强调**优化的重要性**。如上一讲提到的NLJ vs HashJoin选择、索引利用、谓词下推等。讲师或许举例一个查询的优化过程：如WHERE条件下推比先join再filter好；笛卡尔积加filter应改为直接join条件等。

  BusTub优化器实现位于 `optimizer/` 目录，通过模式匹配规则对计划树改写。Lecture 14 Flash Talk嘉宾可能分享工业界查询优化器经验。

**讲师重点：**本讲实现上细节略多，但重点在**算子正确性**与**优化思路**：

- 在连接执行器方面，确保不同 join 类型（inner vs left）输出正确元组集。特别left join需输出未匹配左元组与NULL拼接。
- HashJoin的corner case：无匹配要妥善处理，不要漏掉任何组合。
- Sort执行器的资源管理：要体现外部排序处理方法，而不能天真地一次性读入全部（尽管BusTub比赛任务中可能近似这样做，但希望学生理解外部merge必要性）。

在优化器方面，讲师强调**规则优化 vs 成本优化**区别：BusTub用手工规则，有限但直观；工业级还有基于代价模型的枚举方法。指出**分析执行计划**的技能对调优SQL很重要。

**阅读材料：**教材第15.7节讲了排序和其他算子的实现；第16章讲查询规划与优化。可以阅读 PostgreSQL 查询优化相关文档或 Cascades Framework 论文（优化器规则系统著名实现）。Understanding这些优化有助于在实践中编写高效SQL。

## 项目3：查询执行引擎

*项目目标：*实现 BusTub 的**查询执行引擎**组件，包括各种执行算子和简单的优化器规则。通过本项目，学生将把前两项成果（存储和索引）组合起来，实现对SQL查询的支持，涵盖选择、插入、删除、更新、连接、聚合、排序、限制等功能。

- **Task #1: 基础算子执行器** – 实现对表的顺序扫描和修改:

  - **SeqScanExecutor**：使用 TableHeap 提供的 `TableIterator` 遍历整个表，针对每个元组检查 Plan 中的谓词（如果有），满足则产出转成 `OutputSchema` 格式的 Tuple。注意跳过被标记删除的元组。
  - **InsertExecutor**：从子执行器拉取要插入的 Tuple 列表（没有子则说明插入Plan自带常量值）。对于每个要插入的 Tuple，调用 TableHeap::InsertTuple 写入表页，如果插入成功得到新的RID，然后更新 Catalog 中该表的每个 Index：调用 Index::InsertEntry(index_key, rid)，以便索引包含这条记录。需考虑约束：BusTub不强制唯一约束检查，这里只管插入。最后输出一行表示插入的行数。
  - **DeleteExecutor**：顺序扫描目标表，筛选满足谓词的元组，将它们从表中删除，并从所有相关索引中删除对应键。TableHeap::MarkDelete标记删除（可能延迟真正回收以处理并发事务), Index::DeleteEntry移除索引项。输出删除行数。
  - **UpdateExecutor**：顺序扫描找到满足谓词的元组，对每个元组按Plan给定的新值表达式计算出新 tuple，然后执行“删除旧+插入新”两步：先标记旧元组删除并移除旧索引项，再插入新元组并插入新索引项。这样实现UPDATE。然而这样无法保持同一RID，BusTub简化处理，没有物理就地更新。输出更新行数。

  这些DML执行器体现了DBMS维护索引和数据一致性的逻辑：任何表数据改动需要及时反映到索引，否则索引失效。项目要求保证插入/删除/更新过程中**事务隔离**（Project 4会涉及），此阶段先实现正确功能。

- **Task #2: 聚合和连接执行器** – 实现聚合和JOIN:

  - **AggregationExecutor**：采用**哈希聚合**。Init时遍历child Exec将每个 tuple 分组聚合存入内部 `SimpleAggregationHashTable`；Next每次从哈希表取一组结果。注意实现 aggregator 时 if having 表达式存在，则需在输出前判断是否满足。
  - **NestedLoopJoinExecutor**：嵌套两层迭代。实现可以在Init时调用左child->Init()并获取首左元组, 内层初始化。当内层遍历完毕后获取下一个左tuple并重置内层Exec（再Init）。匹配条件用 Plan 的 Predicate。输出满足条件的拼接tuple（根据OutputSchema组合左+右列）。
  - **NestedIndexJoinExecutor**：外层遍历左表，与NLJ类似；内层不通过执行器遍历，而是每次用索引查右表匹配键。PlanNode 会包含需要的 Index OID 和内表信息。执行器实现为：对每个左tuple，用 predicate 中左右表达式计算 join key（或Plan直接给出 join key列id），构造索引 key，调用 Index->ScanKey 得到匹配RID list。然后对每个RID用 TableHeap->GetTuple 拿右tuple，再组成结果输出。
  - **HashJoinExecutor**：分两阶段。Init时构建哈希表：遍历左child所有 tuple，计算 join key，用 std::unordered_map<join_key, vector<left_tuple>> 存储。Probe阶段Next()从右child拉tuple计算key，查哈希表找到匹配左列表，将每个组合生成输出。为了支持左外连接，需额外在哈希表记录每个左tuple是否匹配过；Probe完后剩余未匹配左tuple也输出（作为NULL填充右列）。BusTub只要求实现Inner和Left两种。

  Join实现的难点主要是正确处理多对多匹配，以及外连接NULL填充。性能上HashJoin是一次性构建哈希表，内存得足够，不考虑溢出。本任务偏重正确功能。

- **Task #3: 排序和限制执行器** – 实现ORDER BY 和 LIMIT:

  - **SortExecutor**：BusTub设计了 ExternalMergeSortExecutor，但实现上可以先偷懒用内存排序然后分页输出。项目要求实现真正的外排序：先将输入分块排序写临时页，再两两归并成一个输出序列。实现在 `ExternalMergeSortExecutor` 类中：主要写 `ExternalMergeSortExecutor::Init()` 完成所有排序工作，将最终排序结果存储可供 Next() 顺序输出。任务提供 SortPlanNode 包含排序列和方向信息。学生需要设计 SortPage 页格式存储 Tuple 列表、以及辅助函数写读 SortPage。归并算法实施时，注意管理缓冲池 page pin/unpin，读完无用页应Delete以免缓冲占满。
  - **LimitExecutor**：易于实现。执行器维护计数，每次Next调用下层Next直到取够N条或下层结束。无特别难点。

- **Task #4: 优化器规则** – 实现基本的优化规则：

  - `SeqScanAsIndexScan`: 找出计划树中 SeqScan + Filter 模式，如果表上某列有索引且 Filter是简单等值，则改为 IndexScan Plan 节点。需要匹配表达式结构和Catalog信息构造等值查询键。
  - `NLJAsHashJoin`: 检测 NestedLoopJoin Plan 条件是否全等值，如果是改为 HashJoin Plan。将连接谓词分解找等式左==右模式，构建 HashJoinPlanNode。

  这些规则需要对 PlanNode 树进行遍历、判断模式然后替换。BusTub提供了 `Rule` 基类和 `Optimizer` 框架可用，学生填充具体逻辑。

**项目难点与调试:**

- **元组和Schema转换:** 执行器接受输入 tuple 的 Schema 与输出 Schema 常不同，需要用 Schema 和 Column 表达式创建新 tuple。例如 SeqScan输出Schema可能是表部分列或不同顺序，必须按输出Schema字段定义调用 tuple.GetValue(input_schema, col_idx) 取值。Join输出Schema是左右Schema拼接子集，也要分别从左tuple和右tuple取值。
- **内存管理:** 执行过程中，tuple和value大量创建，在C++注意生命周期避免悬挂指针。BusTub Tuple内部管理数据，在函数内创建返回时通常拷贝。大数据排序需要关注临时页释放，否则内存泄漏或缓冲耗尽。
- **正确性:** 多算子组合容易出错，如Join里漏考虑NULL情况导致记录错位，或者Agg缺少初始化导致随机值。建议充分利用项目自带测试，包括简单单算子测试和复杂SQL集成测试，逐步定位bug。

*项目总结：*完成Project 3，BusTub已成为一个功能完整的单机关系数据库管理系统：支持SQL基本查询、插入、删除、更新操作；拥有索引加速查询；可以执行连接、聚合等复杂查询；并具有基础的优化改写能力。通过实现各类执行器，学生了解了关系代数算子的实际运行方式和挑战，比如连接的多种实现、聚合如何处理NULL、排序如何借助外存等。把前两个项目的存储和索引模块结合起来，也加深了对数据库系统整体运作的理解。

## Lecture 15: 查询优化 – 连接顺序与代价估计

**核心内容：**介绍数据库**查询优化器**如何基于代价对不同物理计划进行择优，尤其是连接顺序优化。讨论统计信息、代价模型以及动态规划算法用于选择最优连接顺序。

- **优化的重要性:** 前面讲座已举例不同计划代价差异悬殊。本讲系统性说明：给定查询的所有等价执行计划中，优化器尝试找到**最低预估代价**者。以连接查询为重点，因为JOIN顺序组合数爆炸增长，如n表连接有 (2^(n-1)\*(n-1)!) 种笛卡尔次序，必须剪枝选择。

- **统计信息:** 优化器依赖数据的统计信息估计每个算子输出行数（基数）。常用统计包括：

  - 每张表行数（base cardinality）。
  - 每列不同值数 (NDV)、最大/最小值、直方图分布。
  - 列间或多列相关性的统计（一般假设独立近似）。

  有这些可估算过滤谓词选择率，如 `age > 30` 可由直方图估约30岁以上占比，`col = value` 通过1/NDV估计等。BusTub简单起见没有统计模块，但课上会介绍概念。

- **代价模型:** 给每种物理算子赋一个cost formula：

  - SeqScan cost ≈ page数（IO成本）或行数（CPU成本）。
  - IndexScan cost ≈ log(N) + qual selectivity \* N (索引树遍历+取记录数)。
  - Join cost = cost(inner) + cost(outer) + cost to join (如 NLJ: |outer| \* cost(inner)；HashJoin: buildCost + probeCost etc).

  通常IO为主，也考虑CPU操作代价。优化器根据表基数和选择率估算结果基数，以及由此算子cost。Sum各子cost + 算子自身cost即可得计划总成本。

- **连接顺序优化 (Join Order):** 典型优化器用 **动态规划** (DP) 方法：

  - 计算所有单表的最低成本获取方法（SeqScan vs IndexScan选较优）。
  - 计算所有两表连接的最低成本：尝试将两个子集连接，子集的计划取已知最优的，然后加上连接算子成本，比较选择低者。
  - 递归计算k表连接：尝试拆分为子计划(子集大小 m 和 n-m)，取各自最优，再考虑连接成本。
  - 利用剪枝：DP记录每个子集的最小成本，不重复计算。复杂度 O(3^n) 但n一般<=10在查询中。

  讲师或举例3表DP：比如 {A,B,C} 连接：

  - 单表: A(cost1), B(cost2), C(cost3).
  - 两表: best(A⨝B), best(A⨝C), best(B⨝C).
  - 三表: try A⨝(B⨝C) vs B⨝(A⨝C) vs C⨝(A⨝B) => pick cheapest.

  BusTub未实现DP优化器，但理解其流程有助于DB原理掌握。

- **其他优化:** 除连接顺序，还有**谓词下推**（将筛选尽早应用减少中间数据）、**投影下推**（不要传递不必要列）、**子查询改写**（EXISTS转JOIN）、**等价变换**如笛卡尔积+选择转JOIN等。本讲点到为止，深度在高级DB课。

**讲师重点：**通过例子展示优化器决策：

- 例: `SELECT * FROM A JOIN B ON A.x=B.x WHERE A.y=50;`  
  未优化: 先A⨝B后筛选A.y=50 (代价高)；优化: 先A筛选y=50减少A基数，再join B（低成本）。
- 例: 索引 vs 全表扫描: 若谓词选择性很低(取很少记录)，IndexScan代价低；否则SeqScan更好。优化器需比较二者cost选取。
- 连接: 强调**左深树** vs **右深树** vs **bushy树**（有子连接并行），多数系统考虑左深（连续二元连接），因为可简化DP。讲师或强调最左(嵌套loop模式) vs hash join style的差异。

**阅读材料：**教材第16章详述了优化器，包括统计、代价计算和连接顺序动态规划。另外推荐参考 System R 优化器经典论文（Selinger 1979）和 Cascades框架论文, 了解实际优化器实现所用的技术和规则。

## Lecture 16: 事务与并发控制理论

**核心内容：**引入**事务 (Transaction)** 概念和数据库并发控制理论，包括ACID属性、调度串行化和冲突序列、两阶段锁协议(2PL)等。为后续几讲并发控制机制打基础。

- **事务 & ACID:** 事务是用户定义的一系列数据库操作序列，被视为一个逻辑单元。需要满足:

  - **原子性 (Atomicity):** 要么事务的所有操作都完成，要么全部不做。中途失败需回滚撤销已完成部分。
  - **一致性 (Consistency):** 事务从一致数据库状态出发，结束时也必须使数据库保持一致性（满足所有约束）。通常由程序员保证正确性，DBMS负责其他ACID确保不破坏一致性。
  - **隔离性 (Isolation):** 并发执行的事务应该不相互干扰，其效果等同于某个顺序串行执行。
  - **持久性 (Durability):** 已提交事务对数据库的修改必须持久，即使系统故障也不丢失。通过日志和恢复机制保障。

- **调度与可串行化:** 数据库并发调度（Schedule）是多事务操作交织的序列。**可串行化**调度 = 能与某一串行顺序具有相同效果。目的是即使并发执行结果仍如同按某顺序独立执行一样。课程用简单例子说明不正确的交织会破坏一致性：

  - 如T1: read(X), write(X); T2: read(X), write(X)。并发交错不当可能导致丢失更新。
  - **冲突可串行化 (Conflict Serializability)**: 如果一个调度可以通过交换相邻非冲突操作变换为某个串行调度，则称冲突可串行化。判断方法是**依赖图 (precedence graph)**：节点为事务，边表示冲突（如T_i的写X在T_j的读X之前 -> edge Ti->Tj）。如果图无环，则调度冲突可串行化。

- **并发问题类型:** 常见包括**脏读**(读了未提交数据)、**不可重复读**(同一事务两次读到不同数据，因为被他人改了)、**幻读**(范围查询两次返回不同集合，因为插入删除了新行)等。SQL标准定义不同隔离级别如 Read Uncommitted, Read Committed, Repeatable Read, Serializable，对应容忍一定的不一致类型。本课程关注最高级Serializable的实现。

- **锁机制 & 两阶段锁 (2PL):** 主流并发控制基于**锁**:
  - 读操作请求**共享锁 (S锁)**，写操作请求**独占锁 (X锁)**。S锁之间兼容，X锁互斥其他锁。通过锁保证冲突操作不并发。
  - **2PL 协议 (Two-Phase Locking)**: 事务分两个阶段：扩张阶段只获取锁不释放，收缩阶段只释放锁不获取。证明2PL能确保调度串行化。变体包括严格2PL（所有锁在事务提交后才释放，避免中间数据被他人读导致脏读）和严谨2PL（事务终止后才释放锁，强于严格2PL，用于恢复简化）。
  - 2PL可导致死锁（两个事务互相等待对方锁）。需死锁检测（等待图+周期检测）或预防（如获取排序，或超时终止）。
- **乐观并发控制 (OCC):** 另一类理论，不用锁而采取**validate**阶段检查事务是否冲突，若冲突则回滚重试。假设冲突少，通常用在读多写少场景或多版本系统。之后Lecture 18介绍MVCC正是OCC的多版本实现。

**讲师重点：**明确**串行化的严格性**：低于Serializable级别会出现问题，对金融转账等严谨应用必须Serializable。以银行账户例子：T1转账$100 A->B, T2计算总额A+B，串行应不变但并发不正确调度可能短暂看到不一致，总额错误。Serializable隔离保证任何并发结果等价于某顺序，不会错。

讲师也会阐述**锁的意图**：虽然2PL理想但可能限制并发，实际可以有更精细锁策略如多粒度锁/意向锁等。不过本课程Project4用的是**乐观多版本**方案(MVCC)，暂不实现传统锁。

Deadlock例子：T1:X锁A, T2:X锁B，然后T1请求X锁B等待，T2请求X锁A等待 -> 死锁。需检测(Wait-For图)或者超时。Project4里也会处理死锁。

**阅读材料：**教材第18章 (18.1-18.3 两阶段锁, 18.5-18.6 OCC/MVCC)。还有数据库系统概念里的经典例子帮助理解。锁和串行化理论较抽象，需多看例题才能熟练应用。

## Lecture 17: 两阶段锁并发控制 (Strict 2PL)

**核心内容：**深入介绍**严格两阶段锁 (Strict 2PL)** 协议及其在数据库中的实现机制，包括锁管理器、事务状态管理、死锁检测等。这些概念将在Project4中实践实现。

- **严格2PL:** Variation of 2PL where a transaction holds all其**独占锁**直到事务结束(commit/abort)，共享锁可以在读操作完成后释放（为了允许更多并发读）。Strict 2PL 确保了**严格调度**——事务未提交前其写不会被其他事务读，从而避免脏读和不可重复读等问题，很受实用系统欢迎，因为恢复容易：如果事务未提交崩溃，其写锁未释放，其他事务无法读到未提交数据，不存在脏数据扩散。大多数数据库默认隔离级别可视为Strict 2PL或其扩展。

- **锁管理器:** DBMS 通常有全局**锁管理器 (LM)** 模块，维持一个锁表 (resource -> lock info)。每个资源可对应一个锁队列，包含当前持锁事务和等待队列:

  - 提供 API: `LockShared(txn, rid)`, `LockExclusive(txn, rid)`, `Unlock(txn, rid)` 等。
  - 如果请求锁与已有锁兼容（无冲突），直接授予，记录到锁表和事务持锁列表。
  - 如冲突则事务需等待：LM将请求放入等待队列, 阻塞事务直到锁可用（通常通过条件变量sleep/wake实现）。
  - Unlock操作在事务commit/abort时批量释放锁，唤醒等待队列里的下一个兼容锁请求。

  Project4要求实现一个LockManager类，支持S锁/X锁/升级锁, 并行下正确处理并发请求顺序。

- **事务状态:** 事务对象通常有状态：`ACTIVE`, `COMMITTED`, `ABORTED`。事务开始时Active，正常执行结束调用Commit设为Committed, 或遇错误/死锁Abort设为Aborted。LockManager在给予锁或等待时需检查事务状态，若事务已Aborted则不应再授予锁，应该直接取消等待返回失败。
  - Project4 Transaction类带有状态和锁集、隔离级别属性。LockManager需要事务指针以操作这些信息。
- **死锁处理:** 2PL可能死锁。处理方法：

  - **死锁检测**: 建立等待图(节点为事务, edge T_i->T_j 表示 Ti等待Tj锁)，周期性或每次等待时检测环路。若发现死锁，选择一个事务(如最年轻)abort释放锁以打破死锁。
  - **死锁预防**: 如采用“等待-禁用”策略 (Wait-Die / Wound-Wait) 基于事务时间戳决定谁等待谁，违背规则者直接abort，避免环形成。课程大概提原理，但Project4通常实现简单**检测**。

  BusTub LockManager可能采用定期检测线程方式或每次锁请求上检测。实现时graph可用邻接表表示Wait-for关系。

- **意向锁 (Intent locks):** 为支持多粒度锁定（表级、行级并存），使用意向锁 (IS/IX) 表示事务打算获取更细粒度的锁。意向锁协议保证加某粒度X锁前，先在上层获取IX锁，从而防止其他事务同时获取表S锁等。课程提及但Project4不要求多粒度锁，全部按行锁即可，无需实现意向锁。

- **封锁协议 (Locking Protocol):** 介绍**可串行化调度**需要两阶段锁。Strict2PL是常用实现。课程可能提及**保持封锁 (Rigorous)** (更严格所有锁到commit释放)，**可串行化** vs **可恢复** vs **可避免级联**调度区别。Strict2PL提供可避免级联（因为写锁直到commit才释放，没有脏读）。Project4基本采用Strict2PL (写锁推迟释放)。

**讲师重点：**通过示例说明LockManager如何运作：

- 两事务并发操作同一行：T1先X锁行，T2之后请求X锁必须等待。T1 commit释放锁后T2才获锁执行。期间T2阻塞。
- S锁共享：T1 S锁行，T2 S锁行，不冲突都能拿锁。T1欲升级X锁，必须等待T2释放S锁，因为S->X冲突，升级通常需排队到T2完成。

讲师还强调**正确解锁**的重要：若事务在持有X锁未提交就崩溃，不释放锁会阻塞其他，死锁检测应发现这种“等待不存在事务”情形并解锁。数据库一般在事务超时或abort时清理锁。

**阅读材料：**教材第18.1-18.3详述2PL和死锁处理。MySQL/InnoDB锁实现资料可参考以了解实际数据库的锁粒度、死锁日志等。

## Lecture 18: 基于时间戳的并发控制 (OCC & MVCC)

**核心内容：**介绍**基于时间戳的并发控制**方法，包括**乐观并发控制 (OCC)** 和 **多版本并发控制 (MVCC)**。重点放在MVCC如何提供更高并发并避免加锁读、实现**快照隔离**。

- **时间戳排序 (Timestamp ordering):** 给每个事务分配一个全局唯一时间戳（开始时刻），作为其序列顺序标识。调度按照时间戳顺序要求执行：

  - 每个数据项维护两个字段：**最大读时间戳**和**最大写时间戳**。当事务T读X时，如果T的ts < X.max_write_ts说明X已经被一个更晚事务写过，不可给T提供序列一致的视图，应abort T。写X时如果发现X已被更晚的读或写访问过，也冲突abort。通过规则可以确保最终效果按时间戳序列。
  - 这个理论方案简单但abort率可能很高（只要有时间戳乱序冲突就放弃较旧事务）。

- **乐观并发控制 (Optimistic CC, OCC):** 基于这样观点：大多数事务冲突概率低，因此不必上锁等待，让事务乐观执行（读本地复制、缓冲写），在提交前**验证**有无冲突，如果冲突再回滚重试。
  - OCC通常分三个阶段：**读阶段**(transaction read/write to local workspace, writes not visible to others), **验证阶段**(checking no interference with other committing transactions), **写阶段**(apply local writes to DB)。
  - 验证方法：常给事务时间戳End_TS。当T验证时，检查并发事务写集和T读集有无交叠等。基本理论：若T1结束前T2开始，则T2应看不到T1未提交的数据。可以设定：若T要commit，其所有读的对象最新写TS < T.start_ts（没人修改过T期间读过的数据），且写集对象没有被其他还未commit事务读过(避免Write-Write?), 则可commit，否则abort。
  - OCC适合短事务、写写冲突少情况。遇冲突得重做，有成本，但无锁开销，无死锁。许多内存数据库或短事务系统用OCC。
- **多版本并发控制 (MVCC):** 为提高并发，允许数据库维护一个数据项的多个版本，每个版本带上创建的事务ID或时间戳。

  - **快照隔离 (Snapshot Isolation, SI):** 每事务读取时获取数据的一个一致快照（通常是其开始时刻的数据库状态)。这样读操作无需锁定写，只需找 <= snapshot_ts 的最新版本读。这避免读写冲突阻塞，读不会阻碍写，提高性能。
  - MVCC 典型实现：每个记录有**版本链**，每版本存有效时间范围 (start_ts, end_ts)。事务读时以自己的 start_ts 获取恰好在该时间存在的版本。写时创建新版本标记start=txn_id, 旧版本end=txn_id。未提交版本对其他读者不可见。
  - MVCC需要**垃圾回收**旧版本（当不再有活跃事务需要读它们时可以物理删除）。
  - MVCC配合**某种仲裁**解决写写冲突：常采用**First Committer Wins**策略来避免幻影，或在快照隔离基础上加锁或检测预防Phantom问题（因为SI本身不保证可串行化，存在Phantom anomaly，需改进成Serializable SI）。

  Oracle和PostgreSQL默认采用MVCC实现快照隔离，InnoDB亦实现Repeatable Read靠MVCC (并防幻影)。

- **BusTub MVCC (Project4)**: 实现**悲观读锁 + 乐观写**的Snapshot Isolation:

  - 每事务分配一个 **开始时间戳** (read_ts)和**提交时间戳** (commit_ts)。事务读只看早于其start_ts提交的版本。这样保证repeatable read of snapshot。
  - 写操作：BusTub MVCC为每条记录维护版本链 (版本=逻辑Undo记录)，写事务先将当前最新版本拷贝成undo log挂链, 新值写入主记录。
  - 提交时给事务赋递增commit_ts，真正使版本可见；其他并发事务读时通过比较自己start_ts与各版本commit_ts判定可见性。
  - 冲突处理：若两个事务同时写同一条记录，则后写的在验证阶段发现这条记录的最新写TS≥自己的TS，说明冲突，需要abort后者。这相当于First-Writer-Wins机制。

  这些将在Project4实现，保证提供**快照隔离**级别。MVCC优点：读无需锁，高并发读场景下性能卓越。

**讲师重点：**比较2PL vs OCC/MVCC：

- 2PL性能稳定但长读易阻塞写，死锁复杂度。
- OCC冲突少时很高效，否则重试代价高。
- MVCC几乎无读写冲突，但需要版本存储开销，写冲突仍要处理，而且SI不是严格Serializable，需要额外手段。讲师或举Phantom问题例子：T1扫描找满足条件的记录集无a, T2插入a然后commit, T1再扫描幻影a出现破坏RR但在SI下T1看不到T2插入因为T2 commit_ts > T1 start_ts。

**阅读材料：**教材第18.5-18.8节讲述OCC和MVCC。Berkeley的*Architecture of a Database System*里MVCC部分、PostgreSQL MVCC文档、Percona blogs对理解MVCC很有帮助。Project4要求MVCC实现细节，也需结合BusTub spec（Lecture16-19相关内容）仔细阅读。

## Lecture 19: 多版本并发控制 (MVCC) 实现

**核心内容：**详细讨论MVCC在数据库中的具体实现方法，包括**版本链组织**、**版本可见性检查**、**事务回滚**等。以BusTub MVCC为例，说明如何达到快照隔离级别。

- **版本链存储:** MVCC为每个记录保留一个版本链 (Version Chain)。BusTub的设计：
  - 表Heap中仍存当前最新值（对最后提交事务可见），同时每条记录的元数据有一个指向**前序版本链**的指针（PageVersionInfo UndoLink）。这些旧版本存在于事务局部undo log中而不在表Heap。
  - 每个版本 (undo log) 记录被修改前的旧值，以及**创建它的事务ID**和**删除它的事务ID**字段。这些用来判断可见性：如果一个读取事务的start_ts落在某版本的生存区间 (created by <= start_ts < deleted by)，则可见那个版本。
  - 写事务更新一条记录时，拷贝旧值到新undo log，加上自己的txn_id为create, 将旧值链到主记录undo link。然后新值写入主记录，并把主记录的元数据标记上**临时的txn_id**。
- **可见性规则:** 快照隔离下：

  - 事务T读取一个记录：顺着版本链（从最新版本沿undo logs）找到第一个**提交时间戳 <= T.start_ts**的版本。T可见此版本，因为它是在T启动前已经存在的最新有效数据。若遍历到链末说明记录在T开始前不存在，则视为未找到。
  - 对未提交的并发事务写的版本T不可见，因为它的create_ts > T.start_ts（写在T开始后）。因此不会读到别人未提交数据。
  - 对自己事务创建的版本，需要特别处理：BusTub通过**临时时间戳** (txn_id placeholder) 区分提交和未提交。事务在其内部也需能读回自己写（读到尚未提交的新版本）。
  - 删除操作可视为把某版本end_ts=deleting_txn_id。读时如发现某版本end_ts <= T.start_ts说明该版本在T开始前已删除，则跳过继续找更老版本。

  总之MVCC保证一个事务的读操作看到的是在它开始时已提交的所有更新，以及它自身的更新（其实标准SI不包含自身写，但一般实现会返回自己的最新修改）。

- **事务提交/中止:**

  - 提交：分配Commit Timestamp = global txn counter++。然后遍历该事务的写集，对于每个修改的记录:
    - 将该事务的新写版本正式生效：即把记录主存的临时txn_id替换为commit_ts作为其新的版本ID，使之后开始的事务可见此版本。
    - 更新undo logs：给新undo log记录的create_ts赋commit_ts; 给旧版本undo log的delete_ts赋commit_ts（表示旧版本在此事务提交时失效）。
    - 释放记录上的锁（如果有悲观锁，也可以MVCC不用锁）。
  - 中止：将所有该事务做的修改回滚:
    - 对插入的新记录，彻底删除物理删除。
    - 对更新/删除，找到对应undo log里的旧值，恢复到表Heap记录；将undo链修复（丢弃新版本）。
    - 这些需要原子完成，然后标记txn为Aborted。

  BusTub在Project4中通过**Undo日志**+**FrameHeader原子更新**实现回滚和提交。未要求实现日志持久化，Focus在多版本链正确维护。

- **避免不可串行:** SI有**幻读**问题：MVCC本身不防止两个事务分别插入满足对方查询条件的新幻影记录，使结果冲突。解决通常:
  - 串行化SI (SSI)：检测危险结构（如两个事务互相见不到对方写）然后abort其中之一。
  - 或对范围查询也引入锁（next-key locking, PostgreSQL在RR下索引间隙锁避免幻影）。
    BusTub不要求实现Serializable，只提供SI级隔离。幻影在Project4不会考虑。

**讲师重点:** 通过一个版本链图示例解说读写：

- 如X初始版本v1 commit_ts=10；T1(start=15) 更新X产生v2 txn_id(T1), v1 delete_ts=T1；T2(start=12) 读X会见commit_ts<=12的版本= v1（T1更新未提交对T2不可见）；T3(start=20) 读X见最新commit<=20即v2（假设T1 commit_ts=16）；这样各事务都各得其所，没有读不该看的内容。

  讲师也强调**版本维护开销**：每次写增加一个版本，必须妥善GC。Snapshot Isolation下最老活跃事务的start_ts决定可以清理哪些版本。Real system often keep old versions in undo logs or separate space (like MySQL undo segments) and have background vacuum to purge them.

**阅读材料:** 教材第18.7-18.8节讲快照隔离和可串行化SI。PostgreSQL文档“MVCC Implementation”详细描述了其版本链(每行隐藏xmin/xmax), 以及Vacuum。Project4 spec与Lecture内容紧密相关，应结合阅读。

## 项目4：事务与并发控制

*项目目标：*在BusTub中实现**事务管理和并发控制**，采用**乐观多版本并发控制 (MVCC)** 提供**快照隔离**级别。项目分四大任务：

- **Task #1: 时间戳管理** – 为每个事务分配开始时间戳和提交时间戳:

  - BusTub 用一个全局原子 `next_txn_id` 生成唯一事务ID，同时把它也作为start timestamp。事务开始时Transaction对象记录 `txn_id`，也作为其StartTS。Commit时LockManager分配 `txn_id_counter.fetch_add(1)` 作为commit timestamp。
  - TransactionManager::Begin() 创建事务对象并分配start_ts；Commit() 设置commit_ts并调用回调处理版本可见性；Abort() 标记中止并调用回滚逻辑。
  - Implementation: likely use `std::atomic<uint64_t>` increment for global timestamp.

- **Task #2.1: 存储格式** – 定义 UndoLog 记录格式和版本链:

  - BusTub 给每张表的每条记录维护一个 `PageVersionInfo`（有点类似每页有hash表存版本信息，但spec模糊此处）。实现上，可以扩展 TableHeap/TablePage to support storing UndoLink for each tuple.
  - UndoLog结构包含: PrevVersion pointer(串起链), Insert/Delete flag, older value data, prev txn id etc。BusTub将undo log放在Transaction对象的undo集合里。Transaction维护 vector<UndoLog> 修改序列。
  - Insert操作: 生成 UndoLog标记 is_insert=true (for rollback to remove tuple)。Update/Delete: UndoLog记录旧值, RID, prev version pointer=当前记录的头 UndoLink, is_delete indicates if original op was delete (for insert back on abort)。
  - 将 UndoLog 存入 Transaction.workspace. 记录 undone on abort.

- **Task 2.2: 顺序扫描/获取元组** – 修改 SeqScan等执行器读取逻辑:
  - 让读操作遵循MVCC可见性规则：Transaction读取tuple时，要检索版本链返回对当前事务可见的最新版本。
  - BusTub可能简化：TableHeap::GetTuple(txn, rid, out_tuple) 内部根据 txn.start_ts 顺着 UndoLink链找可见版本填充 out_tuple。
  - 需要考虑：若最新主值所属txn未提交 (txn_id in tuple metadata)，若就是当前txn则读取自己写的值，否则不能读，应寻找undo链上上一个版本。Implement checks using TransactionManager info (TransactionManager can track active txn commit status).
  - Inserted vs Deleted: If tuple is marked as deleted by a txn with id <= reader.start_ts, skip it.
- **Task #3: MVCC执行器** – 修改 Insert/Update/Delete执行器:

  - InsertExecutor: 插入新元组：在 TableHeap Insert时，Transaction.txn_id作为临时owner写入tuple元数据 (FrameHeader)。创建 UndoLog for Insert (store RID to remove) push to txn.undo vector。先不make visible (commit sets commit_ts).
  - DeleteExecutor: 删除元组：先TableHeap GetTuple找到当前可见版本; 写前检查该tuple是否已经被其他活跃txn删除 (by checking tuple's txn_id or last writer commit_ts) to avoid double delete conflicts. 创建 UndoLog with old tuple value (for abort insert back) set is_delete=true。MarkDelete: set tuple metadata to owned by txn (like lock) and maybe tombstone flag. Link UndoLog to tuple's UndoLink (point to old older versions).
  - UpdateExecutor: 类似Delete+Insert组合: read old tuple-> UndoLog with old value and is_delete false (meaning it's an update, so on abort we put back old value). Then write new value in place as new tuple data, mark metadata txn.
  - All these must be atomic: Use table page latch or CompareAndSwap to attach undo link + mark txn id to avoid race (spec hints use FrameHeader atomic ops).

  - For each modified tuple also add RID to Transaction.write_set (to update commit/abort).

- **Task #4: 主键索引** – 维护唯一性:

  - If a table has primary key index (metadata has is_primary), then Insert must check index uniqueness:
    - On Insert, do index->ScanKey to see if key already exists with a visible tuple not marked deleted. If yes and not deleted by a txn > current? abort current txn (set transaction state TAINTED as spec). Tainted means it will abort at commit.
    - If key found but that tuple is deleted by an uncommitted txn (which could be aborted later), allow insertion (this is next case of Task 4.2). But initially Task 4.1 no reinsert into deleted slot scenario, probably skip.
  - If conflict, mark this txn state=TAINTED, throw ExecutionException to abort query. Essentially abort current txn.

  - This ensures no two active transactions can commit duplicate primary key. Insert abort logic may need to preserve partial changes (spec says don't clean up aborted inserted tuple).

  _Bonus_ (Task 4.2 optional): handle case if index key exists but points to a deleted tuple by a committed txn (i.e., unique constraint, but deletion freed that key). For SI, if a committed deletion, new insertion with same key is fine (no conflict). If deletion not committed, new insertion should not abort either, but in serializable would conflict. Spec suggests in Task4.2 we consider aborted deletion case.

**项目难点:**

- **Version chain atomic update:** When updating a tuple:
  1. Acquire latch on page or record-level lock.
  2. Retrieve current tuple and UndoLink (older chain).
  3. Create UndoLog, link its `prev_version` to UndoLink, store old tuple data if needed.
  4. Update frame's UndoLink to point to this new UndoLog.
  5. Mark tuple's writer as current txn. Release latch.
     Ensuring no interleaving issues if two transactions try update same record concurrently: likely rely on exclusive lock or atomic CAS:
  - Could use a compare-and-swap on FrameHeader to update UndoLink and txn id in one step if match expected old txn id (like using atomic pair or marking).
  - Or simpler, enforce only one writer allowed at a time via concurrency control (BusTub might rely on index or LockManager for writes - but they choose MVCC route, maybe they still lock exclusive for writing).
- **Transaction dependency:** Must ensure if T1 writes X then T2 reads X concurrently, T2 sees old version. This is via reading logic skipping uncommitted version of X by checking tuple.txn_id != committed (or belonging to other active txn).
- **Deadlock**: though MVCC reduces locks, writing to same item concurrently can cause conflict aborts not deadlocks. However in BusTub, they still use locks for unique index insertion (likely).
- **Testing**: Ensure no anomalies:

  - Dirty read: T1 writes, T2 should not read uncommitted.
  - Non-repeatable: T1 read X at start, T2 commit changes X, T1 re-read gets old version (because snapshot fixed).
  - Phantom: T1 scan with cond, T2 insert new that meets cond commit, T1 re-scan still not see new (because snapshot).
  - Write skew: Two transactions reading each other etc - SI doesn't fix that, but likely no test.
  - Primary key: Insert duplicates concurrently, one should abort.
  - Concurrency: Many threads updates different records, ensure no lock contention stalls too long (should be fine as MVCC writes still need exclusive locks logically but short lived).

  Will likely test with concurrency heavy scenario and ensure correct final DB state.

_项目总结:_ 完成Project4后，BusTub具有完整的事务支持。多个事务可并发执行读写遵循快照隔离，无脏读、不可重复读和幻影（因SI phantom not visible in snapshot). Insert对主键唯一约束进行检查避免双写。通过实现TransactionManager, LockManager, Version chain and Undo logging, 学生深入理解事务隔离保障技术。尤其MVCC相对2PL更复杂的数据结构管理，使学生认识到现实系统权衡：实现复杂度 vs 并发性能。这个项目将前面所有模块串联起来，确保数据库在多用户场景下也工作正确可靠。

## Lecture 20: 数据库恢复 – 日志与重做/撤销

**核心内容：**介绍**数据库恢复**机制，重点是**预写日志(WAL)**、日志记录类型以及**重做/撤销算法** (如 ARIES)。解释如何保证事务持久性和原子性，即使发生系统崩溃也能将数据库恢复到一致状态。

- **WAL(Write-Ahead Logging):** 核心原则：**先写日志，后写数据**。对数据库任何改变，先将变更细节追加记录到日志文件(顺序IO)，并在事务commit之前将日志刷盘；只有日志持久化后，才能将数据页写回磁盘。这样即使宕机，恢复时也能从日志重做已提交事务的修改和撤销未提交事务的痕迹。

  实现上通常:

  - 日志记录包括事务ID、操作类型(插入/更新/删除)、受影响页/行ID、before-image(旧值)、after-image(新值)等。
  - 每事务commit时写入commit记录并force写盘(确保commit之前的所有日志落盘)。数据页本身可延迟写(由缓冲池决定)，因为有日志支撑crash恢复。

- **检查点(Checkpoint):** 为防止恢复时必须重放非常久远的日志，系统周期性做检查点。检查点记录当前已提交事务数据都已刷盘或标记一致状态。典型checkpoint过程:
  - 暂停接收新事务写或标记LSN，刷出缓冲池中所有脏页至磁盘，写一条checkpoint记录到日志，记录此时活跃事务和dirty page表。
  - 以后崩溃恢复可从最近checkpoint后的日志开始，缩短恢复时间。
- **ARIES 恢复算法:** 广泛使用的恢复算法 (IBM ARIES) 分三阶段:

  1. **分析阶段**: 从最后检查点开始扫描日志到尾，重建当时事务表(未提交事务列表)和dirty page表 (哪些页在内存有修改)。找到crash时未提交事务。
  2. **重做阶段**: 从检查点或更早(Dirty page earliest LSN)扫描日志顺序向前，对所有日志执行**重做**操作，恢复数据库到崩溃那一刻的状态。重做包括已提交事务和可能部分完成的事务的修改。通过dirty page LSN和页LSN判断是否需要重做某条日志（如果页已包含更新则跳过）。
  3. **撤销阶段**: 对未提交事务进行**撤销(undo)**，按日志逆序扫描这些事务的修改日志，执行相反操作恢复数据到原值。撤销每一步也会产生补偿日志CLR，以确保撤销操作本身如遇崩溃也可重做撤销（避免重复undo）。最终所有未提交事务撤销完毕并写abort日志。

  ARIES保证即使在恢复过程中再次崩溃也能接着恢复（因为CLR的存在使得undo可重做)。BusTub未要求实现ARIES，但课程讲述其思想。

- **日志记录类型:**

  - Update日志: prev value & new value, page id, offset, txn id, LSN, prev_LSN。
  - Commit日志: txn id, commit LSN (for flush).
  - Abort日志: txn id.
  - CLR (compensation log record): for undo an update, contains info to not repeat undo etc.
  - Begin/End checkpoint: contain active txn list, dirty pages info.

  BusTub likely uses simpler logging, maybe only record enough for redo (since it’s an educational system). But schedule suggests logging is taught for completeness.

- **LSN (Log Sequence Number):** 每条日志的字节偏移编号。页上通常存**page LSN** = last log update that affected this page。用于重做时判断：如果page LSN >= log rec LSN, 表示该update已在页上应用，无需重做，否则要重做。WAL要求: flush log until LSN >= page LSN before page write to disk.

**讲师重点:** 强调**日志优先**的重要性。没有日志或不先写日志，系统无法可靠恢复。举例：

- 没日志: 事务写一半崩溃，数据损坏无记载。
- 先写数据后写日志: 崩溃在日志没写但数据写了，恢复会丢失commit标记，相当于事务应用了却算未commit，不满足原子性。
- 讲检查点时比较：没有checkpoint, 可能重做需要从头(非常慢)，有checkpoint快很多，但实现复杂些。

也描述**恢复场景**：

- 正常宕机: committed should persist (via redo if needed), uncommitted undone。
- 媒介故障: 需要从备份+日志重放整个库(不太讲细节)。

Project4 可能没有实现WAL, 但课程要涵盖以完成ACID最后Durability部分理论。BusTub logging would be heavy if implemented, maybe optional. Possibly they have a project extension for WAL.

**阅读材料:** 教材第19章 (19.1-19.9)覆盖WAL和ARIES。ARIES original paper (Mohan 1992) for deeper understanding. Oracle, Postgres recovery manuals also illustrate how logging is implemented.

## Lecture 21: 数据库恢复 – 崩溃恢复过程

**核心内容：**继续讨论数据库恢复，特别关注ARIES恢复三个阶段的细节和例子，确保理解重做和撤销逻辑。可能通过一个简单日志序列演示从崩溃恢复数据库一致性。

- **分析阶段**: 从检查点开始:
  - 读取Checkpoint log which lists active transactions (with last LSN) and dirty pages (with recLSN = first LSN that made page dirty) at checkpoint。
  - 然后从checkpoint位置顺序扫描日志，更新active txn table:
    - 对每条Begin, Update, Commit, Abort记录更新事务状态(Begin add txn, Update keep lastLSN, Commit mark complete, Abort mark aborting)。
    - Update records: if page not in DirtyPageTable, add it with recLSN = LSN of this update.
    - End of log: Active txn table now has all uncommitted transactions at crash; DirtyPageTable has all dirty pages at crash.
- **重做阶段**: 找重做起点 = min(recLSN in DirtyPageTable) (earliest page dirtied that might not on disk).
  - 从此LSN遍历日志:
    - For each update or CLR record, check DirtyPageTable for page:
      - if page not dirty or recLSN > record LSN skip (shouldn't happen since started at min recLSN).
      - Read page from disk (if not in buffer) and check page.LSN (stored on disk page header).
      - If page.LSN < log record LSN, meaning this update not applied yet, **reapply** (redo) the update: rewrite the after-image to page, set page.LSN = record LSN.
      - Otherwise skip (update already on disk).
    - For commit/abort records maybe no action besides knowing tid? (commit mark txn complete but already done in analysis).
    - Continue to end of log. After this, database state equals what it was at crash moment for all pages reachable from log.
- **撤销阶段**: Use the Active transaction list from analysis (these are uncommitted at crash):

  - For each such txn, find last LSN of each (from txn table). Put these LSNs in a priority queue (max-heap for largest LSN first).
  - While queue not empty:
    - Pop top LSN (largest). Suppose it belongs to Txn T.
    - Read the log record at that LSN (should be update or CLR because commit/abort wouldn't be in active list).
    - If record is not CLR (i.e., original update of T), then undo it:
      - Apply inverse of the update to the page (write before-image).
      - Write a CLR record to log (with info that this undoes that LSN, plus maybe next_undo = prevLSN of undone record).
      - Set page.LSN = CLR's LSN after writing.
    - If record is CLR (meaning an undo of a later update, which implies nested abort scenario or partial undo), still apply its before-image if not applied and consider its next_undo pointer.
    - Whether original or CLR, next to undo: push the prevLSN (for original update) or CLR.nextUndo (for CLR) of this record to heap for further undo.
    - Mark in txn table that LSN undone etc.
    - When a transaction's last to-undo LSN becomes null (no more updates to undo), write an "end txn" log and remove txn from active list.
  - Continue until no more to undo. All uncommitted changes undone.

  The algorithm ensures all actions undone exactly once. CLRs ensure idempotence.

- **并发恢复**: Actually, ARIES can redo and undo multiple tx concurrently by intermixing logs as above. It's efficient and ensures minimal reprocessing.

**讲师重点:**

- 解释**CLR**目的： e.g., if crash during undo, the CLR tells recovery to redo the undo so it won't reapply original update again. Without CLR, recovery might re-undo something twice erroneously.
- Possibly a step-by-step sample:
  - T1 start, update page1, update page2, commit; T2 start, update page2, crash before commit.
  - Show log: T1 updates, commit; T2 update; crash.
  - Checkpoint earlier maybe with T1 active.
  - Recovery:
    - Analysis finds T2 active, page2 dirty (maybe recLSN = T1 first update or T2's? whichever first dirtied).
    - Redo from that: T## Lecture 22: 分布式数据库导论

**核心内容：**概述**分布式数据库**系统的基础，包括数据分片、复制、CAP理论、分布式事务的挑战（两阶段提交）、以及分布式查询处理基本原理。

- **数据分片(Sharding):** 将数据库拆分成多个分区存储在不同节点以扩展容量和并发。分片方式：
  - **水平分片:** 按行划分，每个节点存部分表行；**垂直分片:** 按列划分，不常用在关系型。
  - Shard key选择影响数据分布和查询性能。理想均匀分布且与查询模式匹配。哈希分片提供均衡，但跨分片聚合困难；范围分片按值范围划分，易于范围查询但可能热点集中。
- **数据复制(Replication):** 在多个节点保存同一份数据，提高可用性和读性能。典型架构：
  - **主从复制(Master-Slave):** 一个主节点处理写，其它从节点异步复制日志应用，供读请求。优点实现简单，但存在复制延迟导致读到旧数据。
  - **多主复制(Multi-Master):** 多节点都可写，需要冲突解决和全序一致性协议，复杂度高。
  - **一致性级别:** 强一致性(线性化) vs 最终一致性(异步复制)。根据CAP理论（Consistency, Availability, Partition tolerance，只能三选二），分布式系统往往在网络分区情况下牺牲强一致换可用性。
- **CAP理论:** 在分布式系统中：
  - **C(Consistency):** 所有节点数据一致(相当于强一致)。
  - **A(Availability):** 系统一直可响应请求（即使部分节点故障）。
  - **P(Partition tolerance):** 系统在网络分区情况下仍能运行。
  - CAP定理：在网络分区发生时，不可能同时保证C和A。如强一致系统(如关系DB)在Partition发生通常停止对外服务(牺牲A)；而高可用系统(如NoSQL)选择最终一致(牺牲C)。
- **分布式事务:** 跨多个节点数据的事务需确保ACID，采用**两阶段提交(2PC)**协议:
  1. **准备阶段(Phase 1):** 协调者要求所有相关分片节点准备提交事务（各分片执行本地事务并写WAL，锁定资源，回复“Yes”或“No”）。
  2. **提交阶段(Phase 2):** 如果所有节点都答应准备，协调者广播提交（各节点commit本地事务）；若有任何拒绝或超时，无条件广播回滚。
  - 2PC确保原子性但存在问题：协调者若在phase2前崩溃，资源可能被锁直到恢复(阻塞)。引出3PC改进，但仍牺牲一致性时间窗。
  - 分布式事务的性能开销很大，分布式系统有时避免它而使用补偿事务、最终一致策略。
- **分布式查询处理:** 涉及将SQL查询划分子查询在各分片执行，再汇总:
  - **数据本地化**: 优先将操作推到数据所在节点(如各分片局部聚合再汇总)。
  - **分布式join**: 如果连接键分片对齐(同键在同节点)可局部join，否则需要数据重分布(如散列分片到join键)在网络发送消耗高。
  - **开销模型**: 包括网络传输代价。优化器需要决定如何ship数据。

**讲师重点：**指出单机DB扩展有限，分布式可水平扩展但复杂度增加。通过NoSQL vs NewSQL对比:

- NoSQL系统 (如 Cassandra) 注重AP，弱一致，简单key-value模型，省去了分布式事务和JOIN以换高扩展；
- NewSQL (如 Google Spanner) 尝试提供类似SQL强一致性但通过时间戳和共识算法(Raft/Paxos)达到CP，牺牲部分性能。

2PC示例: T写Shard1和Shard2:

- prepare: both shards do log prepare, ack.
- commit: coordinator sees both ack, logs commit, send commit.
- If Shard2 respond no (maybe uniqueness conflict): coordinator send abort to all (Shard1 abort).

如果Coord崩在commit前:

- Shards uncertain, remain locked until coordinator recovers and decides commit or abort (hence blocking).

**阅读材料:** 教材第20.4-20.5讲分布式DB概念, 23.1-23.4讲并行DB (some similar to distributed)。PACELC theorem (beyond CAP) could be mentioned: trade-off under Partition and normal conditions. Also two-phase commit in textbook 18.9 maybe.

## Lecture 23: 分布式 OLTP 系统案例

**核心内容：**介绍现代**分布式OLTP**数据库系统架构，如Google Spanner、CockroachDB等，说明如何在全球分布环境提供一致事务处理。涉及共识协议（Paxos/Raft）、TrueTime等技术。

- **Google Spanner:** 全球分布式数据库，提供外部一致性事务和SQL查询:
  - 使用**同步时钟** (TrueTime API) 给事务分配精确时间戳区间，实现Serializable snapshot isolation（读在start_ts, 写占用时间区间末保证线性化)。
  - 数据按**目录**分片，使用**Paxos**复制每个分片跨数据中心保证强一致复制。写事务用2PC配合Paxos提交（称2 phase commit on Paxos).
  - 支持**读写事务** (两阶段锁, writes uses commit wait for TT), **只读事务**(读快照,不用锁,TT bounds).
  - 可水平扩展, Google用Spanner支撑AdWords、F1。
  - 性能关键在于TrueTime容忍漂移(比如等待7ms确保全局序)和网络延迟，对跨洋事务有一定延迟成本。
- **CockroachDB:** 开源NewSQL，模仿Spanner:
  - 数据按范围分片, 每分片3副本(Raft一致性复制)。
  - 使用**Hybrid Logical Clocks** (HLC)代替TrueTime, 结合物理+逻辑时钟减少依赖GPS/AtomicClock。
  - 实现Serializable isolation：事务用**timestamp cache** & **write intents** plus **read refresh** to avoid anomalies, commit via 2PC.
  - On commit, leader lease ensures linearizable order, finalize commit TS possibly bumped if conflicts.
  - Achieves high scalability and strong consistency though maybe not as globally consistent as Spanner's external consistency.
- **FaunaDB / Calvin:** Calvin uses deterministic ordering at a sequencer to avoid 2PC overhead (predeclare read/write sets then all nodes apply in same order). Others use mixing OCC + replication.

- **Consistent Hashing / Dynamo:** (Though more AP oriented, maybe skip).
- **Key points:** Distributed commit latency often dominated by cross-node round trips. Many systems prefer to minimize distributed tx by co-locating related data (use same shard for related records). If cannot, then use advanced concurrency control or consensus protocols to maintain correctness. E.g., Spanner's commit wait (~7ms) to allow uncertainty to pass.

**讲师重点:** possibly outlines Spanner architecture:

- TrueTime: TT.now() returns [earliest, latest] actual time, guaranteeing error bound.
- Writes in Spanner: prepare on shards, assign commit timestamp = TT.now().latest, buffers changes until TT.after(commit_ts) to ensure no one in future could have earlier timestamp (external consistency).
- Use of Paxos: each shard replicates log, leader picks commit timestamp.

Maybe share some performance trade-off: strong consistency vs throughput. Spanner requires heavy infra (atomic clocks etc.), so not trivial to adopt widely.

**阅读材料:** Spanner paper (CACM 2013) for TrueTime etc, CockroachDB architecture blogs, Calvin paper (SIGMOD 2012). Lecture likely summarises without depth.

## Lecture 24: 分布式 OLAP 系统案例

**核心内容:** 介绍**分布式OLAP**数据库系统，如MPP数据库和新兴云数据仓库架构（Snowflake, Amazon Redshift等）。重点在于批处理查询的并行执行、列存储和分布式文件系统的运用。

- **MPP架构:** 大规模并行处理数据库，如 Greenplum, Teradata:

  - 数据在多个节点按分片存储(通常按哈希键)，每节点运行一个DB实例。协调节点接收查询并分发子查询到各节点执行，然后汇总结果。
  - 使用**分布式执行计划**：如一条SQL join，优化器决定数据是否需要重分布(reshuffle)或本地join:
    - If join keys aligned with distribution key, can join locally.
    - Otherwise, broadcast smaller table to all nodes or repartition both on join key。
  - OLAP查询常扫描大量数据，MPP通常用列存储+压缩加快IO，每节点高度优化向量化执行。
  - Fault tolerance: MPP传统不太容错(一个节点挂整个查询失败)，现代系统可能以交换操作(类似MapReduce pipeline)重试阶段。

- **Spark SQL / Presto:** 计算框架处理SQL:

  - 基于**内存/磁盘**分布式处理(MapReduce-like, no persistent index). Queries compiled to DAG of tasks (shuffle, map partitions), data mostly in columnar format (Parquet/ORC).
  - Good for ad-hoc analysis on data lake, scale to thousands of nodes, but interactive latency higher vs MPP DB due to overhead.

- **Cloud Data Warehouse (Snowflake, BigQuery):** Separate storage/compute:
  - Data stored in cloud storage (S3, etc) columnar format, compute clusters (virtual warehouses) spun up on demand to process queries.
  - Heavy use of caching, micro-partitions (Snowflake).
  - BigQuery uses Dremel execution (tree of aggregators).
  - These systems trade some performance per node for elasticity, but heavily parallel to achieve good performance.
- **New OLAP optimizations:**
  - **Data skipping/indexing:** Min-max per block or zone maps to skip reading blocks that don't qualify.
  - **Join optimization:** Broadcast join if one side small; partitioned join if both large; multi-stage join.
  - **Vectorized execution:** operate on batch of values at a time for CPU efficiency (used in e.g., Snowflake).
  - **Materialized views** and result cache to accelerate repeated queries.

**讲师重点:**

- Possibly mention **Relational AI** or other new tech: maybe focusing on how RA uses knowledge representation (just guessing from Relational AI mention in schedule).
- Emphasize difference from OLTP: OLAP systems prefer eventual consistency or batch updates, denormalize data for performance, no heavy transaction concurrency.

**阅读材料:** Chapter 20.7 and 22.9 covers distributed and parallel query processing. Snowflake whitepaper for architecture, "Dremel: interactive analysis of web-scale datasets" (BigQuery's base), "The design of the PostgresXL" etc.

## Lecture 25: 课程总结与前沿发展

**核心内容:** 总结课程所学数据库系统内部关键技术，回顾整个架构并展望数据库未来发展趋势。

- **课程回顾:**
  - 数据库体系架构：查询层 -> 执行层 -> 存储引擎 -> 硬件。SQL解析优化执行的整个生命周期。
  - 重要概念：关系模型与SQL、索引(B+树,哈希)、查询优化(代价模型,连接算法)、事务ACID(2PL vs MVCC)、恢复WAL等。
  - 项目汇总：BusTub各模块(BufferPool, Index, Execution, Concurrency)如何组合形成一个功能DBMS。强调动手实现让理论更清晰。
- **前沿和研究方向:**
  - **自驱动数据库**: 自动调参、自适应优化（基于机器学习调整索引选择、缓存策略等）。
  - **HTAP**: 融合OLTP与OLAP数据库 (Hybrid Transaction/Analytical Processing) 能同时支持事务和分析，例如 TiDB, MemSQL.
  - **New Hardware**: 新型存储(SSD, NVM, PMem) 和处理器(GPU, FPGA)对数据库的影响。In-memory DB (HANA)充分利用大内存, indexing和logging都需新设计。
  - **Serverless & Cloud**: 分离存储计算, 自动弹性伸缩, 多租户优化(eg Aurora with log as DB, Spanner on cloud).
  - **Blockchain & ledger DB**: append-only, crypto verification in DB.
  - **Graph/NoSQL**: 特殊数据模型数据库演进，但 SQL/relational仍蓬勃 (with JSON support etc).
- **工程能力:** 希望学生掌握阅读和修改开源数据库代码的基础，理解数据库的复杂性以更有效使用或调优数据库。

- **就业前景:** 数据库领域企业(Oracle, Microsoft, PingCAP, Cockroach Labs, Snowflake etc)在做什么；即使不做内核开发，这门课知识在应用调优、数据架构设计中价值极大。

**讲师重点:** 强调**综合大图**：数据库系统串起存储、操作系统、算法、分布式等多方面知识，是计算机领域交叉学科的结晶。本课程给予概览和实战，后续可深入某专项比如查询优化研究、存储引擎研发等等。

最后可能以趣味问题结尾，如：“SQL or NoSQL?” 答：各有场景，不是此消彼长，而是互补。 Or "The database field is 50+ years old but continually evolving – it’s an exciting time with new systems blending ideas (like NewSQL, HTAP)."

讲师致谢TA和引用社区，并鼓励学生探索高级课程(15-721)或相关研究机会。

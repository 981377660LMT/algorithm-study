1. 使用 Gemini 3 Pro 的 DeepResearch 模式调研大模型的发展脉络，并输出为文档；
2. 结合第一步调查文档与提前制定好的大纲，使用 Gemini 3 Pro 的 Canvas 模式，生成文档的初稿；
3. 逐步 review 文档中的每一 part，需要调整的部分使用 Gemini 3 Pro 的思考模式做补充，再使用 Canvas 模式结合补充让其直接调整文档；
4. 需要绘图解释的部分使用 NanoBanana 2 Pro 绘制；

## 大模型基本概念与发展脉络

1. 如果说传统的判别式 AI（Discriminative AI）是教计算机“像人类一样分类”——识别图像中的物体、判断文本的情感倾向；那么以大语言模型（Large Language Models, LLMs）为代表的生成式 AI（Generative AI）则是在教计算机“像人类一样创造与思考” 。

2. 我们利用 Generative AI 技术构建了 Foundation Models（包括 LLM 和 LVM），最终实现了 AIGC 的生产方式。
3. 大模型区别于常规模型的核心特征：涌现能力 (Emergent Abilities)。
   为什么要把模型做得这么大？因为量变会引起质变。当模型参数规模超过临界值（通常在 100 亿-600 亿参数之间）时，模型会突然获得小模型完全不具备的能力，这就是涌现。

   以下是常规模型与大模型的对比表格：

   | 维度           | 常规模型 (Traditional Models)                                | 大模型 (Large Models)                                                             |
   | :------------- | :----------------------------------------------------------- | :-------------------------------------------------------------------------------- |
   | **参数规模**   | 100M - 500M (百万级)<br>例如：BERT-base, ResNet-50           | 7B - 1T+ (十亿至万亿级)<br>例如：GPT-4, Llama 3-70B, DeepSeek-V3                  |
   | **训练方式**   | 监督学习 (Supervised Learning) 为主                          | 自监督预训练 (Self-supervised Pre-training) + 微调                                |
   | **任务适应性** | 专用 (Specific)，每种任务需单独训练                          | 通用 (General)，单一模型通过 Prompt 适应多种任务                                  |
   | **推理能力**   | 模式识别 (Pattern Recognition)                               | 逻辑推理、思维链 (Chain-of-Thought)                                               |
   | **规模效应**   | **边际效应递减**：参数增加到一定程度后，性能提升停滞（饱和） | **缩放定律 (Scaling Laws)**：参数越过阈值后，能力突然“涌现”，且尚未看到明显天花板 |
   | **泛化能力**   | **弱**：训练集与测试集分布必须一致，遇到未见过的领域表现极差 | **极强**：具备“举一反三”的能力，能处理未见过的任务类型 (Zero-shot)                |
   | **算力需求**   | 单卡或少量 GPU                                               | 庞大的 GPU 集群 (训练)，多卡/量化部署 (推理)                                      |

## 大模型架构与核心技术

1. Embedding：语义的向量空间映射
   以下是 One-Hot 与 Embedding 的对比表格：

   | 维度         | One-Hot (独热编码)     | Embedding (词嵌入)               |
   | :----------- | :--------------------- | :------------------------------- |
   | **数据形态** | 稀疏 (Sparse)          | 稠密 (Dense)                     |
   | **维度**     | 等于词表大小 (如 100k) | 固定维度 (如 512, 4096)          |
   | **语义表达** | 无。词与词孤立，正交。 | 有。语义相近的词，坐标距离更近。 |
   | **典型代表** | 传统搜索关键词匹配     | Word2Vec, BERT, LLM Embeddings   |

   Embedding 最强大之处在于它构建了一个语义几何空间。词与词之间的几何距离直接对应了它们在语义上的相似度。
   当我们搜索文档时，我们不是在匹配关键词，而是在语义空间中寻找距离最近的向量，从而实现“意图搜索”。

   NLP (自然语言处理) 的核心挑战是如何处理序列数据。Embedding 解决了“词”的表示，但“句子”的理解经历了漫长的演进。
   以下是 NLP 架构演进阶段的对比表格：

   | 阶段                 | 核心架构    | 处理方式                    | 视野范围 (Context)     | 缺点                                               |
   | :------------------- | :---------- | :-------------------------- | :--------------------- | :------------------------------------------------- |
   | **早期 (2013 前)**   | N-Gram      | 统计共现概率                | 极窄 (只看前后 N 个词) | 无法理解长句子，毫无逻辑可言。                     |
   | **中期 (2013-2017)** | RNN / LSTM  | 串行 (逐词处理)             | 较长 (依赖隐状态传递)  | 无法并行 (训练极慢)；长距离遗忘 (读了后面忘前面)。 |
   | **爆发 (2017 后)**   | Transformer | 并行 + Attention (上帝视角) | 全局 (一次性看完全文)  | 对显存要求高 (但解决了并行和长记忆问题)。          |

   在 Transformer 出现之前，RNN 和 LSTM 是处理序列数据的主流。这些模型像人类阅读一样，必须按顺序一个词一个词地读取输入。这种串行机制存在两个致命缺陷：一是无法利用 GPU 进行并行计算，导致训练效率极低；二是“长距离依赖”问题，即读到文章末尾时，往往已经遗忘了开头的关键信息（梯度消失问题）。

2. Transformer 的词向量生成
   用户的输入会先经过 Transformer 的 Embedding 结构，形成 Encoder 结构使用的词向量。

   - Tokenization (分词/切分)、 Indexing（查字典）、Embedding (嵌入) —— “查表”
   - 输入序列变成了一个 矩阵 (Matrix)。 形状是：[Token 数量, 向量维度] = [5, 4096]。
   - 注入位置信息“位置编码 (RoPE)”

   做完这一步，这个矩阵(语义+位置)才真正准备好进入 Transformer 的 Encoder 层（Transformer Block）去进行 Attention 计算。

3. Transformer —— Encoder （编码器）
   Encoder 的工作是一次性把整句话读完，并把文字转化成“意思”（即“语义”）。在这个阶段，Transformer 引入了最重要的 自注意力（Self-Attention） 机制。

   - Self-Attention（自注意力）：这是 Encoder 内部发生的最关键步骤。模型会计算每个词与其他词的关联。
   - Encoder 处理后，输出语义矩阵(KV 矩阵)
     Encoder 处理完输入序列（比如 5 个词）后，输出的是一个 [5, 512] 的矩阵（假设维度是 512）。这个矩阵会被复制两份（或者经过两个不同的线性变换），分别变成 Matrix K 和 Matrix V。
     - K (Key) = 档案的标签/`索引`（用来匹配查询）。
       [名词属性, 政治属性, 核心实体, ...] 它的任务是让 Decoder 的注意力机制能找到它。
     - V (Value) = 档案的具体`内容`（用来提取信息）。
       [与拜登关联强, 指代人名, 位于句末, ...] 它的任务是当被选中后，把这些信息贡献给 Decoder 用来生成下一个字。
   - 多头注意力与多层堆叠
     > 为什么 Transformer 输出 KV 矩阵需要搞这么多个 Encoder，一样的结构需要串行排布 6 个？还有之前经常听到的“多头注意力”又是怎么回事？
     - 多头（Multi-Head） 决定了观察的广度，多层堆叠（Stacking Layers） 决定了理解的深度。
       多头（比如 8 个头）意味着有 8 个不同的专家同时在看这句话，他们关注点完全不同
       Transformer 通常会堆叠 6 层、12 层甚至更多（GPT-3 有 96 层）。 为什么不能只有一层？因为一层只能处理浅层的关系，多层才能实现抽象和推理。

4. Transformer —— Decoder（解码器）

   Transformer Decoder（解码器）是生成式 AI 的“输出工厂”，负责将 Encoder 提炼的语义信息转化为最终的文本序列。

   - 核心任务：自回归生成 (Autoregressive)
     Decoder 采用逐字生成的模式。每一步预测出的 Token 都会被加入到输入序列中，作为下一步预测的上下文。

   - 两大关键注意力机制

     - **Masked Self-Attention（带掩码的自注意力）：**
       - **目的：** 确保生成顺序。在训练阶段，通过“掩码”遮住当前位置之后的 Token，防止模型“偷看答案”。
       - **效果：** 模型只能根据已生成的“历史”来预测未来，维持了因果逻辑。
     - **Cross-Attention（交叉注意力）：**
       - **目的：** 知识对齐。Decoder 产生查询（Query），去 Encoder 提供的 KV 矩阵（知识库）中检索相关信息。
       - **效果：** 确保生成的回答（如“拜登”）与输入的问题（如“美国总统”）在语义上高度匹配。

   - 运行流程（以“美国总统是谁”为例）

   1. **接收信号：** 接收 Encoder 的 KV 矩阵和起始符 `<Start>`。
   2. **内部检索：** 通过 Cross-Attention 锁定原文中的“美国”、“总统”等关键词。
   3. **概率预测：** 经过线性层和 Softmax 计算，选出概率最高的字（如“拜”）。
   4. **循环迭代：** 将“拜”作为新输入，重复上述过程，直到输出结束符 `<End>`。

   - 架构演进：Decoder-only
     现代大模型（如 GPT 系列、Llama、DeepSeek）多采用 **Decoder-only** 架构：

     - **取消独立 Encoder：** 不再区分编码和解码阶段。
     - **统一处理：** 将输入问题和输出回答看作一个整体序列，仅通过 Masked Self-Attention 实现理解与生成的融合。
     - **优势：** 结构更简单，更易于规模化（Scaling）训练。

5. 现代 LLM 核心技术
   如果说 Transformer 解决的是大模型的可行性问题（让模型能并行训练），那么以下技术则解决了大模型的工业化问题（推理加速、长文本支持、成本控制）。

   - **KV Cache：推理加速的“笔记术”**
     KV Cache 是大模型在推理（Inference）阶段最核心的加速手段。大模型生成文本是“自回归”的（逐字蹦出），如果没有缓存，每生成一个新字，模型都需要重新计算前面所有字的 KV 矩阵，算力浪费随长度呈平方级增长。

     - **原理：** 将推理过程中已计算好的 K (Key) 和 V (Value) 矩阵存入显存，后续步骤直接复用。
     - **代价：** 它是“显存杀手”。显存占用 = $2 \times 序列长度 \times 层数 \times 头数 \times 维度 \times 数据精度$。长文本对话极易导致显存溢出（OOM）。

   - **RoPE (旋转位置编码)：长文本的“指南针”**
     RoPE 是目前 Llama、Qwen、DeepSeek 等主流模型的标配，解决了模型对“相对位置”的理解问题。

     - **核心思想：** 不再像传统模型那样直接“加”位置向量，而是通过“旋转”语义向量的角度来表示位置。
     - **优势：** 无论两个词在句中的绝对位置在哪，只要它们距离固定，旋转后的夹角就固定。这赋予了模型极强的“外推能力”，使其能处理超出训练长度的长文本。

     理解：类比哈希，加的哈希不如乘的哈希稳定

   - **MoE (混合专家架构)：算力的“分诊台”**
     随着参数量走向万亿级，推理成本变得不可接受。MoE 架构（如 GPT-4, DeepSeek-V3）通过“稀疏激活”解决了这一痛点。
     - **结构：** 将模型中的前馈网络（FFN）切分为多个“专家”（Experts），并引入一个“路由”（Router）。
     - **工作流：** 路由根据输入的 Token 决定调用哪 1-2 个专家，其余专家保持“休眠”。
     - **效果：** 拥有万亿参数的“知识容量”，但每次推理只消耗千亿级别的算力，极大降低了推理成本。

   > 本篇中，我们以实际例子串起了 LLM 的核心架构与技术。了解了原理后，接下来我们将探讨一个模型是如何通过训练与微调获得能力的。

## 大模型训练、微调技术

模型不是天生全知全能的，它需要经历从“通识教育”到“专业培训”的漫长过程。一个可用的大模型通常经历：**预训练 -> 后训练 -> 领域微调/RAG**。

### 1. 预训练 (Pre-training)：大模型的通识教育

预训练是成本最高（占 90% 以上算力）的阶段，本质是**无损压缩**互联网上的海量信息。

- **核心任务：** Next Token Prediction（预测下一个词）。
- **产出：** 基座模型 (Base Model)。博学但不会对话，只会根据上文续写。
- **Scaling Law (缩放定律)：** 模型性能与计算量、参数量、数据量呈幂律关系。这意味着智能水平在训练前即可预测。
- **训练成本：** 以 Llama 3-8B 为例，单张 H100 从头训练需约 23 年，而 Meta 利用数万张显卡集群将其压缩至 2 周内。

### 2. 后训练 (Post-Training)：社会化改造

将“书呆子”基座模型转变为能听懂指令、有礼貌的对话模型（Chat/Instruct Model）。

- **SFT (有监督微调)：** 使用“指令-回答”对，教模型学会对话格式和指令遵循。
- **Alignment (对齐)：** 确保模型符合人类价值观（Helpful, Honest, Harmless）。
  - **RLHF：** 基于人类反馈的强化学习，流程复杂但效果精细。
  - **DPO：** 直接偏好优化，无需奖励模型，已成为开源界主流。
- **强化学习 (RL)：** 如 DeepSeek-R1 证明了纯强化学习可以激发模型的思维链（CoT）推理能力。

### 3. 领域微调 (Domain Fine-tuning)

企业在 Chat 模型基础上，利用私有数据进行的“职业培训”。

- **全参数微调 vs. LoRA：**
  | 维度 | Full Fine-tuning (全量) | LoRA (参数高效) |
  | :--- | :--- | :--- |
  | **显存需求** | 极大 (需 4-8 倍模型显存) | 极小 (消费级显卡可跑) |
  | **原理** | 更新所有参数 | 冻结原权重，训练低秩分解矩阵 $A$ 和 $B$ |
  | **灵活性** | 差，模型文件巨大 | 极佳，支持模块化热插拔 |
- **LoRA 公式：** $h = Wx + BAx$。通过训练极小的 $A$ 和 $B$ 矩阵来改变模型行为，显存占用减少 2/3 以上。

### 4. RAG vs. Fine-tuning：补丁与内化

在业务落地时，RAG（检索增强生成）与微调是互补关系。

- **类比：** RAG 是“开卷考试”（查书），Fine-tuning 是“闭卷考试”（背书）。
- **对比选型：**
  | 维度 | RAG (检索增强) | Fine-tuning (微调) |
  | :--- | :--- | :--- |
  | **知识时效** | 实时更新数据库即可 | 静态，需重新训练 |
  | **幻觉控制** | 优，可溯源至原文 | 中，仍可能瞎编 |
  | **适用场景** | 私有知识库、实时新闻 | 特定文风、统一输出格式 |
- **最佳实践：** **RAFT (Retrieval Augmented Fine Tuning)**。通过微调让模型学会“如何利用检索到的文档来回答问题”。

## 大模型性能指标与效果评估

### 1. 生成质量指标

- **Perplexity (困惑度, PPL)：** 衡量模型预测的不确定性。PPL 越低，文本越通顺。
- **静态基准测试：** 如 MMLU (知识)、GSM8K (数学)、HumanEval (代码)。需警惕数据污染。
- **动态评估 (Chatbot Arena)：** 基于 Elo Rating 的人类偏好盲测，是目前最公认的“黄金标准”。

### 2. 推理性能指标

- **TTFT (首字延迟)：** 用户感知的响应速度，受预填充 (Prefill) 阶段影响。
- **TPOT (单字耗时)：** 决定生成流畅度，ITL (字间延迟) 应小于 50ms。
- **吞吐量：** RPS (并发能力) 与 TPS (算力利用率) 的权衡。

| 场景/操作       | 对 TTFT 的影响 | 对 TPOT 的影响 | 原理简述               |
| :-------------- | :------------- | :------------- | :--------------------- |
| 增大 Batch Size | 🔴 变差        | 🟢 变好        | 增加排队但提升并行效率 |
| 限制最大并发数  | 🔴 变差        | 🟢 变好        | 牺牲并发保住单请求体验 |

## LLM 性能提升的深度解析

### 1. 性能提升的多维权衡 (Trade-offs)

“提升性能”是一个多目标优化问题。当我们在谈论提升时，必须明确**场景、指标与代价**。

| 核心目标         | 评估指标    | 主流手段                | 常见权衡/风险                |
| :--------------- | :---------- | :---------------------- | :--------------------------- |
| **提升通用智力** | MMLU, GSM8K | 数据清洗、Scaling Law   | 训练成本激增，模型上限受限   |
| **降低推理成本** | $/1M tokens | 量化 (INT4/8), KV Cache | 复杂推理任务性能退化         |
| **提升特定任务** | 任务成功率  | 领域微调 (SFT), RAG     | 泛化能力下降，系统复杂度提升 |
| **提升系统吞吐** | Tokens/s    | 批量调度, 动态序列适配  | 长尾延迟增加，调试难度大     |

### 2. 性能提升的四大支柱

到底在提升什么？我们可以从以下四个维度进行拆解：

- **基础能力 (Intelligence)：** 模型的“智商”。通过预训练优化数据质量与算力配比，确保模型能“听懂话、会思考”。
- **效率与成本 (Efficiency)：** 工业化的基石。通过 FlashAttention、量化、Speculative Decoding 等技术，在保证性能的前提下最小化资源开销。
- **安全性 (Safety)：** 信任的保障。通过 RLHF/DPO 对齐，确保模型行为符合伦理，抵御恶意攻击，规避法律风险。
- **现实任务表现 (Experience)：** 从“聊天”到“办事”。通过强化学习（如 DeepSeek-R1）激发思维链，结合 Agent 框架完成端到端任务。

### 3. 不同阶段的关键因素

| 阶段          | 关键因素                                           |
| :------------ | :------------------------------------------------- |
| **预训练**    | 算力规模、数据多样性/去重、模型架构设计 (如 MoE)   |
| **SFT 微调**  | 高质量指令-回复对、学习率与超参设置                |
| **RLHF 对齐** | 偏好数据质量、奖励模型 (RM) 的稳定性、DPO 算法应用 |
| **推理部署**  | 量化/蒸馏、KV Cache 优化、硬件加速 (GPU/NPU)       |

## 总结与展望

- **AI 时代，好奇心是第一生产力：** 只要能提出好的问题，就能逐渐接近答案。
- **文章的意义：** 巩固学习，争取比 AI 的回答更直击核心，并为 AI 发展贡献优质语料。
- **护城河的转移：** 研发同学的竞争力正从“专能性”转变为“全能性”。在知识壁垒消融的未来，**One-Shot** 和 **Few-Shot** 的快速学习与迁移能力将是核心。

---

大模型在后训练阶段已经使用了微调技术（Fine-Tuning），这里的微调和我们在业务落地中常听说的“微调”是一回事吗？
技术原理上**是一回事**（都是通过有监督学习更新模型参数），但在**执行主体、数据来源和核心目标**上有显著区别。

我们可以将其类比为“社会化教育”与“职业技能培训”的区别：

| 维度         | 后训练中的 SFT (Instruction Tuning)  | 垂直领域微调 (Domain Fine-tuning)            |
| :----------- | :----------------------------------- | :------------------------------------------- |
| **执行者**   | 模型原厂 (如 OpenAI, Meta, DeepSeek) | 应用开发者 / 企业                            |
| **发生阶段** | 模型发布前 (从 Base 到 Chat)         | 模型发布后 (下游业务适配)                    |
| **数据内容** | 通用指令：翻译、写代码、安全拒绝     | 垂类数据：公司私有代码、医疗病历、法律文书   |
| **核心目标** | 让模型**“听懂人话”**，学会对话格式   | 让模型学会**“特定领域知识”**或**“特定文风”** |
| **底座模型** | 通常基于 **Base Model**              | 通常基于 **Chat/Instruct Model**             |

**总结：**

- **后训练 SFT** 是为了把一个只会续写的“书呆子”变成能交流的“正常人”。
- **业务微调** 是为了让这个“正常人”变成某个行业的“专家”。

在实际业务落地中，如果目标是注入海量新知识，通常优先考虑 **RAG**；如果目标是规范输出格式或模仿特定文风，则选择 **LoRA 微调**。

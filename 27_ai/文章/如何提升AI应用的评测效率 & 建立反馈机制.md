# `如何提升AI应用的评测效率 & 建立反馈机制`

- AI 发展进入“下半场”，从“上半场”专注于构建新的模型和训练方法，转为“下半场”评估和基准测试的循环。重点从“解决问题”转向“定义问题”，评估重要性已经追齐训练。

  https://ysymyth.github.io/The-Second-Half/

  下半场的真正范式是：

  - 我们开发新的`评估`设置或任务，追求现实世界的效用
  - 我们用现有算法来`解决`这些新问题（努力提高效用而不是刷榜）

---

1. AI 评测背景与挑战：大模型迭代未达通用智能预期，AI 发展进入评估和基准测试循环的“下半场”，评估重要性追齐训练；Agent 从“能用”到“好用”面临挑战，上线后需常态化质量追踪。
2. 业界评测框架与产品：评估方法有基准测试、A/B 测试等；AgentBench 等开源评测框架各有特点和适用场景；Coze 等评测工具各有核心功能和适配场景，但存在生态绑定等接入痛点。
3. 抖店 AI 评测挑战：抖店 AI 业务复杂，不同业务形态评测诉求不同，意图路由判断复杂且标准随技能更迭变化。
4. 抖店 AI 评测场景：分为通用组件、Embedding 模式、Copilot 助手模式、Agent 代理模式，各有评测关注维度。
5. 抖店 AI 评测体系实践：包括离线、在线、循环三种评估阶段；反馈机制结合主动式上下文工程；Tracing 注重过程追踪；Dataset 实现自动生产；Executor 提供智能推荐；Process 并入 CICD；Analyze 进行深度分析。
6. 评估方式与规则沉淀：评估方式有代码评估、大模型评估等；核心逻辑是沉淀通用评估规则，提供低门槛自定义评估器构建方式。
7. 反馈工程化：基于 Trace 链路数据归因问题，调用 MCP 技能给出优化建议；反思 Agent 生成“作战手册”反馈给上下文工程。

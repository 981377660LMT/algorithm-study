速通大模型「黑话」

https://bytetech.info/articles/7544975617161035815#FK6kdfLtsoYo8IxvGRqcikXLn0e

读完本文，希望能够帮助你：

- 理解大模型从接收用户输入（Prompt）到生成内容（Generation）的完整工作流程。
- 对一些核心的流程（例如分词、反向传播、自回归）有较为具象的认识
- 明确区分预训练（Pre-training）、监督微调（SFT）和强化学习（RLHF）这三大核心训练阶段的不同目标与方法，理解一个“原始”的基座模型是如何一步步被“调教”成一个有用的对话助手的。

1. 大模型基本概念 1

   1. 模型很大
   2. 大模型的基本单位：token

      1. BPE（Byte Pair Encoding）
         使用最广泛的一种分词方法是 BPE (Byte Pair Encoding)。GPT 系列、DeepSeek、Gemini 等主流模型采用的都是 BPE 或其变体。BPE 最早源于一种数据压缩算法，其核心思想是通过不断合并最高频的相邻符号对，来迭代地构建词表 (vocabulary)。
         BPE 算法的工作原理 ：

         1. 从字符级别开始，每个字符都是一个 token
         2. 统计所有相邻 token 对的出现频率
         3. 将出现频率最高的 token 对合并为新 token
         4. 重复步骤 2-3，直到达到预设的词表大小

2. 大模型工作流程
   1. 大模型的续写机制：自回归生成（自己回归自己，即把上一步的输出，作为下一步的输入）
      从根本上说，大模型就是一个终极的“下一个词”预测器。它生成内容的本质，就是在给定一段已有文本的基础上，一步一步地预测出最有可能紧随其后的下一个 token 是什么。
   2. Transformer 架构与自注意力机制：让模型学会“抓重点”
      Transformer 的核心机制，能捕捉上下文关联（区别于 RNN 和 CNN），并行处理数据（区别于 RNN 的序列处理），从而大幅提升模型的理解和生成能力。
   3. 检索增强生成(RAG)
3. 大模型的规模与架构
   1. Scaling Laws：许多研究表明，模型参数越多，规模越大，算力越高，表现就越好。这一现象被称为扩展定律 (Scaling Law)，也就是常说的“大力出奇迹”。
   2. 稠密模型 vs 稀疏模型
   3. 混合专家模型(MoE)
      Grok、DeepSeek 等都是 MoE 模型。
4. 大模型的训练流程
   预训练、后训练（微调）和 后训练（强化学习）

   预训练：让模型学习基础知识和语言规律的阶段
   基座模型：预训练后得到的通用模型
   监督微调(SFT)：通过标注数据调整模型行为的过程
   强化学习(RL)：让模型学习更符合人类偏好的技术

5. 大模型的部署与优化
   1. 蒸馏模型（模仿大模型行为的小模型）
   2. 量化模型(通过降低参数精度压缩的模型)

https://www.xiaohongshu.com/user/profile/64b94e39000000001403fb79

## 如何判断判断一名垂类工程师是否合格

这篇文章的核心观点是：**对于垂直领域（垂类）的大模型工程师来说，最能体现其能力深度的，不是看他会调多少参数，而是看他是否懂得如何“建立评估体系（Evaluation System）”。**

作为一名普通研发，你可以把它类比为：**不仅要会写代码（训练模型），更要会写单元测试和集成测试（评估模型），而且这个测试用例必须是针对具体业务场景定制的。**

### 1. 为什么“通用标准”不够用？

- **原文提到**：MMLU, GPQA, GSM8K 等。
- **解读**：这些类似于“高考卷”或“托福考试”。它们能测出一个人的通用智力水平，但测不出一个“资深 Java 开发”解决内存泄漏的能力，也测不出一名“法律顾问”撰写合同的能力。
- **研发视角**：就像你不能只看一个服务的 `Ping` 延时（通用指标）来判断电商下单流程是否正常，你需要的是具体的“下单成功率”和“库存扣减准确性”（垂类业务指标）。

### 2. 什么是“垂类评估体系”？

文章强调了三个维度，我将其转化为研发容易理解的概念：

#### 维度一：数据来源（Test Case 的来源）

- **原文**：“主动构建...公开垂类 benchmark...公司以往数据转化...项目转化”。
- **解读**：
  - **硬核做法**：不只是拿开源的数据跑一下，而是把公司过去积累的业务日志、客服问答记录、故障报告清洗出来，做成“测试题”。
  - **例子**：如果你在做医疗大模型，不能只问通过了医生考试没，更要拿真实的医院病历去问模型：“这个病人该开什么药？”，看模型回答的和真实医生开的是否一致。

#### 维度二：评测类型（Test Case 的类型）

- **原文**：“选择题、主观题、偏好数据集、Agent 数据集”。
- **解读**：
  - **选择题**：由于有标准答案，容易自动化测试（单元测试）。
  - **主观题**：类似“写一封道歉信”，没有唯一标准答案，可能需要用另一个大模型（GPT-4）来打分（AI 辅助测试）。
  - **Agent 数据集**：测试模型能不能正确调用工具（API）。比如，“帮我查明天的天气”，模型通过测试的标准是准确生成了 `{ "action": "get_weather", "date": "tomorrow" }` 这样的 JSON。

#### 维度三：工具链（Testing Framework）

- **原文**：“OpenAI Eval 库、OpenCompass 库基础上二次开发”。
- **解读**：
  - 不要重新造轮子，使用现有的测试框架。
  - **二次开发**：就像你用 JUnit 或 PyTest，但需要针对你们公司的微服务架构写一套特定的 Runner 或 Plugin，用来批量跑这成千上万道题，并生成报表。

### 3. 如何判断面试者的水平？

- **初级水平**：只会说“我模型在 MMLU 上跑了 60 分”。（只看通用跑分）。
- **中级水平**：泛泛而谈“准确率”、“召回率”。（懂指标，但不懂业务）。
- **高级水平（文中推崇的）**：“我建立了一个包含 5000 条公司内部真实工单的测试集，分为故障排查、代码生成两类，并开发了一个脚本自动比对模型输出和人工处理结果的相似度。每当有新模型发布，我跑一遍这个测试集，如果分数提升，我就认为新模型对我们业务更有效。”

### 总结

这篇文章告诉我们，在大模型落地应用中，**Evaluation（评估）是核心生产力**。

如果你需要在工作中应对这类场景，可以尝试这样做：

1.  **收集错题本**：把模型在业务中回答错误的问题收集起来。
2.  **定义“好”的标准**：针对这些问题，人工写出或确认一个“标准答案”。
3.  **自动化对比**：写脚本（哪怕只是简单的字符串匹配或用大模型做 judge）来判断新版本模型的回答是否更接近标准答案。

这就是所谓的“建立行业评估体系”。

---

## SFT 成功落地关键：维护小而精数据集

这篇文章和评论区不仅在讲技术，还在讲“工程哲学”。对于非算法背景的研发来说，SFT（Supervised Fine-Tuning，监督微调）通常听起来很玄乎，但这篇文章其实是在帮你**省钱、省时间**。

### 1. 核心概念翻译：SFT 是什么？

- **术语**：SFT（Supervised Fine-Tuning，监督微调）。
- **通俗类比**：
  - **基座模型（Base Model）**：好比一个刚刚大学毕业的高材生，通识能力很强，什么都知道一点，但不懂你们公司的具体业务。
  - **SFT**：好比“入职培训”或“岗前特训”。通过给他看你们公司的《操作手册》和《历史优秀案例》，让他从“通才”变成“专才”。
  - **Prompt 工程（提示词）**：好比每次干活前口头叮嘱他：“注意这条要这么做”。
  - **RAG（检索增强生成）**：好比干活时允许他去查公司的知识库文档。

### 2. 文章核心观点：反常识的“数据量”

通常我们认为 AI 需要海量数据（大数据），但这篇文章强调：**在强基座时代，做垂直业务应用，数据在于精而不在多。**

- **传统误区**：“我要训练模型，给我准备 1 万条数据！”（耗时耗力，甚至可能因为垃圾数据太多把模型教傻了）。
- **文中建议**：“先给我 50 条高质量的，最多几百条就够了。”
  - **只要几百条？**：是的，因为现在的基座模型（如 Claude 3.5, GPT-4o, DeepSeek-V3 等）已经非常聪明了。你只需要通过少量样本告诉它“格式是什么”、“语气是什么”、“思考路径是什么”，它就能举一反三。
  - **场景限制**：前提是**任务明确**、**场景单一**（比如：把非结构化会议记录转成特定格式的 JSON，或者自动化打标签）。如果是开放式聊天（To C），几百条肯定不够。

### 3. 如何搞定这“几百条数据”？（操作手册）

如果你接到任务要微调一个模型，不要蛮干人工标注，作者给出了高效路径：

1.  **二八原则选数据**：不要把所有历史数据都丢进去。找模型最容易做错的那些“难题”（Hard Cases），加上少量简单题。
2.  **AI 标注，人工审核**：
    - 不要自己一条条写答案。
    - 用最强的模型（老师傅，如 GPT-4o）先把题目做一遍。
    - `人工只需要检查老师傅做得对不对。这比从零写答案快得多。`

### 4. 动态维护数据集（非常有价值的工程观点）

- **观点**：**数据集是活的，是有生命周期的。**
- **场景**：
  - 去年你用的基座是 Llama 2，它比较笨，需要你教它很多基础常识，所以你的数据集里有很多简单题。
  - 今年你换了 DeepSeek-V3 或 Qwen 2.5，它天生就懂那些常识了。
- **操作**：这就好比新招的员工学历越来越高，入职培训就可以删掉那些“教大家如何使用电脑”的基础课程，只保留最高阶的业务培训。**即使删除旧数据，模型效果可能反而更好（更精悍）。**

### 总结

作为研发，如果你要在项目里落地大模型微调：

1.  **从小开始**：不要一上来就搞万级数据工程。先搞 50-100 条高质量数据验证效果。
2.  **目标聚焦**：明确 SFT 只是为了让模型适应特定格式或特定业务逻辑，不要指望靠几百条数据给模型注入海量新知识（那得靠 RAG）。
3.  **工具辅助**：用强模型生成数据，人工负责审核（Human-in-the-loop）。

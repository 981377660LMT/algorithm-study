- SpecCoding：Plan -> build，所想即所得
  理想的产品形态：需求对齐、方案对齐、任务拆解、编码任务执行
  每个开发任务只需要想清楚怎么做，通过几轮思路对齐后，就能按照预期快速实现代码
- VideCoding：Chat -> build

---

假定你是面试官，考察候选人一道算法题，此时你已经有了明确的解题思路和标准答案。经过几番对话交流后，候选人理解了你预期的最优思路，按照要求完成了交付。
如果将候选人比作 AI，这就是 SpecCoding，基于已经明确规范的实现过程，期望得到预期的输出。
而如果你是候选人，面对未知的问题，提出自己的方案，通过与面试官的交流验证方案可行性。如果此时面试官可以给你提供最终的代码示例，把面试官当做 AI，这就是 Vibe Coding。你给出模糊的想法或需求，AI 尝试理解并生成代码，你基于直觉判断和调整迭代优化直到满意。

---

- 理解
  - down -> top
  - 架构和模块拆分
- 规划
- 执行
- 评估

---

- 客户端代码生成的挑战：
  - 历史包袱
  - 二方库封装
  - figma mcp 做了裁剪，还原效果不稳定

---

- CE
  - 长期记忆 -> 分块存知识库+mcp 召回
  - 短期记忆
  - 工具能力 -> 脚手架工具、figma-mcp 等
  - Agent 搭建

---

这段内容描述了一个在**超大规模工程项目**中，如何让 **AI 集成开发环境（AIIDE）** 有效工作的完整技术架构方案。
核心矛盾点在于：**巨型工程代码量（TB 级别） vs LLM 有限的上下文（Context Window）及推理能力。**
以下是对这段规划的深入讲解，我们将按**理解规划（输入侧）**和**执行（输出侧）**两个维度进行拆解。

---

### 一、 理解规划阶段：让 AI 看懂庞然大物

这一阶段的目标是解决“AI 无法一次性吃下整个代码库”的问题。核心思路是 **“分治策略”** 和 **“知识压缩”**。

#### 1. 代码理解（解决上下文限制与业务映射）

- **难点**：

  - **上下文溢出**：工程太大，不可能把所有文件塞进 Prompt。
  - **黑盒组件**：内部有很多自研的二方库（Binary Libraries），公共模型（如 GPT-4）没见过这些私有代码，不知道怎么调 API。

- **方案详解**：

  - **仓库标准化治理（AI-friendly Architecture）**：

    - _原理_：物理拆分。如果一个仓库包含所有业务，AI 没法读。将业务按模块拆分成独立的小仓库（Repo），确保每个仓库代码量在 LLM 可处理范围内。
    - _优势_：实现了物理隔离，AI 只需要通过“文件名”就能快速缩小检索范围。

    > SOLID：`职责分离、高内聚低耦合、代码组织、代码注释、接口定义、显式依赖`

  - **工程结构索引（Indexing）**：
    - _原理_：不给 AI 看具体的每一行代码，而是给它看“目录”和“摘要”。生成一份 JSON 或 Markdown 这种轻量级文件，列出核心类、关键函数的签名（Signature）和注释。
    - _优势_：极大地节省 Token。AI 先查索引找到“我需要改 UserProfile.swift”，再按需请求读取该文件的具体内容。
  - **基础组件知识库（RAG - 检索增强生成）**：
    - _原理_：针对私有二方库，提取其 API 文档和使用案例（Usage Examples），存入向量数据库。
    - _优势_：当 AI 需要写一个网络请求时，它不是瞎编，而是从知识库里检索出内部网络库的标准写法。
  - **动态上下文脚手架**：
    - _原理_：基于当前需求，动态勾选需要的组件。
    - _优势_：相当于一个“按需加载”的加载器，只加载与本次修改有关的上下文，排除噪音。

#### 2. 需求理解（解决设计稿还原度问题）

- **难点**：

  - **Figma 数据非结构化**：设计稿是视觉信息，包含大量图层嵌套，直接`发截图给多模态模型（Vision Model）容易丢失细节（如具体的 Padding 数值、字号）。`
  - **幻觉**：LLM 容易根据经验瞎猜布局，而不是严格还原。

- **方案详解**：
  - **Figma 描述插件 & JSON 中间层**：
    - _原理_：开发 Figma 插件，将视觉稿导出为一种**结构化的 DSL（领域特定语言）**或简化的 JSON。比如描述为 `{ "type": "button", "color": "#FF0000", "padding": 12 }`。
    - _优势_：把“看图猜谜”变成了“完形填空”。JSON 数据更精准，更适合 LLM 处理。
  - **MCP (Model Context Protocol) 集成**：
    - _原理_：利用 MCP 协议连接 Figma 和 IDE，让 AI 能实时读取设计数据，而不是依赖静态导出文件。

---

### 二、 执行阶段：让 AI 写的代码可控、可查

这一阶段的目标是解决“AI 生成一堆垃圾代码难以调试”和“上下文丢失”的问题。核心思路是 **“原子化”** 和 **“状态持久化”**。

#### 1. 原子任务合码（解决代码质量监控）

- **难点**：

  - **Review 困难**：如果 AI 一次性生成了 500 行代码，包含了 UI、逻辑和数据层，人类 Reviewer 很难发现其中的逻辑漏洞。
  - **排查难**：一旦报错，很难定位是哪一部分生成错了。

- **方案详解**：
  - **Small MR (Merge Request) 机制**：
    - _原理_：将一个大需求拆解为 Task A, Task B, Task C。
      - Task A：只写数据模型（Model）。提交 -> Review -> 合入。
      - Task B：只写 UI 布局。提交 -> Review -> 合入。
    - _优势_：**将错误隔离在小范围内**。类似于微服务思想，只有上一步正确了，才执行下一步，降低了回滚成本。

#### 2. 工作流设计（解决 AI “健忘”问题）

- **难点**：

  - **多轮对话遗忘**：在执行 Task C 时，LLM 可能忘了 Task A 里定义的变量名叫什么了。
  - **黑盒操作**：开发者不知道 AI 现在卡在哪一步，是在思考还是在死循环。

- **方案详解**：
  - **MemoryBank 机制（记忆库）**：
    - _原理_：参考 Cursor 或 AutoGPT 的设计，在项目根目录维护一个隐藏的 `memory.md` 或状态文件。
    - _内容_：
      1.  **Context**: 项目背景是什么。
      2.  **Plan**: 总共有 5 步。
      3.  **Active**: 当前正在做第 3 步。
      4.  **Result**: 第 1、2 步产出的关键文件名。
    - _优势_：**外挂大脑**。每次对话开始前，先读取这个文件，确保 AI 永远知道“我是谁，我在哪，我要干什么”。

---

### 总结：这套方案的高明之处

这套方案本质上是在构建一个 **AI 专用的 DevOps 流水线**：

1.  **输入侧**：通过治理和索引，把“非结构化的大型代码库”转换成了“AI 可读的结构化知识库”。
2.  **处理侧**：通过 DSL 化，把“模糊的设计图像”转换成了“精准的数据描述”。
3.  **输出侧**：通过 Small MR 和 MemoryBank，把“黑盒的一揽子生成”转换成了“透明的、原子化的、有状态的增量开发”。

这解决了在巨型企业级项目中落地 AI 编程助手最核心的**可信度（Reliability）**和**可维护性（Maintainability）**问题。

---

## 实践

### 长期记忆

1. 基础组件知识库
2. 存量代码理解
3. ai-friendly 架构建设

### 工具能力

1. 简易脚手架工具
2. figma 描述插件
3. figma D2C UI 还原

### Agent 与短期记忆

1. System Prompt
2. Trae Agent 全链路搭建
3. 原子任务 SmallMR

## Learning

1. 良好的`知识库`是使用 LLM 解决特定领域问题的关键基础设施
   - 示例、组件、相关代码片段 -> 局部最优解
2. 检索
3. `标准化`的长期价值
4. 优先完成全链路能力建设，不要死磕单点
5. 搭建中尽可能摆脱对 SOTA 模型依赖，及时暴露问题

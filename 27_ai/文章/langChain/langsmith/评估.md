# LangSmith 评估 (Evaluation) 快速上手

评估是量化衡量 LLM 应用性能的关键手段。通过结构化的测试，可以识别失败案例、对比版本差异并提升可靠性。

### 1. 核心三要素

- **Dataset (数据集):** 测试输入（Inputs）和期望输出（Reference Outputs/Ground Truth）的集合。
- **Target Function (目标函数):** 你想要测试的应用逻辑（如一个 LLM 调用或整个 RAG 工作流）。
- **Evaluators (评估器):** 对目标函数的输出进行打分的函数（如：LLM-as-judge）。

### 2. 环境配置

安装依赖并配置 API Key：

```bash
# 安装 SDK
npm install langsmith openevals openai
# 配置环境变量
export LANGSMITH_TRACING=true
export LANGSMITH_API_KEY="lsv2_pt_..."
export OPENAI_API_KEY="sk-..."
```

### 3. 开发流程 (SDK 模式)

1.  **创建数据集:** 使用 `client.createDataset` 和 `client.createExamples` 上传测试用例。
2.  **定义目标函数:** 编写一个接收 `inputs` 并返回结果的异步函数。
3.  **定义评估器:** 使用 `openevals` 等库创建评估逻辑（如 `CORRECTNESS_PROMPT`）。
4.  **运行评估:** 调用 `evaluate()` 函数。

```typescript
import { evaluate } from 'langsmith/evaluation'

await evaluate(target, {
  data: 'Sample dataset',
  evaluators: [correctnessEvaluator],
  experimentPrefix: 'my-first-eval'
})
```

### 4. 关键概念

- **Experiment (实验):** 评估的一次具体运行。在 UI 中可以对比不同实验的得分、耗时和成本。
- **Reference Outputs:** 数据集中的“标准答案”，评估器通常将其与模型输出进行对比。
- **LLM-as-judge:** 使用更强大的模型（如 GPT-4o）作为裁判，根据 Prompt 对目标模型的输出进行定性/定量打分。

### 5. 进阶建议

- **自定义评估器:** 除了预设的正确性评估，还可以编写自定义代码评估器（如检查 JSON 格式、计算相似度等）。
- **Playground 评估:** 可以在 LangSmith UI 的 Playground 中直接针对数据集运行不同的 Prompt 进行快速对比。

### 6. 离线评估 vs 在线评估

根据应用生命周期的不同阶段，LangSmith 支持两种评估模式：

| 特性         | 离线评估 (Offline)                | 在线评估 (Online)                |
| :----------- | :-------------------------------- | :------------------------------- |
| **阶段**     | 开发中、部署前                    | 生产环境、部署后                 |
| **目标**     | 数据集 (Datasets/Examples)        | 生产追踪 (Runs/Threads)          |
| **参考答案** | 包含 Reference Outputs (标准答案) | 无标准答案                       |
| **核心用途** | 基准测试、回归测试、单元测试      | 实时监控、异常检测、收集生产反馈 |
| **评估重点** | 准确性 (与标准答案对比)           | 安全性、响应质量、用户行为模式   |

**建议流程：** 先通过**离线评估**验证功能并建立信心，部署后通过**在线评估**监控真实流量中的表现。在线评估发现的失败案例可以一键加入离线数据集，用于后续的修复验证。

### 7. 常见场景评估方案

针对不同类型的 LLM 应用，评估的侧重点有所不同：

#### A. 智能体 (Agents)

- **最终响应 (Final Response):** 将 Agent 视为黑盒，仅评估最终输出是否解决了用户问题。
- **单步评估 (Single Step):** 孤立评估某一步骤（如：在给定上下文下，是否选择了正确的工具及参数）。
- **轨迹评估 (Trajectory):** 评估 Agent 达成目标的路径（工具调用序列）是否符合预期，通常使用 LLM-as-judge 对比参考轨迹。

#### B. 检索增强生成 (RAG)

- **无参考答案评估 (RAG Triad):**
  - **上下文相关性 (Context Relevance):** 检索到的文档是否对回答问题有用。
  - **忠实度 (Faithfulness):** 回答是否完全基于检索到的文档，是否存在幻觉。
  - **回答相关性 (Answer Relevance):** 回答是否直接解决了用户的问题。
- **有参考答案评估:**
  - **回答正确性 (Answer Correctness):** 比较模型输出与标准答案的语义一致性。

#### C. 文本摘要 (Summarization)

- 重点在于**事实准确性**和**简洁性**。
- 通常不使用标准答案（因为摘要很主观），而是使用 LLM-as-judge 根据特定准则（如：是否遗漏关键信息）进行打分。

#### D. 分类与打标 (Classification & Tagging)

- **有标签数据集:** 使用传统的机器学习指标，如准确率 (Accuracy)、精确率 (Precision) 和召回率 (Recall)。
- **无标签监控:** 在线评估中，使用 LLM 实时检测输出是否包含敏感内容（毒性检测）或进行情感分析。

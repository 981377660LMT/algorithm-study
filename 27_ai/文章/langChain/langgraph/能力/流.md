这份文档详细介绍了 **LangGraph** 的流式传输（Streaming）机制，旨在通过实时反馈（如逐字显示 LLM 输出）来提升用户体验。

以下是核心概念和模式的总结：

### 1. 支持的流式模式 (Stream Modes)
通过 `streamMode` 参数控制输出内容：
*   **`updates`**：仅流式传输每一步执行后的**状态增量**（State Deltas）。
*   **`values`**：流式传输每一步执行后的**完整状态**（Full State）。
*   **`messages`**：流式传输 LLM 的 **Token**（逐字输出）和元数据。
*   **`custom`**：流式传输节点内部通过 `writer` 发送的**自定义数据**（如进度条、非 LangChain 模型输出）。
*   **`debug`**：流式传输尽可能多的执行细节，用于调试。

### 2. 核心功能
*   **多模式流式传输**：可以同时请求多种模式（例如 `streamMode: ["updates", "custom"]`），返回 `[mode, chunk]` 元组。
*   **子图流式传输**：设置 `subgraphs: true` 可获取嵌套子图的输出。
*   **LLM Token 流**：
    *   使用 `streamMode: "messages"`。
    *   支持通过 `metadata.tags` 或 `metadata.langgraph_node` 过滤特定节点或模型的输出。
*   **自定义数据流**：
    *   在节点或工具中，从 `LangGraphRunnableConfig` 获取 `writer`。
    *   调用 `config.writer({ ... })` 发送数据。
    *   客户端使用 `streamMode: "custom"` 接收。

### 代码示例
以下展示了如何使用 `updates` 模式进行流式传输：

```typescript
// ...existing code...
// 调用 graph.stream 获取迭代器
for await (const chunk of await graph.stream(
  { topic: "ice cream" },
  { streamMode: "updates" } // 指定模式
)) {
  console.log(chunk);
}
// ...existing code...
```

### 总结
LangGraph 的流式系统非常灵活，不仅支持标准的 LLM Token 流，还允许开发者精确控制状态更新的粒度，甚至通过 `custom` 模式集成任意外部流式数据源。
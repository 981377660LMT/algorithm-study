https://www.youtube.com/watch?v=5G2Db41pSHE&list=PL2mpR0RYFQsADmYpW2YWBrXJZ_6EL_3nu

## 1. Intro: Indexing

https://www.cs.jhu.edu/~langmea/resources/lecture_notes/bwt/205_intro_indexing.pdf

![alt text](image.png)

一种辅助数据结构或映射关系，用来在大量数据中更快地查找特定信息。

两个原则：

1. Grouping
2. Ordering

## 2. Entropy & coding

https://www.cs.jhu.edu/~langmea/resources/lecture_notes/bwt/210_entropy_coding.pdf

![alt text](image-1.png)
![alt text](image-2.png)
![alt text](image-3.png)
熵 (Entropy) 是信息论中衡量随机变量不确定性和信息量的核心指标，为任何无损压缩设定了理论下界。

- 香农熵：

```math
H(X) = -\sum_{x \in X} p(x) \log_2 p(x)
```

当所有概率相同时，熵最大，此时为 worst-case entropy 。

```math
H(X) = \log_2 |X|
```

- Huffman 编码
  - 如何解码
    1. 已知哈夫曼树的解码
       不断读入编码比特；
       每读到能到达树叶，就解出一个符号；
       循环直至整条编码流被解完
    2. 仅知道「符号 ↔ 编码」映射的解码
       因为哈夫曼编码是前缀码 (Prefix-free code)，不存在“某个编码是另一个编码前缀”的情况，所以可以使用前缀匹配来解码。一旦能匹配上一个编码，就知道它对应的符号是什么。
  - 性质
    1. 前缀码 (Prefix-free code)
    2. 最优码 (Optimal code)
       在单符号无记忆场景（每个符号独立出现且频率不随时间变化）下，哈夫曼算法能构造出平均码长 最优 的前缀码
       近似熵界
       ```math
         H(X) \leq \bar{L} < H(X) + 1
       ```
       其中， \(\bar{L}\) 是平均码长，\(H(X)\) 是熵。哈夫曼码保证其平均码长与熵之差至多 1 比特。
       一般情况下，哈夫曼算法是构建“熵编码”中最常见、效果较佳的选择

## 3. High order empirical entropy

https://www.cs.jhu.edu/~langmea/resources/lecture_notes/215_high_order_ent_pub.pdf

- Zero order empirical entropy seems insufficient when context matters

  在信息论和文本压缩等领域，当我们从一段实际数据（如字符串）来“估计”某个模型的熵时，最简单的做法就是“零阶”假设：认为每个符号独立且分布一致，`用符号频率来估计符号出现的概率，然后计算该伯努利/多项式分布的香农熵，这就是“零阶经验熵（zero-order empirical entropy）”`。

  然而，真实序列往往有上下文相关性：一个符号出现的概率会依赖前面出现的一些符号（例如自然语言中的 n-gram 模型）。如果我们想在经验层面捕捉这些相关性，就需要用高阶经验熵（high-order empirical entropy）的概念，通常也称k 阶经验熵（k-th order empirical entropy）。

  在实际数据压缩与统计语言模型中，“高阶经验熵”常用来衡量可压缩程度或语言可预测性，指导我们设计合适的 Markov / n-gram 级别的编码器或语言模型。

## 4. BWT

https://www.cs.jhu.edu/~langmea/resources/lecture_notes/10_bwt_and_fm_index_v2.pdf
![alt text](image-4.png)

## 5. Bitvectors and RSA queries

https://www.cs.jhu.edu/~langmea/resources/lecture_notes/230_bitvectors_rsa_pub.pdf

如何查询 Bitvectors?

- RSA 接口:

  - Rank
  - Select
  - Access

- 第一种实现：预处理(打表)
  ![alt text](image-6.png)
- 第二种实现：预处理+二分
  ![alt text](image-7.png)
- 第三种实现：前缀和+位运算
  O(1) rank, O(log n) select, O(1) access
  O(n/w) 空间
- 第四种实现：Jacobson's rank + Clark's select
  ![alt text](image-9.png)

## 6. Jacobson's rank

O(1) rank

https://www.cs.jhu.edu/~langmea/resources/lecture_notes/235_jacobsons_rank_pub.pdf

关键点：

1. 思路：分块 + 前缀和 + 小块查表
   实现：大块 (superblock) + 小块 (block) + 余位计数 (popcount或者2^b查表) 三段求和
   ![alt text](image-8.png)

## 7. Clark's select

O(1) select

https://www.cs.jhu.edu/~langmea/resources/lecture_notes/240_clarks_select_pub.pdf

![alt text](image-10.png)

- 构建：

1. 分块，使得每个块包含(logn)^2 个1
2. 保存每个块的起始位置
3. 根据阈值(logn)^4 划分 sparse/dense

- 查询：

1. find what chunk it's in (division by log n)
2. if chunkis sparse (> log'n bits)
   - look up answer in sparse offset table
3. if chunk is dense(< log.n bits)
   - look up chunk's offset
   - find what sub-chunk it's in (divide by Vlog n)
   - look up sub-chunk's relative offset
   - if sub-chunk is sparse(> 1/2 log n bits)
     - look up answer in sparse 1-bit table
     - return (c.i)+(c.iii) + (c.iv.A)
   - if sub-chunk is dense
     - look up answer in all possible dense/dense table
     - return (c.i)+(c.iii) +(c.v.A)

## 8. wavelet trees 1

https://www.cs.jhu.edu/~langmea/resources/lecture_notes/bwt/245_wavelet_trees.pdf

![Partition alphabet](image-11.png)
![RSA queries extend naturally to strings](image-12.png)

- access(i)
  ![alt text](image-13.png)
  ![alt text](image-16.png)
- rank(c, i)
  ![alt text](image-14.png)
  ![alt text](image-15.png)
- select(c, i)
  从底部开始
  ![alt text](image-18.png)
  ![alt text](image-17.png)

## 9. wavelet trees 2

## 10. FMIndex 1

![alt text](image-5.png)

## 11. FMIndex 2

## 12. Wheeler graphs 1

## 13. Wheeler graphs 2

## 14. Wheeler graphs 3

## 15. Wheeler graphs 4

## 16. Wheeler graphs 5

## 17. BWT for repetitive texts 1

## 18. BWT for repetitive texts 2

## 19. BWT for repetitive texts 3

倒排索引（Inverted Index）是信息检索系统和搜索引擎的核心数据结构之一，用于快速定位包含某个词项（term）的文档集合。它最主要的思想是：给定一个词项，能够立刻得到包含它的文档列表，而不是在搜索时遍历所有文档。

下面从“构建”和“查询”两个主要阶段，详细介绍倒排索引常用的核心算法、流程和相关优化手段。内容较多，建议分模块理解。

---

## 一、倒排索引的构建

从文档集合中构建倒排索引通常包括以下步骤：

1. **文档获取（Document Collection）**
2. **分词与预处理（Tokenization & Preprocessing）**
3. **统计词项与文档映射关系（Term-Document Mapping）**
4. **排序和压缩（Sorting & Compression）**
5. **索引合并（Index Merging）**

下面分别进行介绍。

---

### 1. 文档获取

- **数据源**：文件系统、网络爬虫、数据库等。
- **数据清洗**：如果文档是非结构化文本，需要做初步的格式化转换或拆分。

此步骤不是算法的核心，但需要确保数据能够正确地流入后续处理流程。

---

### 2. 分词与预处理

在文本检索中，针对文档进行分词（Tokenization）、归一化（Normalization）、去停用词（Stopword Removal）、词干提取（Stemming）或词形还原（Lemmatization）等操作，是构建倒排索引的重要预处理过程。主要包含以下子算法：

1. **分词(Tokenization)**

   - 对文本进行切分，得到一系列词项（term）。
   - 例如中文分词可以用\`正向最大匹配\`、\`双向最大匹配\`或利用\`统计/机器学习/深度学习\`等方法训练分词模型。

2. **去停用词(Stopword Removal)**

   - 一些高频、无实质含义的词（如“的”“了”“and”“the”）可过滤掉。
   - 通过简单哈希查表或二分搜索判断当前词项是否在停用词表中，是则丢弃。

3. **归一化(Normalization)**

   - 包括大小写转换、去除标点符号等；
   - 对中文而言，可能还包括简繁转换；
   - 对多语言文本也需要相应的映射规则。

4. **词干提取(Stemming)** / **词形还原(Lemmatization)**
   - 英文常见的 Porter Stemmer 或 Snowball Stemmer 算法，用于去除单词后缀，例如“computations”“computing”都变成“comput”。
   - 中文中不常使用Stemming，但可以利用同义词/词形词典做一定扩展或归并。

> **小结**：分词和预处理阶段的算法重点在于：**正确高效地将文档分解为可检索的标准化词项**，再将不必要的词项剔除。

---

### 3. 统计词项与文档映射关系（Term-Document Mapping）

在完成分词后，我们需要将“**词项**”与“**文档（或文档ID）**”建立对应关系。该过程通常使用哈希表或搜索树（例如红黑树、B树等）存储中间结果，常见流程为：

1. **遍历所有文档**：对于每个文档，依次处理各个分词得到的term；
2. **记录出现位置**：将 `(term, doc_id, position_in_doc)` 写入临时结构；
3. **累积统计**：
   - 若某词项在某文档中出现多次，需要记录所有出现位置（或者至少记录出现次数）。

这样会得到一个巨大的中间映射，如：

```
(词项A, doc1, pos1), (词项A, doc1, pos2), (词项A, doc2, pos3), ...
(词项B, doc1, pos5), (词项B, doc3, pos6), ...
...
```

在真正写入索引文件之前，往往需要做一次或多次外部排序（external sort），将相同的 `(term)` 聚拢到一起并在其下按 `doc_id` 排序。

#### - 外部排序算法（External Sorting）

如果文档集合特别大，中间数据量可能远超内存，此时需要外部排序算法，例如 **多路归并排序（multi-way merge sort）**。其基本思路：

1. 将文档分块，每块在内存里排序后写入磁盘的临时文件；
2. 最终对这些有序临时文件做多路归并，得到全局有序结果。

这样能高效地处理海量数据，同时使得“同一词项”以及“对应的doc_id列表”能被聚集到一起，为后续构建倒排列表奠定基础。

---

### 4. 排序和压缩（Sorting & Compression）

完成外部排序后，就可以真正生成倒排索引。常见形式如下：

```
Term A -> [(doc1, [pos1, pos2, ...]), (doc2, [posX, posY, ...]), ...]
Term B -> [(doc1, [posM, ...]), (doc3, [posN, ...]), ...]
...
```

每个词项对应一个倒排列表（Posting List），倒排列表通常包含以下内容：

- `doc_id`
- 词项在文档中的出现位置（可选）
- 出现次数（可选）
- 权重（可选），如 TF-IDF 值等

为了进一步减少存储量，提高查询效率，往往会用到**压缩**算法及**跳表（跳跃表）**、**跳跃指针（skip pointers）**、**分块索引**等技术。

#### 4.1 压缩算法

对于索引中的 `doc_id` 和位置（pos），往往使用**差分编码（delta encoding）**和**变长编码（variable-length encoding）**进行压缩。例如：

- **差分编码（Delta Encoding）**：在倒排列表中，`doc_id` 是递增存储的，所以可以只存相邻 `doc_id` 的差值。例如 `doc_id` 列表为 `[10, 14, 15, 100, 101]`，可以存为 `[10, 4, 1, 85, 1]`；因为差值通常更小，更有利于后续的变长压缩。
- **变长编码（Variable-length Encoding）**：将小整数用更短的编码表示，大整数用更长的编码表示。常见方法有：
  - **VByte**（Variable Byte）：把一个字节最高位当作“是否继续”的标记；
  - **Gamma Code** / **Delta Code**：基于二进制前缀和偏移量的编码；
  - **FST**（Finite State Transducer）：适用于压缩大量相似前缀的字符串集合，比如对Term本身的前缀进行压缩。

#### 4.2 跳跃表 / 跳跃指针（Skip Pointers）

当倒排列表很长时，为了在查询时更快地跳过不需要的 `doc_id`，会在倒排列表中插入跳跃指针。例如每隔固定数量的文档ID记录一个“跳跃节点”（skip entry），里面包含：

- 当前的 `doc_id`
- 索引在文件中的偏移量
- 其他统计信息（如最大/最小 `doc_id` 等）

在查询时，如果目标 `doc_id` 大于某个跳跃节点标记的范围，便可以**跳过**中间一大段，显著提升合并或求交集的效率。  
这种数据结构概念上与“跳表”或“分块”类似，常见于Lucene等搜索引擎的实现。

---

### 5. 索引合并（Index Merging）

在实际搜索系统中，常常是**实时**或**增量**地往索引里添加文档，或者对索引数据进行批量更新。因此，完整的倒排索引可能会分为多个“段”（segment）。

- 当新数据进来时，先快速为新数据构建一个小索引段（in-memory segment），再与大索引离线合并。
- 合并时再进行排序、压缩、去重等操作，得到一个新的较大索引段。
- 原来的小索引段可以删除或标记失效，以节省空间。

合并过程也会用到**多路归并**算法，即分别遍历多个段的倒排索引文件，将相同Term合并在一起，合并后的倒排列表进行去重、排序并重新压缩。

---

## 二、倒排索引的查询与检索

一旦倒排索引建立好，用户在进行查询时，搜索引擎或信息检索系统会执行大体如下流程：

1. **查询解析**：对用户输入做分词、预处理，得到查询词项集合。
2. **检索倒排表**：从索引中查找这些词项的倒排列表。
3. **合并/交集**：根据布尔逻辑（AND/OR/NOT）或其他策略，合并多个词项对应的倒排列表得到候选文档集合。
4. **排序/打分**：对候选文档进行相关性计算，例如 TF-IDF、BM25，或基于神经网络模型的重新打分，最终得到排序结果。
5. **返回结果**。

这里重点介绍一下“检索倒排表”与“合并/交集”常用的算法和技巧。

---

### 1. 检索倒排表（Lookup）

查询词项通常先经过一个**词典（dictionary / lexicon）**结构查询，获得该词项所在的**倒排列表**起始偏移及元数据。词典查找常用的数据结构有：

- **哈希表**：O(1) 平均查找，但需要能在内存中容纳。
- **B+树** / **前缀树（Trie/FST）**：适合大规模词典，支持前缀搜索。
- **有限状态机**（FST） 等特定压缩结构。

一旦找到对应倒排列表，就可以读出对应的文档ID列表（或经过解压），参与后续的合并操作。

---

### 2. 合并 / 求交集（Merge / Intersection）

当用户输入多个关键词，如 “信息 检索”，需要获取同时包含“信息”和“检索”的文档ID列表，即做倒排列表的**交集**操作。常用算法有：

1. **两路归并（Two-pointer Merge）**

   - 假设倒排列表按 `doc_id` 有序，可用两个指针分别遍历两份列表，小的移动一格，相等时输出一个结果。
   - 复杂度与列表长度成正比：O(m + n)。

2. **跳跃指针（Skip Pointers）**

   - 如果倒排列表很长，可以利用跳跃指针快速移动。例如当一个列表指针停在 `doc_id=100`，而另外一个列表当前指针的 `doc_id=500` 时，可以利用跳跃表跳过 `101~499` 这一段，提高效率。
   - 实际实现中，需要结合具体段落或块状索引数据结构，达到快速跳跃的效果。

3. **块式跳跃（Block Skipping / Blocked Retrieval）**

   - 将倒排列表分成固定大小的块，每个块都维护一个 “最小doc_id / 最大doc_id / 块内数量” 等信息，碰到不可能的块直接跳过。
   - Lucene 等系统中会把倒排列表按一定大小分块，并在每个块里存储压缩后的文档ID和跳跃信息。

4. **波特岭交集算法（Galloping / Exponential Search Intersection）**
   - 如果一个列表远比另一个列表要短，可以对较短的列表中的doc_id，用“指数搜索（exponential search）”在较长的列表里定位，进一步加速交集操作。

> **延伸**：对于OR操作，需要做倒排列表的**并集**；NOT操作则要在一个倒排列表中减去另一个集合。算法思路类似，都是基于有序合并和跳跃指针来实现。

---

### 3. 评分与排序（Ranking）

当我们拿到了所有满足查询条件的文档后，通常会进一步计算每个文档与查询的相关性分数。常用的公式或模型包括：

- **TF-IDF**：计算文档对查询词项的TF（词频）和词项的IDF（逆文档频率）乘积。
- **BM25**：经典的统计语言学模型，对词项在文档内分布和平均文档长度进行归一化处理。
- **向量空间模型** / **神经模型**：在更复杂的语义检索中使用向量表示和检索。

这些涉及到对倒排列表中的词频信息以及全局的文档统计信息（如文档长度、文档总数等）进行计算或查表。

---

## 三、总结

**倒排索引（Inverted Index）** 在现代搜索引擎和信息检索系统中至关重要，通过“词典 + 倒排列表”的结构，能够高效地完成从“词项”到“包含该词项的文档列表”的映射。其核心算法涵盖了：

1. **构建过程**

   - 文本分词与预处理（Tokenization, Normalization, Stemming, 去停用词等）
   - 外部排序（External Sorting, 多路归并）
   - 压缩（Delta Encoding, Variable-length Encoding）
   - 索引合并（多段合并）
   - 跳跃表 / 分块索引加速查找

2. **查询过程**
   - 词典查找、定位倒排列表
   - 倒排列表合并（两路归并、跳跃指针、块式跳跃等）
   - 相关性计算（TF-IDF、BM25、向量模型等）

这些算法和数据结构相辅相成，使得在海量文档中进行关键字检索时，能在极短时间内返回结果。随着大规模数据与实时增量需求的提升，进一步的优化也会引入更复杂的并行化、分布式存储以及向量检索等新技术，但倒排索引依然是大多数文本检索系统的基本功。希望以上内容能帮助你深入理解倒排索引里使用的主要算法及其背后的设计思路。

https://www.zhihu.com/question/26726794/answer/151282052

k 近邻、贝叶斯、决策树、svm、逻辑斯蒂回归和最大熵模型、隐马尔科夫、条件随机场、adaboost、em 这些在一般工作中分别用到的频率多大？一般用途是什么？需要注意什么？

**没有最好的分类器，只有最合适的分类器。**
随机森林平均来说最强，但也只在 9.9%的数据集上拿到了第一，优点是鲜有短板。
SVM 的平均水平紧随其后，在 10.7%的数据集上拿到第一。
神经网络（13.2%）和 boosting（~9%）表现不错。
数据维度越高，随机森林就比 AdaBoost 强越多，但是整体不及 SVM[2]。
数据量越大，神经网络就越强。

- 近邻 (Nearest Neighbor)
  典型的例子是 KNN，它的思路就是——对于待判断的点，找到离它最近的几个数据点，根据它们的类型决定待判断点的类型。
  它的特点是完全跟着数据走，没有数学模型可言。

  适用情景：
  需要一个特别容易解释的模型的时候。
  比如`需要向用户解释原因的推荐算法`。

- 贝叶斯 (Bayesian)
  典型的例子是 Naive Bayes，核心思路是根据条件概率计算待判断点的类型。
  是相对容易理解的一个模型，至今依然被`垃圾邮件过滤器`使用。

  适用情景：
  需要一个比较容易解释，而且不同维度之间相关性较小的模型的时候。
  可以高效处理高维数据，虽然结果可能不尽如人意。

- 决策树 (Decision tree)
  决策树的特点是它总是在沿着特征做切分。随着层层递进，这个划分会越来越细。
  虽然生成的树不容易给用户看，但是数据分析的时候，通过观察树的上层结构，能够对分类器的核心思路有一个直观的感受。
  举个简单的例子，当我们预测一个孩子的身高的时候，决策树的第一层可能是这个孩子的性别。男生走左边的树进行进一步预测，女生则走右边的树。这就说明性别对身高有很强的影响。

  适用情景：
  因为它能够生成清晰的基于特征(feature)选择不同预测结果的树状结构，数据分析师希望更好的理解手上的数据的时候往往可以使用决策树。
  同时它也是相对容易被攻击的分类器[3]。这里的攻击是指人为的改变一些特征，使得分类器判断错误。常见于垃圾邮件躲避检测中。`因为决策树最终在底层判断是基于单个条件的，攻击者往往只需要改变很少的特征就可以逃过监测。`
  受限于它的简单性，决策树更大的用处是作为一些更有用的算法的基石。

- 随机森林 (Random forest)
  提到决策树就不得不提随机森林。顾名思义，森林就是很多树。
  严格来说，随机森林其实算是一种`集成算法`。它首先随机选取不同的特征(feature)和训练样本(training sample)，`生成大量的决策树`，然后综合这些决策树的结果来进行最终的分类。
  随机森林在现实分析中被大量使用，它相对于决策树，在准确性上有了很大的提升，同时一定程度上改善了决策树容易被攻击的特点。

  适用情景：
  数据维度相对低（几十维），同时对准确性有较高要求时。
  `因为不需要很多参数调整就可以达到不错的效果，基本上不知道用什么方法的时候都可以先试一下随机森林`。

- SVM (Support vector machine，支持向量机)
  SVM 的核心思想就是找到不同类别之间的`分界面`，使得两类样本尽量落在面的两边，而且离分界面尽量远。
  最早的 SVM 是平面的，局限很大。但是`利用核函数(kernel function)，我们可以把平面投射(mapping)成曲面，进而大大提高 SVM 的适用范围`。
  提高之后的 SVM 同样被大量使用，在实际分类中展现了很优秀的正确率。

  适用情景：
  SVM 在很多数据集上都有优秀的表现。
  相对来说，SVM 尽量保持与样本间距离的性质导致它抗攻击的能力更强。
  和随机森林一样，`这也是一个拿到数据就可以先尝试一下的算法`。

- 逻辑斯蒂回归 (Logistic regression)
  逻辑斯蒂回归这个名字太诡异了，我就叫它 LR 吧，反正讨论的是分类器，也没有别的方法叫 LR。顾名思义，它其实是回归类方法的一个变体。
  回归方法的核心就是`为函数找到最合适的参数，使得函数的值和样本的值最接近`。例如线性回归(Linear regression)就是对于函数 f(x)=ax+b，找到最合适的 a,b。
  LR 拟合的就不是线性函数了，它拟合的是一个概率学中的函数，f(x)的值这时候就反映了样本属于这个类的概率。
  适用情景：LR 同样是很多分类算法的基础组件，它的好处是输出值自然地落在 0 到 1 之间，并且有概率意义。因为它本质上是一个线性的分类器，所以`处理不好特征之间相关的情况`。虽然效果一般，却胜在模型清晰，背后的概率学经得住推敲。它拟合出来的参数就代表了每一个特征(feature)对结果的影响。也是一个理解数据的好工具。

- 判别分析 (Discriminant analysis)
  判别分析的典型例子是线性判别分析(Linear discriminant analysis)，简称 LDA。
  （这里注意不要和隐含狄利克雷分布(Latent Dirichlet allocation)弄混，虽然都叫 LDA 但说的不是一件事。）
  LDA 的核心思想是把高维的`样本投射(project)到低维`上，如果要分成两类，就投射到一维。要分三类就投射到二维平面上。这样的投射当然有很多种不同的方式，LDA 投射的标准就是让`同类的样本尽量靠近，而不同类的尽量分开`。对于未来要预测的样本，用同样的方式投射之后就可以轻易地分辨类别了。
  使用情景：判别分析适用于高维数据需要降维的情况，自带降维功能使得我们能方便地观察样本分布。它的正确性有数学公式可以证明，所以同样是很经得住推敲的方式。但是它的`分类准确率往往不是很高，所以不是统计系的人就把它作为降维工具用吧`。同时注意它是假定样本成正态分布的，所以那种同心圆形的数据就不要尝试了。

- 神经网络 (Neural network)
  神经网络现在是火得不行啊。它的`核心思路是利用训练样本(training sample)来逐渐地完善参数`。还是举个例子预测身高的例子，如果输入的特征中有一个是性别（1:男；0:女），而输出的特征是身高（1:高；0:矮）。那么当训练样本是一个个子高的男生的时候，在神经网络中，从“男”到“高”的路线就会被强化。
  同理，如果来了一个个子高的女生，那从“女”到“高”的路线就会被强化。`最终神经网络的哪些路线比较强，就由我们的样本所决定`。
  神经网络的优势在于，它可以有很多很多层。如果输入输出是直接连接的，那它和 LR 就没有什么区别。但是通过大量中间层的引入，它就能够捕捉很多输入特征之间的关系。卷积神经网络有很经典的不同层的可视化展示(visulization)，我这里就不赘述了。
  神经网络的提出其实很早了，但是它的准确率依赖于庞大的训练集，原本受限于计算机的速度，分类效果一直不如随机森林和 SVM 这种经典算法。

  使用情景：
  数据量庞大，参数之间存在内在联系的时候。
  当然现在神经网络不只是一个分类器，它还可以用来生成数据，用来做降维，这些就不在这里讨论了。

- 提升算法（Boosting）
  接下来讲的一系列模型，都属于集成学习算法(Ensemble Learning)，基于一个核心理念：三个臭皮匠，顶个诸葛亮。翻译过来就是：当我们把多个较弱的分类器结合起来的时候，它的结果会比一个强的分类器更典型的例子是 AdaBoost。
  AdaBoost 的实现是一个渐进的过程，从一个最基础的分类器开始，每次寻找一个最能解决当前错误样本的分类器。用加权取和(weighted sum)的方式把这个新分类器结合进已有的分类器中。它的好处是自带了特征选择（feature selection），只使用在训练集中发现有效的特征(feature)。这样就降低了分类时需要计算的特征数量，也在一定程度上解决了高维数据难以理解的问题。最经典的 AdaBoost 实现中，它的每一个弱分类器其实就是一个决策树。这就是之前为什么说决策树是各种算法的基石。
  使用情景：好的 Boosting 算法，它的准确性不逊于随机森林。虽然在[1]的实验中只有一个挤进前十，但是实际使用中它还是很强的。因为自带特征选择（feature selection）所以对新手很友好，是一个“不知道用什么就试一下它吧”的算法。
